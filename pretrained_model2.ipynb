{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' sfj a fsaf kd sfj a fsaf kd sfj a fsaf kd'"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "at = 'sfj a fsaf kd'\n",
    "a = ''\n",
    "for j in range(3):\n",
    "    at = ' '.join(at.split())\n",
    "    a = a+' '+at\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import array\n",
    "import pickle as plk\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Bidirectional\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM, GRU\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.utils import np_utils\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Embedding\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import os\n",
    "from sacrebleu import sentence_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df = pd.read_excel('/Users/luyin/Desktop/project/Q&A.xlsx',header = 0)\n",
    "df = pd.read_excel(os.getcwd()+'/Q&A_Database_new.xlsx','QA', skiprows=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l = df['Breakout'].unique() # 79 unique analyst\n",
    "dic = {} #create dictionary for questions\n",
    "for category in l:\n",
    "    list_ = list(df.loc[df['Breakout']  == category]['Question'])\n",
    "    dic[category] = list_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import re\n",
    "import string\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "\n",
    "# tokenizer = spacy.load('en_core_web_sm')\n",
    "punctuations = string.punctuation\n",
    "\n",
    "#tokenize sentence by sentence\n",
    "def question_split(input_):\n",
    "    list_ = []\n",
    "    for q in input_:\n",
    "        q = q.split('\\n')\n",
    "        if len(q) ==1:\n",
    "            list_.append(q)\n",
    "    return list_\n",
    "\n",
    "def tokenize(sent):\n",
    "#   sent = re.sub('[^A-Za-z&]', ' ', sent) # replace non-letter with space\n",
    "#   sent = re.sub(r'\\b[a-zA-Z]\\b', '', sent) #remove single letter \n",
    "    sent = re.sub('y ou', 'you', sent)\n",
    "    sent = re.sub('y es', 'yes', sent)\n",
    "    sent = re.sub('v o', 'vo', sent)\n",
    "    sent = re.sub(\"don't\", 'dont', sent)\n",
    "    sent = re.sub('[^A-Za-z&]', ' ', sent)\n",
    "#     tokens = tokenizer(sent)\n",
    "    return sent.split()\n",
    "\n",
    "# tokens = tokenize(\" going to hit them one way or another strong dollar did seem to have a huge impact and y ou, what are you doing? I'm 's what do you and me think or like apples and Apple is looking at buying and bought U.K. startup for $1 billion. '\\n' another sentence\")\n",
    "# for token in tokens:\n",
    "#     print (token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['CIB', 'Capital', 'Balance sheet', 'Expenses', 'CB',\n",
       "       'Credit costs', 'Accounting and taxes', 'CCB', 'Other topics',\n",
       "       'AWM', 'Legal', 'Macroeconomic update', 'Revenue',\n",
       "       'Regulatory topics', 'Capital '], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Breakout'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_analyst_q(name):\n",
    "    all_tokens = []\n",
    "    for q in dic[name]:\n",
    "        tokens = tokenize(q)\n",
    "        all_tokens += tokens\n",
    "    return all_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load training and validation dataset\n",
    "dic_len = len(dic['Balance sheet'])\n",
    "all_data = dic['Balance sheet']\n",
    "train_data = dic['Balance sheet'][:500]\n",
    "val_data = dic['Balance sheet'][500: dic_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_tokens = []\n",
    "for q in dic['Balance sheet']:\n",
    "    tokens = tokenize(q)\n",
    "    all_tokens += tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_vocab(input_):\n",
    "    vocab = sorted(set(input_))\n",
    "    vocab_to_int = dict((c, i) for i, c in enumerate(vocab))\n",
    "    int_to_vocab = dict((i, c) for i, c in enumerate(vocab))\n",
    "    n_total = len(input_)\n",
    "    n_vocab = len(vocab)\n",
    "    print (\"Total Words: {}\".format(n_total))\n",
    "    print (\"Total Vocab: {}\".format(n_vocab))\n",
    "    return n_total, n_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# organize into sequences of tokens\n",
    "def build_sequence(length, input_):\n",
    "    sequences = list()\n",
    "    for l in range(length, len(input_)):\n",
    "        seq = input_[l-length:l]\n",
    "        line = ' '.join(seq)\n",
    "        sequences.append(line)\n",
    "#     print('Total Sequences: {}' .format(len(sequences)))\n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_generator(dic, length):\n",
    "    out_list = []\n",
    "    raw = question_split(dic)\n",
    "    for i in range(len(raw)):\n",
    "        sent = tokenize(raw[i][0].lower())\n",
    "        q_sequence = build_sequence(length, sent)\n",
    "        out_list += q_sequence\n",
    "    return out_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_d = data_generator(all_data,20)\n",
    "train_d = data_generator(train_data,20)\n",
    "val_d = data_generator(val_data,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def save_file(lines, filename):\n",
    "#     data = '\\n'.join(lines)\n",
    "#     file = open(filename, 'w')\n",
    "#     file.write(data)\n",
    "#     file.close()\n",
    "\n",
    "# out_filename = 'Balance sheet.txt'\n",
    "# save_file(all_d, out_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load doc into memory\n",
    "# def load_file(filename):\n",
    "#     file = open(filename, 'r')\n",
    "#     text = file.read()\n",
    "#     file.close()\n",
    "#     return text\n",
    " \n",
    "# in_filename = 'Balance sheet.txt'\n",
    "# doc = load_file(in_filename)\n",
    "# lines = doc.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = Tokenizer()\n",
    "# tokenizer.fit_on_texts(lines) #fit on texts\n",
    "# sequences = tokenizer.texts_to_sequences(lines)\n",
    "# vocab_size = len(tokenizer.word_index) + 1\n",
    "# sequences = array(sequences)\n",
    "# X, y = sequences[:,:-1], sequences[:,-1]\n",
    "# y = to_categorical(y, num_classes=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words_to_load = 50000\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "SOS_IDX = 2\n",
    "EOS_IDX = 3\n",
    "import numpy as np\n",
    "# reserve the 1st 2nd token for padding and <UNK> respectively\n",
    "with open('/Users/cyian/Desktop/NYU/FALL2018/DS-GA1011_NLP/hws/HW2/wiki-news-300d-1M.vec') as f:\n",
    "    loaded_embeddings_ft_en = np.zeros((words_to_load+4, 300))\n",
    "    words_ft_en = {}\n",
    "    idx2words_ft_en = {}\n",
    "    ordered_words_ft_en = []\n",
    "    ordered_words_ft_en.extend(['<pad>', '<unk>', '<s>', '</s>'])\n",
    "    loaded_embeddings_ft_en[0,:] = np.zeros(300)\n",
    "    loaded_embeddings_ft_en[1,:] = np.random.normal(size = 300)\n",
    "    loaded_embeddings_ft_en[2,:] = np.random.normal(size = 300)\n",
    "    loaded_embeddings_ft_en[3,:] = np.random.normal(size = 300)\n",
    "    for i, line in enumerate(f):\n",
    "        if i >= words_to_load: \n",
    "            break\n",
    "        s = line.split()\n",
    "        loaded_embeddings_ft_en[i+4, :] = np.asarray(s[1:])\n",
    "        words_ft_en[s[0]] = i+4\n",
    "        idx2words_ft_en[i+4] = s[0]\n",
    "        ordered_words_ft_en.append(s[0])\n",
    "    words_ft_en['<pad>'] = PAD_IDX\n",
    "    words_ft_en['<unk>'] = UNK_IDX\n",
    "    words_ft_en['<s>'] = SOS_IDX\n",
    "    words_ft_en['</s>'] = EOS_IDX\n",
    "    idx2words_ft_en[PAD_IDX] = '<pad>'\n",
    "    idx2words_ft_en[UNK_IDX] = '<unk>'\n",
    "    idx2words_ft_en[SOS_IDX] = '<s>'\n",
    "    idx2words_ft_en[EOS_IDX] = '</s>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_to_id(word_list):\n",
    "    return [words_ft[x] if x in ordered_words_ft else UNK_IDX for x in word_list.split()]\n",
    "def sent_to_id(sent_list):\n",
    "    return [word_to_id(x) for x in sent_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# len_list = []\n",
    "# for x in lines:\n",
    "#     len_list.append(len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(len_list)[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "EMBEDDING_DIM = 300\n",
    "MAX_SEQUENCE_LENGTH = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keras_rnn_predict(samples, empty=SOS_IDX, rnn_model=model, maxlen=MAX_SEQUENCE_LENGTH):\n",
    "    \"\"\"for every sample, calculate probability for every possible label\n",
    "    you need to supply your RNN model and maxlen - the length of sequences it can handle\n",
    "    \"\"\"\n",
    "    data = sequence.pad_sequences(samples, maxlen=maxlen, value=empty)\n",
    "    return rnn_model.predict(data, verbose=0)\n",
    "\n",
    "def beamsearch(predict = keras_rnn_predict,\n",
    "               k=1, maxsample=400, use_unk=False, oov=UNK_IDX, empty=SOS_IDX, eos=EOS_IDX):\n",
    "    \"\"\"return k samples (beams) and their NLL scores, each sample is a sequence of labels,\n",
    "    all samples starts with an `empty` label and end with `eos` or truncated to length of `maxsample`.\n",
    "    You need to supply `predict` which returns the label probability of each sample.\n",
    "    `use_unk` allow usage of `oov` (out-of-vocabulary) label in samples\n",
    "    \"\"\"\n",
    "    \n",
    "    dead_k = 0 # samples that reached eos\n",
    "    dead_samples = []\n",
    "    dead_scores = []\n",
    "    live_k = 1 # samples that did not yet reached eos\n",
    "    live_samples = [[empty]]\n",
    "    live_scores = [0]\n",
    "\n",
    "    while live_k and dead_k < k:\n",
    "        # for every possible live sample calc prob for every possible label \n",
    "        probs = predict(live_samples, empty=empty)\n",
    "\n",
    "        # total score for every sample is sum of -log of word prb\n",
    "        cand_scores = np.array(live_scores)[:,None] - np.log(probs)\n",
    "        if not use_unk and oov is not None:\n",
    "            cand_scores[:,oov] = 1e20\n",
    "        cand_flat = cand_scores.flatten()\n",
    "\n",
    "        # find the best (lowest) scores we have from all possible samples and new words\n",
    "        ranks_flat = cand_flat.argsort()[:(k-dead_k)]\n",
    "        live_scores = cand_flat[ranks_flat]\n",
    "\n",
    "        # append the new words to their appropriate live sample\n",
    "        voc_size = probs.shape[1]\n",
    "        live_samples = [live_samples[r//voc_size]+[r%voc_size] for r in ranks_flat]\n",
    "\n",
    "        # live samples that should be dead are...\n",
    "        zombie = [s[-1] == eos or len(s) >= maxsample for s in live_samples]\n",
    "        \n",
    "        # add zombies to the dead\n",
    "        dead_samples += [s for s,z in zip(live_samples,zombie) if z]  # remove first label == empty\n",
    "        dead_scores += [s for s,z in zip(live_scores,zombie) if z]\n",
    "        dead_k = len(dead_samples)\n",
    "        # remove zombies from the living \n",
    "        live_samples = [s for s,z in zip(live_samples,zombie) if not z]\n",
    "        live_scores = [s for s,z in zip(live_scores,zombie) if not z]\n",
    "        live_k = len(live_samples)\n",
    "\n",
    "    return dead_samples + live_samples, dead_scores + live_scores   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16037"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "print(len(train_d[0].split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16037, 3146)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16037, 19)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16037"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_input[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_SEQUENCE_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 16037)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16037, 300)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_20 (Embedding)     (None, 19, 300)           15001200  \n",
      "_________________________________________________________________\n",
      "bidirectional_33 (Bidirectio (None, 19, 200)           240600    \n",
      "_________________________________________________________________\n",
      "bidirectional_34 (Bidirectio (None, 200)               180600    \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 300)               30300     \n",
      "=================================================================\n",
      "Total params: 15,472,800\n",
      "Trainable params: 471,600\n",
      "Non-trainable params: 15,001,200\n",
      "_________________________________________________________________\n",
      "Train on 16037 samples, validate on 6896 samples\n",
      "Epoch 1/100\n",
      "16037/16037 [==============================] - 26s 2ms/step - loss: -48.9629 - acc: 0.2273 - val_loss: -57.5986 - val_acc: 0.3361\n",
      "\n",
      "Epoch 00001: loss improved from inf to -48.96286, saving model to GRU_1.hdf5\n",
      "Epoch 2/100\n",
      "16037/16037 [==============================] - 22s 1ms/step - loss: -60.5103 - acc: 0.3310 - val_loss: -57.7173 - val_acc: 0.3379\n",
      "\n",
      "Epoch 00002: loss improved from -48.96286 to -60.51030, saving model to GRU_1.hdf5\n",
      "Epoch 3/100\n",
      "16037/16037 [==============================] - 22s 1ms/step - loss: -60.5267 - acc: 0.2870 - val_loss: -57.7058 - val_acc: 0.3351\n",
      "\n",
      "Epoch 00003: loss improved from -60.51030 to -60.52667, saving model to GRU_1.hdf5\n",
      "Epoch 4/100\n",
      "16037/16037 [==============================] - 21s 1ms/step - loss: -60.5478 - acc: 0.2910 - val_loss: -57.6952 - val_acc: 0.3238\n",
      "\n",
      "Epoch 00004: loss improved from -60.52667 to -60.54783, saving model to GRU_1.hdf5\n",
      "Epoch 5/100\n",
      "16037/16037 [==============================] - 22s 1ms/step - loss: -60.6172 - acc: 0.3082 - val_loss: -57.8984 - val_acc: 0.3305\n",
      "\n",
      "Epoch 00005: loss improved from -60.54783 to -60.61716, saving model to GRU_1.hdf5\n",
      "Epoch 6/100\n",
      "16037/16037 [==============================] - 21s 1ms/step - loss: -60.9552 - acc: 0.2422 - val_loss: -58.7126 - val_acc: 0.2781\n",
      "\n",
      "Epoch 00006: loss improved from -60.61716 to -60.95523, saving model to GRU_1.hdf5\n",
      "Epoch 7/100\n",
      "16037/16037 [==============================] - 21s 1ms/step - loss: -62.3875 - acc: 0.2880 - val_loss: -59.9775 - val_acc: 0.3273\n",
      "\n",
      "Epoch 00007: loss improved from -60.95523 to -62.38750, saving model to GRU_1.hdf5\n",
      "Epoch 8/100\n",
      "16037/16037 [==============================] - 22s 1ms/step - loss: -63.0500 - acc: 0.2656 - val_loss: -60.2512 - val_acc: 0.1992\n",
      "\n",
      "Epoch 00008: loss improved from -62.38750 to -63.04997, saving model to GRU_1.hdf5\n",
      "Epoch 9/100\n",
      "16037/16037 [==============================] - 22s 1ms/step - loss: -63.6359 - acc: 0.2790 - val_loss: -60.2666 - val_acc: 0.2819\n",
      "\n",
      "Epoch 00009: loss improved from -63.04997 to -63.63592, saving model to GRU_1.hdf5\n",
      "Epoch 10/100\n",
      "16037/16037 [==============================] - 29s 2ms/step - loss: -63.9913 - acc: 0.2499 - val_loss: -60.8310 - val_acc: 0.2745\n",
      "\n",
      "Epoch 00010: loss improved from -63.63592 to -63.99133, saving model to GRU_1.hdf5\n",
      "Epoch 11/100\n",
      "16037/16037 [==============================] - 25s 2ms/step - loss: -64.4915 - acc: 0.2729 - val_loss: -60.8856 - val_acc: 0.3177\n",
      "\n",
      "Epoch 00011: loss improved from -63.99133 to -64.49152, saving model to GRU_1.hdf5\n",
      "Epoch 12/100\n",
      "16037/16037 [==============================] - 26s 2ms/step - loss: -65.0027 - acc: 0.2754 - val_loss: -60.8021 - val_acc: 0.3182\n",
      "\n",
      "Epoch 00012: loss improved from -64.49152 to -65.00267, saving model to GRU_1.hdf5\n",
      "Epoch 13/100\n",
      "16037/16037 [==============================] - 22s 1ms/step - loss: -65.6871 - acc: 0.2933 - val_loss: -60.2580 - val_acc: 0.3314\n",
      "\n",
      "Epoch 00013: loss improved from -65.00267 to -65.68709, saving model to GRU_1.hdf5\n",
      "Epoch 14/100\n",
      "16037/16037 [==============================] - 22s 1ms/step - loss: -66.0836 - acc: 0.3026 - val_loss: -60.4116 - val_acc: 0.2941\n",
      "\n",
      "Epoch 00014: loss improved from -65.68709 to -66.08356, saving model to GRU_1.hdf5\n",
      "Epoch 15/100\n",
      "16037/16037 [==============================] - 22s 1ms/step - loss: -66.5214 - acc: 0.2920 - val_loss: -59.9626 - val_acc: 0.2626\n",
      "\n",
      "Epoch 00015: loss improved from -66.08356 to -66.52136, saving model to GRU_1.hdf5\n",
      "Epoch 16/100\n",
      "16037/16037 [==============================] - 28s 2ms/step - loss: -66.8843 - acc: 0.2732 - val_loss: -60.0046 - val_acc: 0.2575\n",
      "\n",
      "Epoch 00016: loss improved from -66.52136 to -66.88431, saving model to GRU_1.hdf5\n",
      "Epoch 17/100\n",
      "16037/16037 [==============================] - 28s 2ms/step - loss: -67.3386 - acc: 0.2756 - val_loss: -59.8398 - val_acc: 0.2210\n",
      "\n",
      "Epoch 00017: loss improved from -66.88431 to -67.33857, saving model to GRU_1.hdf5\n",
      "Epoch 18/100\n",
      "16037/16037 [==============================] - 29s 2ms/step - loss: -67.9150 - acc: 0.2658 - val_loss: -59.8960 - val_acc: 0.2826\n",
      "\n",
      "Epoch 00018: loss improved from -67.33857 to -67.91498, saving model to GRU_1.hdf5\n",
      "Epoch 19/100\n",
      "16037/16037 [==============================] - 24s 1ms/step - loss: -67.8668 - acc: 0.2652 - val_loss: -59.8433 - val_acc: 0.2578\n",
      "\n",
      "Epoch 00019: loss did not improve from -67.91498\n",
      "Epoch 20/100\n",
      "16037/16037 [==============================] - 23s 1ms/step - loss: -69.3534 - acc: 0.2689 - val_loss: -59.5723 - val_acc: 0.2581\n",
      "\n",
      "Epoch 00020: loss improved from -67.91498 to -69.35339, saving model to GRU_1.hdf5\n",
      "Epoch 21/100\n",
      "16037/16037 [==============================] - 24s 2ms/step - loss: -70.2455 - acc: 0.2680 - val_loss: -58.9835 - val_acc: 0.2762\n",
      "\n",
      "Epoch 00021: loss improved from -69.35339 to -70.24550, saving model to GRU_1.hdf5\n",
      "Epoch 00021: early stopping\n"
     ]
    }
   ],
   "source": [
    "# integer encode sequences of words\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(all_tokens) #fit on texts\n",
    "def data_loader(data_input):\n",
    "    X = []\n",
    "    y = []\n",
    "    for i in range(len(data_input)):\n",
    "        X.append([words_ft_en[x] if x in ordered_words_ft_en else UNK_IDX for x in data_input[i].split()[:-1]])\n",
    "        sub = data_input[i].split()[-1]\n",
    "        if sub in ordered_words_ft_en:\n",
    "            y.append(loaded_embeddings_ft_en[words_ft_en[sub]])\n",
    "        else:\n",
    "            y.append(loaded_embeddings_ft_en[UNK_IDX])\n",
    "    return np.array(X), np.array(y)\n",
    "train_input, train_label =  data_loader(train_d)\n",
    "val_input, val_label =  data_loader(val_d)\n",
    "# sequences = tokenizer.texts_to_sequences(lines)\n",
    "# vocab_size = len(tokenizer.word_index) + 1\n",
    "# MAX_NUM_WORDS = words_to_load\n",
    "# tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "# tokenizer.fit_on_texts(lines)\n",
    "# sequences = tokenizer.texts_to_sequences(lines)\n",
    "# word_index = tokenizer.word_index\n",
    "# data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "# X, y = data[:,:-1], data[:,-1]\n",
    "vocab_size = words_to_load+4\n",
    "# y = to_categorical(y, num_classes=vocab_size)\n",
    "# seq_length = X.shape[1]\n",
    " \n",
    "# define model\n",
    "model = Sequential()\n",
    "embedding_layer = Embedding(vocab_size,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[loaded_embeddings_ft_en],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH-1,\n",
    "                            trainable=False)\n",
    "model.add(embedding_layer)\n",
    "model.add(Bidirectional(GRU(100, return_sequences=True)))\n",
    "model.add(Bidirectional(GRU(100)))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(train_label.shape[1], activation='softmax'))\n",
    "\n",
    "#import the checkpoint to save current model\n",
    "filepath=\"GRU_1.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "earlystopper = EarlyStopping(monitor='val_loss', patience=10, verbose=1)\n",
    "callbacks_list = [checkpoint, earlystopper]\n",
    "# compile model\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "# fit the model\n",
    "model.fit(train_input, train_label, validation_data =(val_input, val_label), batch_size= 200, epochs=100, callbacks=callbacks_list)\n",
    "\n",
    " \n",
    "# save the model to file\n",
    "model.save('model.h5')\n",
    "# save the tokenizer\n",
    "plk.dump(tokenizer, open('tokenizer.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the layers\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, GRU\n",
    "from keras.layers.embeddings import Embedding\n",
    "word_dim = 300\n",
    "num_tokens = words_to_load+4\n",
    "word_vec_input = Input(shape=(word_dim,))\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "decoder_embed = Embedding(input_dim=num_tokens, output_dim=word_dim, mask_zero=True)\n",
    "decoder_gru_1 = GRU(word_dim, return_sequences=True, return_state=False)\n",
    "decoder_gru_2 = GRU(word_dim, return_sequences=True, return_state=True)\n",
    "decoder_dense = Dense(num_tokens, activation='softmax')\n",
    "\n",
    "# Connect the layers\n",
    "embedded = decoder_embed(decoder_inputs)\n",
    "gru_1_output = decoder_gru_1(embedded, initial_state=word_vec_input)\n",
    "gru_2_output, state_h = decoder_gru_2(gru_1_output)\n",
    "decoder_outputs = decoder_dense(gru_2_output)\n",
    "\n",
    "# Define the model that will be used for training\n",
    "training_model = Model([word_vec_input, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# Also create a model for inference (this returns the GRU state)\n",
    "decoder_model = Model([word_vec_input, decoder_inputs], [decoder_outputs, state_h])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, None, 300)    15001200    input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            (None, 300)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "gru_3 (GRU)                     (None, None, 300)    540900      embedding_3[0][0]                \n",
      "                                                                 input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "gru_4 (GRU)                     [(None, None, 300),  540900      gru_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, None, 50004)  15051204    gru_4[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 31,134,204\n",
      "Trainable params: 31,134,204\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "You must compile a model before training/testing. Use `model.compile(optimizer, loss)`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-56d669fb31c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdecoder_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m    953\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    679\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 681\u001b[0;31m                 raise RuntimeError('You must compile a model before '\n\u001b[0m\u001b[1;32m    682\u001b[0m                                    \u001b[0;34m'training/testing. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m                                    'Use `model.compile(optimizer, loss)`.')\n",
      "\u001b[0;31mRuntimeError\u001b[0m: You must compile a model before training/testing. Use `model.compile(optimizer, loss)`."
     ]
    }
   ],
   "source": [
    "decoder_model.fit(train_input, train_label, validation_data =(val_input, val_label), batch_size= 200, epochs=100, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from random import randint\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    " \n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    file = open(filename, 'r')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text\n",
    " \n",
    "# generate a sequence from a language model\n",
    "def generate_seq(model, tokenizer, seq_length, seed_text, n_words):\n",
    "    bleu_score = []\n",
    "    result = list()\n",
    "    in_text = seed_text\n",
    "    for _ in range(n_words):\n",
    "    # encode the text as integer\n",
    "        encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        # truncate sequences to a fixed length\n",
    "        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
    "        # predict probabilities for each word\n",
    "        yhat = model.predict_classes(encoded, verbose=0)\n",
    "        # map predicted word index to word\n",
    "        out_word = ''\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == yhat:\n",
    "                out_word = word\n",
    "                break\n",
    "    # append to input\n",
    "        in_text += ' ' + out_word\n",
    "        result.append(out_word)\n",
    "    seq = ' '.join(result)\n",
    "    bleu_score = sentence_bleu(seq, seed_text)\n",
    "    return seq, bleu_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "so given now we have more clarity with the tax reform do you think maybe for and beyond we might\n",
      "\n",
      "see the industry that you re seeing in the fed book or is it a little bit on the balance sheet and i m curious if you could give us a little bit of the balance sheet and i m wondering you could have any bit more color in the\n"
     ]
    }
   ],
   "source": [
    "#test 1\n",
    "# load cleaned text sequences\n",
    "in_filename = 'Balance sheet.txt'\n",
    "doc = load_doc(in_filename)\n",
    "lines = doc.split('\\n')\n",
    "seq_length = len(lines[0].split()) - 1\n",
    " \n",
    "# load the model\n",
    "model = load_model('GRU_1.hdf5')\n",
    " \n",
    "# load the tokenizer\n",
    "tokenizer = plk.load(open('tokenizer.pkl', 'rb'))\n",
    " \n",
    "# select a seed text\n",
    "seed_text = lines[randint(0,len(lines))]\n",
    "print(seed_text + '\\n')\n",
    " \n",
    "# generate new text\n",
    "generated, bleu = generate_seq(model, tokenizer, seq_length, seed_text, 50)\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09752505550693362"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial text:\n",
      "m just i guess wondering what the opportunity set for wells fargo could be if reg reform does come to\n",
      "\n",
      "Generated text:\n",
      "the industry that you ve been us in the fed hike to the industry that you re seeing more of the volatility of the industry that you ve been able\n"
     ]
    }
   ],
   "source": [
    "#test 1\n",
    "# load cleaned text sequences\n",
    "in_filename = 'Balance sheet.txt'\n",
    "doc = load_doc(in_filename)\n",
    "lines = doc.split('\\n')\n",
    "seq_length = len(lines[0].split()) - 1\n",
    " \n",
    "# load the model\n",
    "# model = load_model('model.h6')\n",
    "\n",
    "model.load_weights(filepath)\n",
    " \n",
    "# load the tokenizer\n",
    "tokenizer = plk.load(open('tokenizer.pkl', 'rb'))\n",
    " \n",
    "# select a seed text\n",
    "seed_text = lines[randint(0,len(lines))]\n",
    "print('Initial text:')\n",
    "print(seed_text + '\\n')\n",
    " \n",
    "# generate new text\n",
    "print('Generated text:')\n",
    "generated = generate_seq(model, tokenizer, seq_length, seed_text, 30)\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial text:\n",
      "to deal with a negative line when things are going well and then on securitized products that were up what\n",
      "\n",
      "Generated text:\n",
      "the fed funds years i know you could give us a little bit on the industry that you ve been able to see the volatility of the industry that you re seeing more of the volatility of the industry that you re seeing in the fed book and then that\n"
     ]
    }
   ],
   "source": [
    "#test 2\n",
    "\n",
    "# load the tokenizer\n",
    "tokenizer = plk.load(open('tokenizer.pkl', 'rb'))\n",
    " \n",
    "# select a seed text\n",
    "seed_text = lines[randint(0,len(lines))]\n",
    "print('Initial text:')\n",
    "print(seed_text + '\\n')\n",
    " \n",
    "# generate new text\n",
    "print('Generated text:')\n",
    "generated = generate_seq(model, tokenizer, seq_length, seed_text, 50)\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further improvement \n",
    "\n",
    "### 1. Improve the sequence build up based on each question, or expand training data to all questions from same category\n",
    "### 2. Tune parameter to see if the performace is better in generating questions (batch size, epoch time etc.)\n",
    "### 3. Research on the question generation process including tones and use of words\n",
    "### 4. Improve the dropout (0.2, 0.4 0.6 etc.) to LSTM input layer to prevent overfitting for the model and may generate a more natural language\n",
    "\n",
    "Dropout is a technique where randomly selected neurons are ignored during training. They are “dropped-out” randomly. This means that their contribution to the activation of downstream neurons is temporally removed on the forward pass and any weight updates are not applied to the neuron on the backward pass.\n",
    "paper about dropout\n",
    "http://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
