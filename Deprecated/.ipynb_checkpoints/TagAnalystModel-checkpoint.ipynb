{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.decomposition import NMF, PCA\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, make_scorer, accuracy_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('dark_background')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set for your computer\n",
    "data_directory = '/'.join(os.getcwd().split(\"/\")[:-1]) + '/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = [173,  74,  20, 101,  83,   1,  38,  39,  72,  50,  21, 164,  57,\n",
    "       169,   8,  63, 102,  34,  80, 192, 139,  88, 112, 116,  61,  46,\n",
    "        51, 165, 135,  89, 108,   7,  25,  15, 125,  93, 130,  71]\n",
    "\n",
    "def historicTagPct(var):\n",
    "    overall_hist_pct_list = [0 for _ in range(orig_data[var].unique().shape[0])]\n",
    "    hist_tag_cols = ['{}Hist_{}'.format(var, t) for t in tag_cols]\n",
    "\n",
    "    for c, (i, a_group) in enumerate(orig_data.groupby([var])):\n",
    "\n",
    "        subgroups = [(d, g) for d, g in a_group.groupby(['Date'])]\n",
    "        hist_pct_list = [pd.DataFrame(columns=[var, 'Date'] + hist_tag_cols) for _ in range(len(subgroups))]\n",
    "\n",
    "\n",
    "        for j, (d,g) in enumerate(subgroups):\n",
    "            if j == 0:\n",
    "                hist_pct_list[j] = hist_pct_list[j].append(pd.DataFrame({var:[i], 'Date':[subgroups[j][0]]}), sort=False).fillna(0)\n",
    "                continue\n",
    "\n",
    "            hist_data = pd.concat([s_g[1] for s_g in subgroups[:j]]).groupby(['Tag']).size().reset_index()\n",
    "            hist_data = hist_data.append(pd.DataFrame([(t, 0.0) for t in np.setdiff1d(tag_cols, hist_data['Tag'].values)], columns=['Tag', 0]), sort=False)\n",
    "            hist_data['TagPct'] = hist_data[0]/hist_data[0].sum()\n",
    "            hist_data['Date'] = d\n",
    "\n",
    "            hist_pivot = hist_data.pivot(index='Date', columns='Tag', values='TagPct').fillna(0).reset_index()\n",
    "            hist_pivot.columns = ['Date'] + [\"{}Hist_{}\".format(var, c) for c in hist_pivot.columns[1:]]\n",
    "            hist_pivot.index = [i]\n",
    "            hist_pivot = hist_pivot.rename_axis(None, axis=1).rename_axis(var).reset_index()\n",
    "            hist_pct_list[j] = hist_pct_list[j].append(hist_pivot, sort=False).fillna(0)\n",
    "\n",
    "        overall_hist_pct_list[c] = pd.concat(hist_pct_list)\n",
    "\n",
    "    overall_hist_pct = pd.concat(overall_hist_pct_list)\n",
    "    \n",
    "    return(overall_hist_pct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in data\n",
    "orig_data = pd.read_csv(data_directory + 'qaData.csv', parse_dates=['Date'])\n",
    "orig_data['EarningTag2'] = orig_data['EarningTag2'].str.strip()\n",
    "\n",
    "#Add Year and Month, Quarter from Data\n",
    "orig_data['Year'] = orig_data['Date'].dt.year\n",
    "orig_data['Month'] = orig_data['Date'].dt.month\n",
    "orig_data['Quarter'] = orig_data['Month'].apply(lambda x: 1 if x < 4 else 2 if x < 7 else 3 if x < 9 else 4)\n",
    "orig_data['Company'] = orig_data['Company'].str.title().str.replace(\" \", \"\")\n",
    "orig_data['EventType'] = orig_data['EventType'].str.title().str.replace(\" \", \"\")\n",
    "orig_data['Participants'] = orig_data['Participants'].str.title().str.replace(\" \", \"\")\n",
    "orig_data['AnalystName'] = orig_data['AnalystName'].str.title().str.replace(\" \", \"\")\n",
    "orig_data['AnalystCompany'] = orig_data['AnalystCompany'].str.title().str.replace(\" \", \"\")\n",
    "orig_data['Tag'] = orig_data['EarningTag2'].str.title().str.replace(\" \", \"\")\n",
    "\n",
    "orig_data = orig_data.loc[~orig_data['AnalystName'].isna()].copy()\n",
    "\n",
    "tag_cols = orig_data['Tag'].unique().tolist()\n",
    "\n",
    "#orig_data['MonthSin'] = np.sin((orig_data['Month']-1)*(2.*np.pi/12))\n",
    "#orig_data['MonthCos'] = np.cos((orig_data['Month']-1)*(2.*np.pi/12))\n",
    "\n",
    "#orig_data['QuarterSin'] = np.sin((orig_data['Quarter'])*(2.*np.pi/12))\n",
    "#orig_data['QuarterCos'] = np.cos((orig_data['Quarter'])*(2.*np.pi/12))\n",
    "\n",
    "#Pivot tag\n",
    "pivot_data = (pd.pivot_table(orig_data, index=['Company', 'Participants', 'AnalystName', 'AnalystCompany', 'Month', 'Year', 'Quarter', 'EventType', 'Date'], columns='Tag', aggfunc='size', fill_value=0)).reset_index()\n",
    "\n",
    "#Melt data\n",
    "pivot_melt_data = pd.melt(pivot_data, id_vars=['Company', 'Participants', 'AnalystName', 'AnalystCompany', 'Month', 'Year', 'Quarter', 'EventType', 'Date'], var_name='Tag', value_name='NumQ')\n",
    "#One-hot encode\n",
    "pivot_melt_data = pd.concat([pivot_melt_data, \n",
    "                             pd.get_dummies(pivot_melt_data['Company'], prefix='C', prefix_sep=\"_\"),\n",
    "                             pd.get_dummies(pivot_melt_data['AnalystName'], prefix='A', prefix_sep=\"_\"),\n",
    "                             pd.get_dummies(pivot_melt_data['EventType'], prefix='ET', prefix_sep=\"_\"),\n",
    "                             pd.get_dummies(pivot_melt_data['Tag'], prefix='T', prefix_sep=\"_\")], axis=1).reset_index(drop=True)\n",
    "\n",
    "#Analysts Present Data\n",
    "event_analyst_data = orig_data[['Company', 'Participants', 'AnalystName', 'AnalystCompany', 'Month', 'Year', 'Quarter', 'EventType', 'Date']].drop_duplicates().reset_index(drop=True)\n",
    "event_analyst_data = pd.concat([event_analyst_data, \n",
    "                                pd.get_dummies(event_analyst_data['AnalystName'], prefix='AP', prefix_sep=\"_\")], axis=1).drop(['AnalystName', 'AnalystCompany'], axis=1)\n",
    "event_analyst_data = event_analyst_data.groupby(['Company', 'Participants', 'Year', 'Month', 'Quarter', 'EventType', 'Date']).sum().reset_index()\n",
    "\n",
    "all_features_data = pd.merge(pivot_melt_data, event_analyst_data, on=['Company', 'Participants', 'Month', 'Year', 'Quarter', 'EventType', 'Date'])\n",
    "\n",
    "#Participants Present Data\n",
    "event_part_raw_data = orig_data[['Company', 'Participants', 'Month', 'Quarter', 'Year', 'EventType']].drop_duplicates().reset_index(drop=True)\n",
    "event_part_raw_data = pd.concat([event_part_raw_data, event_part_raw_data['Participants'].str.split(',', expand=True)], axis=1).drop('Participants', axis=1)\n",
    "\n",
    "event_part_melt_data = pd.melt(event_part_raw_data, id_vars=['Company', 'Month', 'Quarter', 'Year', 'EventType'], value_name='Participant').dropna().reset_index(drop=True)\n",
    "event_part_ohe_data = pd.concat([event_part_melt_data, pd.get_dummies(event_part_melt_data['Participant'], prefix='P', prefix_sep=\"_\")], axis=1).drop(['Participant', 'variable'], axis=1)\n",
    "event_part_data = event_part_ohe_data.groupby(['Company', 'Month', 'Quarter', 'Year', 'EventType']).sum().reset_index()\n",
    "\n",
    "all_features_data = pd.merge(all_features_data, event_part_data, on=['Company', 'Month', 'Year', 'Quarter', 'EventType'])\n",
    "\n",
    "for i in ['AnalystName','Company', 'Quarter']:\n",
    "\n",
    "    all_features_data = pd.merge(all_features_data, historicTagPct(i), on=[i, 'Date'])\n",
    "\n",
    "#Index Data\n",
    "groups = []\n",
    "for i, (name, group) in enumerate(all_features_data.groupby(['Company', 'Participants', 'Month', 'Year', 'Quarter', 'EventType', 'Date'])):\n",
    "    g2 = group.copy()\n",
    "    g2['EventNumber'] = i\n",
    "    groups.append(g2)\n",
    "    \n",
    "indexed_data = pd.concat(groups)\n",
    "\n",
    "#Merge\n",
    "indexed_data = indexed_data.drop(['Company', 'AnalystName', 'AnalystCompany', 'Participants', 'Tag', 'EventType', 'Date'], axis=1)\n",
    "indexed_data = indexed_data.reset_index(drop=True)\n",
    "indexed_data['NumQ'] = indexed_data['NumQ'].astype(bool).astype(int)\n",
    "\n",
    "train, test = indexed_data.loc[~indexed_data['EventNumber'].isin(test_set)].copy().reset_index(drop=True), \\\n",
    "                indexed_data.loc[indexed_data['EventNumber'].isin(test_set)].copy().reset_index(drop=True)\n",
    "\n",
    "X_train, y_train = train.drop(['NumQ','EventNumber'], axis=1), train['NumQ'].values\n",
    "X_test, y_test = test.drop(['NumQ', 'EventNumber'], axis=1), test['NumQ'].values\n",
    "\n",
    "cols = train.drop(['NumQ','EventNumber'], axis=1).columns    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_comp = 100\n",
    "\n",
    "scores = np.zeros(max_comp)\n",
    "acc = np.zeros(max_comp)\n",
    "scores_gbc = np.zeros(max_comp)\n",
    "acc_gbc = np.zeros(max_comp)\n",
    "scores_rf = np.zeros(max_comp)\n",
    "acc_rf = np.zeros(max_comp)\n",
    "\n",
    "estimator = LogisticRegression().fit(X_train, y_train)\n",
    "preds = estimator.predict_proba(X_test)[:,1]\n",
    "scores[0] = roc_auc_score(y_test, preds)\n",
    "acc[0] = accuracy_score(y_test, preds.round())\n",
    "    \n",
    "estimator_gbc = GradientBoostingClassifier(warm_start=True).fit(X_train, y_train)\n",
    "preds_gbc = estimator_gbc.predict_proba(X_test)[:,1]\n",
    "scores_gbc[0] = roc_auc_score(y_test, preds_gbc)\n",
    "acc_gbc[0] = accuracy_score(y_test, preds_gbc.round())\n",
    "    \n",
    "estimator_rf = RandomForestClassifier(warm_start=True).fit(X_train, y_train)\n",
    "preds_rf = estimator_rf.predict_proba(X_test)[:,1]\n",
    "scores_rf[0] = roc_auc_score(y_test, preds_rf)\n",
    "acc_rf[0] = accuracy_score(y_test, preds_rf.round())\n",
    "\n",
    "for comp in range(1, max_comp):\n",
    "    model = NMF(n_components=comp)\n",
    "    X_train_W = model.fit_transform(X_train)\n",
    "    X_test_W = model.transform(X_test)\n",
    "    \n",
    "    estimator = LogisticRegression().fit(X_train_W, y_train)\n",
    "    preds = estimator.predict_proba(X_test_W)[:,1]\n",
    "    scores[comp] = roc_auc_score(y_test, preds)\n",
    "    acc[comp] = accuracy_score(y_test, preds.round())\n",
    "    \n",
    "    estimator_gbc = GradientBoostingClassifier(warm_start=True).fit(X_train_W, y_train)\n",
    "    preds_gbc = estimator_gbc.predict_proba(X_test_W)[:,1]\n",
    "    scores_gbc[comp] = roc_auc_score(y_test, preds_gbc)\n",
    "    acc_gbc[comp] = accuracy_score(y_test, preds_gbc.round())\n",
    "    \n",
    "    estimator_rf = RandomForestClassifier(warm_start=True).fit(X_train_W, y_train)\n",
    "    preds_rf = estimator_rf.predict_proba(X_test_W)[:,1]\n",
    "    scores_rf[comp] = roc_auc_score(y_test, preds_rf)\n",
    "    acc_rf[comp] = accuracy_score(y_test, preds_rf.round())\n",
    "    \n",
    "\n",
    "print('logit ROC:', scores.max(), scores.argmax())\n",
    "print('logit ACC', acc.max(), acc.argmax())\n",
    "print('GBC ROC', scores_gbc.max(), scores_gbc.argmax())\n",
    "print('GBC ACC', acc_gbc.max(), acc_gbc.argmax())\n",
    "print('RF ROC', scores_rf.max(), scores_rf.argmax())\n",
    "print('RF ACC', acc_rf.max(), acc_rf.argmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSearchCV(GradientBoostingClassifier(warm_start=True), cv=5, param_grid={'learning_rate':10.0**np.arange(-3,0,1)}, return_train_score=False, scoring=make_scorer(roc_auc_score))\n",
    "grid.fit(X_train, y_train)\n",
    "print(grid.best_score_)\n",
    "best_learning_rate = grid.best_params_['learning_rate']\n",
    "\n",
    "grid = GridSearchCV(GradientBoostingClassifier(learning_rate=best_learning_rate,\n",
    "                                               warm_start=True), \n",
    "                    cv=5, \n",
    "                    param_grid={'min_samples_split':np.arange(2, 10, 2, dtype=int)}, \n",
    "                    return_train_score=False, scoring=make_scorer(roc_auc_score))\n",
    "grid.fit(X_train, y_train)\n",
    "print(grid.best_score_)\n",
    "best_min_samples_split = grid.best_params_['min_samples_split']\n",
    "\n",
    "grid = GridSearchCV(GradientBoostingClassifier(learning_rate=best_learning_rate,\n",
    "                                               min_samples_split=best_min_samples_split,\n",
    "                                               warm_start=True), \n",
    "                    cv=5, \n",
    "                    param_grid={'max_features':['auto', 'sqrt', 'log2']}, \n",
    "                    return_train_score=False, scoring=make_scorer(roc_auc_score))\n",
    "grid.fit(X_train, y_train)\n",
    "print(grid.best_score_)\n",
    "best_max_features = grid.best_params_['max_features']\n",
    "\n",
    "grid = GridSearchCV(GradientBoostingClassifier(learning_rate=best_learning_rate,\n",
    "                                               min_samples_split=best_min_samples_split,\n",
    "                                               max_features = best_max_features,\n",
    "                                               warm_start=True), \n",
    "                    cv=5, \n",
    "                    param_grid={'max_depth':np.arange(1, 10, 1, dtype=int)}, \n",
    "                    return_train_score=False, scoring=make_scorer(roc_auc_score))\n",
    "grid.fit(X_train, y_train)\n",
    "print(grid.best_score_)\n",
    "best_max_depth = grid.best_params_['max_depth']\n",
    "\n",
    "grid = GridSearchCV(GradientBoostingClassifier(learning_rate=best_learning_rate,\n",
    "                                               min_samples_split=best_min_samples_split,\n",
    "                                               max_features = best_max_features,\n",
    "                                               max_depth = best_max_depth,\n",
    "                                               warm_start=True), \n",
    "                    cv=5, \n",
    "                    param_grid={'min_samples_leaf':np.arange(1, 11, 2, dtype=int)}, \n",
    "                    return_train_score=False, scoring=make_scorer(roc_auc_score))\n",
    "grid.fit(X_train, y_train)\n",
    "print(grid.best_score_)\n",
    "best_min_samples_leaf = grid.best_params_['min_samples_leaf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NMF(n_components=44).fit(X_train)\n",
    "X_train_W = model.transform(X_train)\n",
    "\n",
    "\n",
    "grid = GridSearchCV(GradientBoostingClassifier(warm_start=True), cv=5, param_grid={'learning_rate':10.0**np.arange(-3,0,1)}, return_train_score=False, scoring=make_scorer(roc_auc_score))\n",
    "grid.fit(X_train_W, y_train)\n",
    "print(grid.best_score_)\n",
    "best_learning_rate = grid.best_params_['learning_rate']\n",
    "\n",
    "grid = GridSearchCV(GradientBoostingClassifier(learning_rate=best_learning_rate,\n",
    "                                               warm_start=True), \n",
    "                    cv=5, \n",
    "                    param_grid={'min_samples_split':np.arange(2, 10, 2, dtype=int)}, \n",
    "                    return_train_score=False, scoring=make_scorer(roc_auc_score))\n",
    "grid.fit(X_train_W, y_train)\n",
    "print(grid.best_score_)\n",
    "best_min_samples_split = grid.best_params_['min_samples_split']\n",
    "\n",
    "grid = GridSearchCV(GradientBoostingClassifier(learning_rate=best_learning_rate,\n",
    "                                               min_samples_split=best_min_samples_split,\n",
    "                                               warm_start=True), \n",
    "                    cv=5, \n",
    "                    param_grid={'max_features':['auto', 'sqrt', 'log2']}, \n",
    "                    return_train_score=False, scoring=make_scorer(roc_auc_score))\n",
    "grid.fit(X_train_W, y_train)\n",
    "print(grid.best_score_)\n",
    "best_max_features = grid.best_params_['max_features']\n",
    "\n",
    "grid = GridSearchCV(GradientBoostingClassifier(learning_rate=best_learning_rate,\n",
    "                                               min_samples_split=best_min_samples_split,\n",
    "                                               max_features = best_max_features,\n",
    "                                               warm_start=True), \n",
    "                    cv=5, \n",
    "                    param_grid={'max_depth':np.arange(1, 10, 1, dtype=int)}, \n",
    "                    return_train_score=False, scoring=make_scorer(roc_auc_score))\n",
    "grid.fit(X_train_W, y_train)\n",
    "print(grid.best_score_)\n",
    "best_max_depth = grid.best_params_['max_depth']\n",
    "\n",
    "grid = GridSearchCV(GradientBoostingClassifier(learning_rate=best_learning_rate,\n",
    "                                               min_samples_split=best_min_samples_split,\n",
    "                                               max_features = best_max_features,\n",
    "                                               max_depth = best_max_depth,\n",
    "                                               warm_start=True), \n",
    "                    cv=5, \n",
    "                    param_grid={'min_samples_leaf':np.arange(1, 11, 2, dtype=int)}, \n",
    "                    return_train_score=False, scoring=make_scorer(roc_auc_score))\n",
    "grid.fit(X_train_W, y_train)\n",
    "print(grid.best_score_)\n",
    "best_min_samples_leaf = grid.best_params_['min_samples_leaf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NMF(n_components=49).fit(X_train)\n",
    "X_train_W = model.transform(X_train)\n",
    "X_test_W = model.transform(X_test)\n",
    "\n",
    "estimator = GradientBoostingClassifier().fit(X_train_W, y_train)\n",
    "roc_auc_score(y_test, estimator.predict_proba(X_test_W)[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
