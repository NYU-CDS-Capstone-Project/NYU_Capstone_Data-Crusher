{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"pretrained_gru_final.ipynb","version":"0.3.2","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"QdVoPPDmZ73g","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":191},"outputId":"77f03053-bd57-4e1b-d686-f037ffb756af","executionInfo":{"status":"ok","timestamp":1544320828167,"user_tz":300,"elapsed":27890,"user":{"displayName":"Yiyan Chen","photoUrl":"","userId":"08805131014141803120"}}},"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive/')\n"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive/\n"],"name":"stdout"}]},{"metadata":{"id":"kVg5cQWEadzp","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":99},"outputId":"9b2191b5-f6fd-41ee-ae0a-fcf544d30255","executionInfo":{"status":"ok","timestamp":1544323697673,"user_tz":300,"elapsed":2561,"user":{"displayName":"Yiyan Chen","photoUrl":"","userId":"08805131014141803120"}}},"cell_type":"code","source":["!ls \"/content/drive/My Drive/capstone\""],"execution_count":52,"outputs":[{"output_type":"stream","text":[" combined_csv_forecast.csv   train_label.p   wiki-news-300d-1M.vec\n","'Q&A_Database_new.xlsx'      val_input.p\n"," train_input.p\t\t     val_label.p\n"],"name":"stdout"}]},{"metadata":{"id":"A9rpgFrtaoKp","colab_type":"code","colab":{}},"cell_type":"code","source":["!pip install -q keras"],"execution_count":0,"outputs":[]},{"metadata":{"id":"YFJjfhnjfXiT","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":201},"outputId":"61a044a1-bf55-4fc7-cb5e-ee1f2c732f80","executionInfo":{"status":"ok","timestamp":1544320856719,"user_tz":300,"elapsed":5346,"user":{"displayName":"Yiyan Chen","photoUrl":"","userId":"08805131014141803120"}}},"cell_type":"code","source":["!pip3 install sacrebleu"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Collecting sacrebleu\n","  Downloading https://files.pythonhosted.org/packages/37/51/bffea2b666d59d77be0413d35220022040a1f308c39009e5b023bc4eb8ab/sacrebleu-1.2.12.tar.gz\n","Requirement already satisfied: typing in /usr/local/lib/python3.6/dist-packages (from sacrebleu) (3.6.6)\n","Building wheels for collected packages: sacrebleu\n","  Running setup.py bdist_wheel for sacrebleu ... \u001b[?25l-\b \bdone\n","\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/ea/0a/7d/ddcbdcd15a04b72de1b3f78e7e754aab415aff81c423376385\n","Successfully built sacrebleu\n","Installing collected packages: sacrebleu\n","Successfully installed sacrebleu-1.2.12\n"],"name":"stdout"}]},{"metadata":{"id":"TsMRimlvfcr9","colab_type":"code","colab":{}},"cell_type":"code","source":["import numpy as np\n","from numpy import array\n","import pickle as plk\n","import pandas as pd\n","from keras.models import Sequential\n","from keras.layers import Dense, Bidirectional\n","from keras.layers import Dropout\n","from keras.layers import LSTM, GRU\n","from keras.callbacks import ModelCheckpoint, EarlyStopping\n","from keras.utils import np_utils\n","from keras.utils import to_categorical\n","from keras.layers import Embedding\n","from keras.preprocessing.text import Tokenizer\n","import os\n","from sacrebleu import sentence_bleu"],"execution_count":0,"outputs":[]},{"metadata":{"id":"GM4OdnbwgDKD","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":193},"outputId":"86438029-82fe-4e65-8963-eec838d0418a","executionInfo":{"status":"ok","timestamp":1544320875067,"user_tz":300,"elapsed":3778,"user":{"displayName":"Yiyan Chen","photoUrl":"","userId":"08805131014141803120"}}},"cell_type":"code","source":["!pip install xlrd"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Collecting xlrd\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/07/e6/e95c4eec6221bfd8528bcc4ea252a850bffcc4be88ebc367e23a1a84b0bb/xlrd-1.1.0-py2.py3-none-any.whl (108kB)\n","\r\u001b[K    9% |███                             | 10kB 18.3MB/s eta 0:00:01\r\u001b[K    18% |██████                          | 20kB 4.7MB/s eta 0:00:01\r\u001b[K    28% |█████████                       | 30kB 6.5MB/s eta 0:00:01\r\u001b[K    37% |████████████                    | 40kB 4.2MB/s eta 0:00:01\r\u001b[K    47% |███████████████                 | 51kB 5.1MB/s eta 0:00:01\r\u001b[K    56% |██████████████████              | 61kB 6.0MB/s eta 0:00:01\r\u001b[K    65% |█████████████████████           | 71kB 6.8MB/s eta 0:00:01\r\u001b[K    75% |████████████████████████        | 81kB 7.6MB/s eta 0:00:01\r\u001b[K    84% |███████████████████████████     | 92kB 6.1MB/s eta 0:00:01\r\u001b[K    94% |██████████████████████████████  | 102kB 6.7MB/s eta 0:00:01\r\u001b[K    100% |████████████████████████████████| 112kB 6.1MB/s \n","\u001b[?25hInstalling collected packages: xlrd\n","Successfully installed xlrd-1.1.0\n"],"name":"stdout"}]},{"metadata":{"id":"DA-rbv0hf9Se","colab_type":"code","colab":{}},"cell_type":"code","source":["path = os.getcwd()+ '/drive/My Drive/capstone'\n","df = pd.read_excel(path+'/Q&A_Database_new.xlsx','QA', skiprows=3)\n","df2 = pd.read_csv(path+ '/combined_csv_forecast.csv', encoding='latin-1')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"WO6pAiZ9f_le","colab_type":"code","colab":{}},"cell_type":"code","source":["df['Breakout'] = [x.strip() for x in df['Breakout'].values]\n","l = df['Breakout'].unique() # 14 unique categories\n","dic = {} #create dictionary for questions\n","for category in l:\n","  list_1 = list(df.loc[df['Breakout'] == category]['Question'])\n","  dic[category] = list_1\n","l2 = df2['sub_lvl'].unique()\n","dic_trans = {}\n","for x in l2:\n","    sub = x.split('-')[1]\n","    if sub != 'CIB' and sub!= 'CB' and sub!='CCB' and sub!='AWM':\n","        dic_trans[x] = sub[0]+sub.lower()[1:]\n","    else:\n","        dic_trans[x] = sub\n","#create dictionary for questions\n","for category in l2:\n","    cat = dic_trans[category]\n","    list_ = list(df2.loc[df2['sub_lvl']  == category]['Question'])\n","    dic[cat]+=list_"],"execution_count":0,"outputs":[]},{"metadata":{"id":"eyLwmah7isBn","colab_type":"code","colab":{}},"cell_type":"code","source":["import spacy\n","import re\n","import string\n","# Load English tokenizer, tagger, parser, NER and word vectors\n","\n","# tokenizer = spacy.load('en_core_web_sm')\n","punctuations = string.punctuation\n","\n","#tokenize sentence by sentence\n","def question_split(input_):\n","    list_ = []\n","    for q in input_:\n","        q = q.split('\\n')\n","        if len(q) ==1 and len(tokenize(q[0]))>= 10:\n","            list_.append(q)\n","        else:\n","            for i in range(len(q)):\n","                if len(tokenize(q[i])) >= 10:\n","                    list_.append(q[i])\n","    return list_\n","\n","def tokenize(sent):\n","#   sent = re.sub('[^A-Za-z&]', ' ', sent) # replace non-letter with space\n","#   sent = re.sub(r'\\b[a-zA-Z]\\b', '', sent) #remove single letter \n","    sent = re.sub('y ou', 'you', sent)\n","    sent = re.sub('y es', 'yes', sent)\n","    sent = re.sub('v o', 'vo', sent)\n","    sent = re.sub(\"don't\", 'dont', sent)\n","    sent = re.sub('[^A-Za-z&]', ' ', sent)\n","    sent = re.sub( r'([a-zA-Z])([,.!?])', r'\\1 \\2', sent)\n","#     tokens = tokenizer(sent)\n","    return sent.split()\n","\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ZNX43ejVizuo","colab_type":"code","colab":{}},"cell_type":"code","source":["#load training and validation dataset\n","CATEGORY = 'Balance sheet'\n","dic_len = len(dic[CATEGORY])\n","all_data = dic[CATEGORY]\n","split_val = int(dic_len*0.8)\n","train_data =dic[CATEGORY][:split_val]\n","val_data = dic[CATEGORY][split_val: ]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"lf8_Pxoxi2cy","colab_type":"code","colab":{}},"cell_type":"code","source":["# organize into sequences of tokens\n","def build_sequence(length, input_):\n","    input_ = ['<s>'] + input_ + ['</s>']\n","    if len(input_) < length:\n","        input_ = input_ + ['<pad>'] * (length - len(input_))\n","    sequences = list()\n","    for l in range(length, len(input_)+1):\n","        seq = input_[l-length:l]\n","        line = ' '.join(seq)\n","        sequences.append(line)\n","#     print('Total Sequences: {}' .format(len(sequences)))\n","    return sequences"],"execution_count":0,"outputs":[]},{"metadata":{"id":"az7pud5VjC9a","colab_type":"code","colab":{}},"cell_type":"code","source":["def data_generator(dic, length):\n","    out_list = []\n","    raw = question_split(dic)\n","    for i in range(len(raw)):\n","        sent = tokenize(raw[i][0].lower())\n","        if len(sent) >= 10:\n","            q_sequence = build_sequence(length, sent)\n","            out_list += q_sequence\n","    return out_list"],"execution_count":0,"outputs":[]},{"metadata":{"id":"B6-SxutKpGDn","colab_type":"code","colab":{}},"cell_type":"code","source":["all_data_words = []\n","for i in range(len(all_data)):\n","  sent = tokenize(all_data[i].lower())\n","  all_data_words += sent\n","all_data_words = list(set(all_data_words))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"waWg5fDtjEae","colab_type":"code","colab":{}},"cell_type":"code","source":["# add words not in the embeddings\n","words_to_load = 50000\n","PAD_IDX = 0\n","UNK_IDX = 1\n","SOS_IDX = 2\n","EOS_IDX = 3\n","import numpy as np\n","# reserve the 1st 2nd token for padding and <UNK> respectively\n","with open(path+'/wiki-news-300d-1M.vec') as f:\n","    loaded_embeddings_ft_en = np.zeros((words_to_load+4, 300))\n","    words_ft_en = {}\n","    idx2words_ft_en = {}\n","    ordered_words_ft_en = []\n","    ordered_words_ft_en.extend(['<pad>', '<unk>', '<s>', '</s>'])\n","    loaded_embeddings_ft_en[0,:] = np.zeros(300)\n","    loaded_embeddings_ft_en[1,:] = np.random.normal(size = 300)\n","    loaded_embeddings_ft_en[2,:] = np.random.normal(size = 300)\n","    loaded_embeddings_ft_en[3,:] = np.random.normal(size = 300)\n","    for i, line in enumerate(f):\n","        if i >= words_to_load: \n","            break\n","        s = line.split()\n","        loaded_embeddings_ft_en[i+4, :] = np.asarray(s[1:])\n","        words_ft_en[s[0]] = i+4\n","        idx2words_ft_en[i+4] = s[0]\n","        ordered_words_ft_en.append(s[0])\n","    length = len(np.setdiff1d(all_data_words, ordered_words_ft_en))\n","    tmp_embeddings = np.zeros((length, 300))\n","    for idx, word in enumerate(np.setdiff1d(all_data_words, ordered_words_ft_en)):\n","        words_ft_en[word] = idx+words_to_load+4\n","        idx2words_ft_en[idx+words_to_load+4] = word\n","        tmp_embeddings[idx, :] = np.random.normal(size = 300)\n","    loaded_embeddings_ft_en = np.concatenate((loaded_embeddings_ft_en, tmp_embeddings), axis = 0)\n","    words_ft_en['<pad>'] = PAD_IDX\n","    words_ft_en['<unk>'] = UNK_IDX\n","    words_ft_en['<s>'] = SOS_IDX\n","    words_ft_en['</s>'] = EOS_IDX\n","    idx2words_ft_en[PAD_IDX] = '<pad>'\n","    idx2words_ft_en[UNK_IDX] = '<unk>'\n","    idx2words_ft_en[SOS_IDX] = '<s>'\n","    idx2words_ft_en[EOS_IDX] = '</s>'\n","    ordered_words_ft_en = list(words_ft_en.keys())"],"execution_count":0,"outputs":[]},{"metadata":{"id":"zckTfH8kkuCl","colab_type":"code","colab":{}},"cell_type":"code","source":["import nltk\n","EMBEDDING_DIM = 300\n","MAX_SEQUENCE_LENGTH = 30"],"execution_count":0,"outputs":[]},{"metadata":{"id":"BH1d63qvk4Zd","colab_type":"code","colab":{}},"cell_type":"code","source":["all_d = data_generator(all_data,MAX_SEQUENCE_LENGTH)\n","train_d = data_generator(train_data,MAX_SEQUENCE_LENGTH)\n","val_d = data_generator(val_data,MAX_SEQUENCE_LENGTH)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"qiKjFifmlEbv","colab_type":"code","colab":{}},"cell_type":"code","source":["from keras.optimizers import RMSprop\n","def data_loader(data_input):\n","    X = []\n","    y = []\n","    for i in range(len(data_input)):\n","        X.append([words_ft_en[x] if x in ordered_words_ft_en else UNK_IDX for x in data_input[i].split()[:-1]])\n","        sub = data_input[i].split()[-1]\n","        if sub in ordered_words_ft_en:\n","            y.append(loaded_embeddings_ft_en[words_ft_en[sub]])\n","        else:\n","            y.append(loaded_embeddings_ft_en[UNK_IDX])\n","    return np.array(X), np.array(y)\n","# train_input, train_label =  data_loader(train_d)\n","# val_input, val_label =  data_loader(val_d)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"-H5SVX22tSfw","colab_type":"code","colab":{}},"cell_type":"code","source":["# plk.dump(train_input, open(path + '/train_input.p', 'wb'))\n","# plk.dump(train_label, open(path + '/train_label.p', 'wb'))\n","# plk.dump(val_input, open(path+ '/val_input.p', 'wb'))\n","# plk.dump(val_label, open(path +'/val_label.p', 'wb'))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"_d4Yq-xdtlPf","colab_type":"code","colab":{}},"cell_type":"code","source":["def save_file(lines, filename):\n","    data = '\\n'.join(lines)\n","    file = open(filename, 'w')\n","    file.write(data)\n","    file.close()\n","\n","out_filename = 'Balance sheet.txt'\n","save_file(all_data, out_filename)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"L_BqVmGWt0RQ","colab_type":"code","colab":{}},"cell_type":"code","source":["# load doc into memory\n","def load_file(filename):\n","    file = open(filename, 'r')\n","    text = file.read()\n","    file.close()\n","    return text\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"55cJfxEUvIaV","colab_type":"code","colab":{}},"cell_type":"code","source":["train_input = plk.load(open(path + '/train_input.p', 'rb'))\n","train_label = plk.load(open(path + '/train_label.p', 'rb'))\n","val_input = plk.load(open(path + '/val_input.p', 'rb'))\n","val_label = plk.load(open(path + '/val_label.p', 'rb'))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"nLOkXowpviyy","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"36ec873d-903f-4813-aa10-a9c229a8ab91","executionInfo":{"status":"ok","timestamp":1544305164631,"user_tz":300,"elapsed":346,"user":{"displayName":"Yiyan Chen","photoUrl":"","userId":"08805131014141803120"}}},"cell_type":"code","source":["train_label.shape[1]"],"execution_count":45,"outputs":[{"output_type":"execute_result","data":{"text/plain":["300"]},"metadata":{"tags":[]},"execution_count":45}]},{"metadata":{"id":"goK1JYz0k7tl","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":493},"outputId":"05394751-d67a-4555-8272-824700b00883"},"cell_type":"code","source":["# sequences = tokenizer.texts_to_sequences(lines)\n","# vocab_size = len(tokenizer.word_index) + 1\n","# MAX_NUM_WORDS = words_to_load\n","# tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n","# tokenizer.fit_on_texts(lines)\n","# sequences = tokenizer.texts_to_sequences(lines)\n","# word_index = tokenizer.word_index\n","# data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n","# X, y = data[:,:-1], data[:,-1]\n","vocab_size = loaded_embeddings_ft_en.shape[0]\n","# y = to_categorical(y, num_classes=vocab_size)\n","# seq_length = X.shape[1]\n"," \n","# define model\n","model = Sequential()\n","embedding_layer = Embedding(vocab_size,\n","                            EMBEDDING_DIM,\n","                            weights=[loaded_embeddings_ft_en],\n","                            input_length=MAX_SEQUENCE_LENGTH-1,\n","                            trainable=True)\n","model.add(embedding_layer)\n","model.add(Bidirectional(GRU(100, return_sequences=True)))\n","model.add(Bidirectional(GRU(100)))\n","model.add(Dense(100, activation='tanh'))\n","model.add(Dense(train_label.shape[1], activation='softmax'))\n","\n","#import the checkpoint to save current model\n","filepath=path+\"/GRU_1.hdf5\"\n","checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n","earlystopper = EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n","callbacks_list = [checkpoint, earlystopper]\n","# compile model\n","\n","# rms = RMSprop(lr=0.01)\n","model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","model.summary()\n","# fit the model\n","model.fit(train_input, train_label, validation_data =(val_input, val_label), batch_size= 200, epochs=100, callbacks=callbacks_list)\n","\n"," \n","# save the model to file\n","model.save(path+'/model_all.h5')\n","# save the tokenizer\n","plk.dump(tokenizer, open('tokenizer.pkl', 'wb'))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_2 (Embedding)      (None, 29, 300)           15193200  \n","_________________________________________________________________\n","bidirectional_3 (Bidirection (None, 29, 200)           240600    \n","_________________________________________________________________\n","bidirectional_4 (Bidirection (None, 200)               180600    \n","_________________________________________________________________\n","dense_3 (Dense)              (None, 100)               20100     \n","_________________________________________________________________\n","dense_4 (Dense)              (None, 300)               30300     \n","=================================================================\n","Total params: 15,664,800\n","Trainable params: 15,664,800\n","Non-trainable params: 0\n","_________________________________________________________________\n","Train on 129327 samples, validate on 39485 samples\n","Epoch 1/100\n","129327/129327 [==============================] - 195s 2ms/step - loss: -459.1612 - acc: 0.3789 - val_loss: -483.8609 - val_acc: 0.4453\n","\n","Epoch 00001: loss improved from inf to -459.16121, saving model to /content/drive/My Drive/capstone/GRU_1.hdf5\n","Epoch 2/100\n","129327/129327 [==============================] - 185s 1ms/step - loss: -484.7765 - acc: 0.4453 - val_loss: -485.5343 - val_acc: 0.4453\n","\n","Epoch 00002: loss improved from -459.16121 to -484.77649, saving model to /content/drive/My Drive/capstone/GRU_1.hdf5\n","Epoch 3/100\n"," 16400/129327 [==>...........................] - ETA: 2:30 - loss: -489.2988 - acc: 0.4456"],"name":"stdout"}]},{"metadata":{"id":"B5VUFsVIvHEE","colab_type":"code","colab":{}},"cell_type":"code","source":["from random import randint\n","from keras.models import load_model\n","from keras.preprocessing.sequence import pad_sequences\n"," \n","# load doc into memory\n","def load_doc(filename):\n","    file = open(filename, 'r')\n","    text = file.read()\n","    file.close()\n","    return text\n"," \n","# generate a sequence from a language model\n","def generate_seq(model, seed_text_idx, n_words):\n","    bleu_score = []\n","    result = list()\n","    target_text = lines[seed_text_idx]\n","    seed_text = ' '.join(target_text.split()[:10])\n","    target_text_test = ' '.join(target_text.split()[10:])\n","    in_text = seed_text\n","    \n","    for _ in range(n_words):\n","    # encode the text as integer\n","#         encoded = tokenizer.texts_to_sequences([in_text])[0]\n","        encoded = [words_ft_en[x] if x in ordered_words_ft_en else UNK_IDX for x in in_text.split()]\n","        # truncate sequences to a fixed length\n","        encoded = pad_sequences([encoded], maxlen=MAX_SEQUENCE_LENGTH-1, truncating='pre')\n","        # predict probabilities for each word\n","        yhat = model.predict_classes(encoded, verbose=0)\n","        # map predicted word index to word\n","        out_word = idx2words_ft_en[yhat[0]]\n","        if yhat[0] == EOS_IDX:\n","          break\n","    # append to input\n","        in_text += ' ' + out_word\n","        result.append(out_word)\n","    seq = ' '.join(result)\n","    ret_seq = seed_text + ' '+seq\n","    bleu_score = sentence_bleu(seq, target_text_test)\n","    return ret_seq, target_text, bleu_score"],"execution_count":0,"outputs":[]},{"metadata":{"id":"_p46F4FLsR3L","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":108},"outputId":"b1a7d97e-861d-42b3-9f2b-df625d39b7f8","executionInfo":{"status":"ok","timestamp":1544325488367,"user_tz":300,"elapsed":31896,"user":{"displayName":"Yiyan Chen","photoUrl":"","userId":"08805131014141803120"}}},"cell_type":"code","source":["#test 1\n","# load cleaned text sequences\n","in_filename = 'Balance sheet.txt'\n","doc = load_file(in_filename)\n","lines = ['<s> '+x+ ' </s>' for x in doc.split('\\n') if x != '']\n","# seq_length = len(lines[0].split()) - 1\n"," \n","# load the model\n","model = load_model(path+'/GRU_1.hdf5')\n"," \n","# # load the tokenizer\n","# tokenizer = plk.load(open('tokenizer.pkl', 'rb'))\n"," \n","# select a seed text\n","seed_text_idx = randint(0,len(lines))\n"," \n","# generate new text\n","generated, target, bleu = generate_seq(model, seed_text_idx, 50)\n","print(target)\n","print(generated)\n","print('BLEU score is: ', bleu)"],"execution_count":91,"outputs":[{"output_type":"stream","text":["<s> I appreciate that. And maybe one last one. Just curious, the AOCI jumped from $12B to $17B. I know spreads widened out a lot, but rates came down. Just curious if you can give us a little color behind the scenes?   </s>\n","<s> I appreciate that. And maybe one last one. Just research research research research research make make make make make does does does does does does does does research research by by by by by by by by by by by by by by by by by by by by by by by by by by by by by by\n","BLEU score is:  0.02062403823169552\n"],"name":"stdout"}]},{"metadata":{"id":"ywvHgzhOriBh","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}