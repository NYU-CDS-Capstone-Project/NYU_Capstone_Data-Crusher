{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import array\n",
    "import pickle as plk\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Bidirectional\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM, GRU\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Embedding\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_excel('/Users/luyin/Desktop/project/Q&A.xlsx',header = 0)\n",
    "df = pd.read_excel(os.getcwd()+'/Q&A_Database_new.xlsx','QA', skiprows=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l = df['Analyst name'].unique() # 79 unique analyst\n",
    "dic = {} #create dictionary for questions\n",
    "\n",
    "for name in l:\n",
    "    list_ = list(df.loc[df['Analyst name']  == name]['Question'])\n",
    "    dic[name] = list_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import re\n",
    "import string\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "\n",
    "tokenizer = spacy.load('en_core_web_sm')\n",
    "punctuations = string.punctuation\n",
    "\n",
    "def tokenize(sent):\n",
    "#   sent = re.sub('[^A-Za-z&]', ' ', sent) # replace non-letter with space\n",
    "#   sent = re.sub(r'\\b[a-zA-Z]\\b', '', sent) #remove single letter \n",
    "    sent = re.sub('^[0-9]+', '', sent)\n",
    "    tokens = tokenizer(sent)\n",
    "    return [(token.text.lower()) for token in tokens if (token.text.lower() not in punctuations and token.is_alpha and token.text.lower() not in {'\\xa0', ' ',\" \"})]\n",
    "\n",
    "# tokens = tokenize(\" going to hit them one way or another strong dollar did seem to have a huge impact and y ou, what are you doing? I'm 's what do you and me think or like apples and Apple is looking at buying and bought U.K. startup for $1 billion. '\\n' another sentence\")\n",
    "# for token in tokens:\n",
    "#     print (token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_analyst_q(name):\n",
    "    all_tokens = []\n",
    "    for q in dic[name]:\n",
    "        tokens = tokenize(q)\n",
    "        all_tokens += tokens\n",
    "    return all_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokens = tokenize_analyst_q('Glenn Schorr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12657"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_vocab(input_):\n",
    "    vocab = sorted(set(input_))\n",
    "    vocab_to_int = dict((c, i) for i, c in enumerate(vocab))\n",
    "    int_to_vocab = dict((i, c) for i, c in enumerate(vocab))\n",
    "    n_total = len(input_)\n",
    "    n_vocab = len(vocab)\n",
    "    print (\"Total Words: {}\".format(n_total))\n",
    "    print (\"Total Vocab: {}\".format(n_vocab))\n",
    "    return n_total, n_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# organize into sequences of tokens\n",
    "def build_sequence(length, input_):\n",
    "    sequences = list()\n",
    "    for l in range(length, len(input_)):\n",
    "        seq = input_[l-length:l]\n",
    "        line = ' '.join(seq)\n",
    "        sequences.append(line)\n",
    "    print('Total Sequences: {}' .format(len(sequences)))\n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Words: 12657\n",
      "Total Vocab: 1658\n",
      "Total Sequences: 12637\n"
     ]
    }
   ],
   "source": [
    "n_total, n_vocab = build_vocab(tokens)\n",
    "sequences = build_sequence(20, tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_file(lines, filename):\n",
    "    data = '\\n'.join(lines)\n",
    "    file = open(filename, 'w')\n",
    "    file.write(data)\n",
    "    file.close()\n",
    "\n",
    "out_filename = 'Glenn_Schorr_sequences.txt'\n",
    "save_file(sequences, out_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load doc into memory\n",
    "def load_file(filename):\n",
    "    file = open(filename, 'r')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text\n",
    " \n",
    "in_filename = 'Glenn_Schorr_sequences.txt'\n",
    "doc = load_file(in_filename)\n",
    "lines = doc.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(lines) #fit on texts\n",
    "sequences = tokenizer.texts_to_sequences(lines)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "sequences = array(sequences)\n",
    "X, y = sequences[:,:-1], sequences[:,-1]\n",
    "y = to_categorical(y, num_classes=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   1,  263,    6, ...,    2,   31,   21],\n",
       "       [ 263,    6,  237, ...,   31,   21,   59],\n",
       "       [   6,  237,   37, ...,   21,   59,  238],\n",
       "       ...,\n",
       "       [ 108,    4,   82, ...,   51, 1658,  329],\n",
       "       [   4,   82,    4, ..., 1658,  329,    4],\n",
       "       [  82,    4,  237, ...,  329,    4,   18]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_load = 50000\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "import numpy as np\n",
    "# reserve the 1st 2nd token for padding and <UNK> respectively\n",
    "with open('/Users/cyian/Desktop/NYU/FALL2018/DS-GA1001_NLP/hws/HW2/wiki-news-300d-1M.vec') as f:\n",
    "    loaded_embeddings_ft = np.zeros((words_to_load+2, 300))\n",
    "    words_ft = {}\n",
    "    idx2words_ft = {}\n",
    "    ordered_words_ft = []\n",
    "    ordered_words_ft.extend(['<pad>', '<unk>'])\n",
    "    loaded_embeddings_ft[0,:] = np.zeros(300)\n",
    "    loaded_embeddings_ft[1,:] = np.random.normal(size = 300)\n",
    "    for i, line in enumerate(f):\n",
    "        if i >= words_to_load: \n",
    "            break\n",
    "        s = line.split()\n",
    "        loaded_embeddings_ft[i+2, :] = np.asarray(s[1:])\n",
    "        words_ft[s[0]] = i+2\n",
    "        idx2words_ft[i+2] = s[0]\n",
    "        ordered_words_ft.append(s[0])\n",
    "    words_ft['<pad>'] = 0\n",
    "    words_ft['<unk>'] = 1\n",
    "    idx2words_ft[0] = '<pad>'\n",
    "    idx2words_ft[1] = '<unk>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_to_id(word_list):\n",
    "    return [words_ft[x] if x in ordered_words_ft else UNK_IDX for x in word_list.split()]\n",
    "def sent_to_id(sent_list):\n",
    "    return [word_to_id(x) for x in sent_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the performance in equities was great and you mentioned it pretty much across the board do you think there any',\n",
       " 'performance in equities was great and you mentioned it pretty much across the board do you think there any seasonality',\n",
       " 'in equities was great and you mentioned it pretty much across the board do you think there any seasonality any',\n",
       " 'equities was great and you mentioned it pretty much across the board do you think there any seasonality any one',\n",
       " 'was great and you mentioned it pretty much across the board do you think there any seasonality any one time',\n",
       " 'great and you mentioned it pretty much across the board do you think there any seasonality any one time events',\n",
       " 'and you mentioned it pretty much across the board do you think there any seasonality any one time events block',\n",
       " 'you mentioned it pretty much across the board do you think there any seasonality any one time events block trades',\n",
       " 'mentioned it pretty much across the board do you think there any seasonality any one time events block trades anything',\n",
       " 'it pretty much across the board do you think there any seasonality any one time events block trades anything like',\n",
       " 'pretty much across the board do you think there any seasonality any one time events block trades anything like that',\n",
       " 'much across the board do you think there any seasonality any one time events block trades anything like that that',\n",
       " 'across the board do you think there any seasonality any one time events block trades anything like that that would',\n",
       " 'the board do you think there any seasonality any one time events block trades anything like that that would lift',\n",
       " 'board do you think there any seasonality any one time events block trades anything like that that would lift such',\n",
       " 'do you think there any seasonality any one time events block trades anything like that that would lift such good',\n",
       " 'you think there any seasonality any one time events block trades anything like that that would lift such good performance',\n",
       " 'think there any seasonality any one time events block trades anything like that that would lift such good performance in',\n",
       " 'there any seasonality any one time events block trades anything like that that would lift such good performance in the',\n",
       " 'any seasonality any one time events block trades anything like that that would lift such good performance in the quarter',\n",
       " 'seasonality any one time events block trades anything like that that would lift such good performance in the quarter you',\n",
       " 'any one time events block trades anything like that that would lift such good performance in the quarter you and',\n",
       " 'one time events block trades anything like that that would lift such good performance in the quarter you and others',\n",
       " 'time events block trades anything like that that would lift such good performance in the quarter you and others have',\n",
       " 'events block trades anything like that that would lift such good performance in the quarter you and others have been',\n",
       " 'block trades anything like that that would lift such good performance in the quarter you and others have been talking',\n",
       " 'trades anything like that that would lift such good performance in the quarter you and others have been talking with',\n",
       " 'anything like that that would lift such good performance in the quarter you and others have been talking with your',\n",
       " 'like that that would lift such good performance in the quarter you and others have been talking with your prime',\n",
       " 'that that would lift such good performance in the quarter you and others have been talking with your prime brokerage',\n",
       " 'that would lift such good performance in the quarter you and others have been talking with your prime brokerage clients',\n",
       " 'would lift such good performance in the quarter you and others have been talking with your prime brokerage clients to',\n",
       " 'lift such good performance in the quarter you and others have been talking with your prime brokerage clients to help',\n",
       " 'such good performance in the quarter you and others have been talking with your prime brokerage clients to help improve',\n",
       " 'good performance in the quarter you and others have been talking with your prime brokerage clients to help improve roas',\n",
       " 'performance in the quarter you and others have been talking with your prime brokerage clients to help improve roas in',\n",
       " 'in the quarter you and others have been talking with your prime brokerage clients to help improve roas in the',\n",
       " 'the quarter you and others have been talking with your prime brokerage clients to help improve roas in the business',\n",
       " 'quarter you and others have been talking with your prime brokerage clients to help improve roas in the business is',\n",
       " 'you and others have been talking with your prime brokerage clients to help improve roas in the business is part',\n",
       " 'and others have been talking with your prime brokerage clients to help improve roas in the business is part of',\n",
       " 'others have been talking with your prime brokerage clients to help improve roas in the business is part of that',\n",
       " 'have been talking with your prime brokerage clients to help improve roas in the business is part of that flowing',\n",
       " 'been talking with your prime brokerage clients to help improve roas in the business is part of that flowing through',\n",
       " 'talking with your prime brokerage clients to help improve roas in the business is part of that flowing through in',\n",
       " 'with your prime brokerage clients to help improve roas in the business is part of that flowing through in just',\n",
       " 'your prime brokerage clients to help improve roas in the business is part of that flowing through in just better',\n",
       " 'prime brokerage clients to help improve roas in the business is part of that flowing through in just better equity',\n",
       " 'brokerage clients to help improve roas in the business is part of that flowing through in just better equity performance',\n",
       " 'clients to help improve roas in the business is part of that flowing through in just better equity performance more',\n",
       " 'to help improve roas in the business is part of that flowing through in just better equity performance more business',\n",
       " 'help improve roas in the business is part of that flowing through in just better equity performance more business with',\n",
       " 'improve roas in the business is part of that flowing through in just better equity performance more business with clients',\n",
       " 'roas in the business is part of that flowing through in just better equity performance more business with clients in',\n",
       " 'in the business is part of that flowing through in just better equity performance more business with clients in jamie',\n",
       " 'the business is part of that flowing through in just better equity performance more business with clients in jamie letter',\n",
       " 'business is part of that flowing through in just better equity performance more business with clients in jamie letter he',\n",
       " 'is part of that flowing through in just better equity performance more business with clients in jamie letter he talked',\n",
       " 'part of that flowing through in just better equity performance more business with clients in jamie letter he talked about',\n",
       " 'of that flowing through in just better equity performance more business with clients in jamie letter he talked about mentioning',\n",
       " 'that flowing through in just better equity performance more business with clients in jamie letter he talked about mentioning the',\n",
       " 'flowing through in just better equity performance more business with clients in jamie letter he talked about mentioning the need',\n",
       " 'through in just better equity performance more business with clients in jamie letter he talked about mentioning the need to',\n",
       " 'in just better equity performance more business with clients in jamie letter he talked about mentioning the need to push',\n",
       " 'just better equity performance more business with clients in jamie letter he talked about mentioning the need to push the',\n",
       " 'better equity performance more business with clients in jamie letter he talked about mentioning the need to push the new',\n",
       " 'equity performance more business with clients in jamie letter he talked about mentioning the need to push the new g',\n",
       " 'performance more business with clients in jamie letter he talked about mentioning the need to push the new g sib',\n",
       " 'more business with clients in jamie letter he talked about mentioning the need to push the new g sib rules',\n",
       " 'business with clients in jamie letter he talked about mentioning the need to push the new g sib rules to',\n",
       " 'with clients in jamie letter he talked about mentioning the need to push the new g sib rules to the',\n",
       " 'clients in jamie letter he talked about mentioning the need to push the new g sib rules to the product',\n",
       " 'in jamie letter he talked about mentioning the need to push the new g sib rules to the product and',\n",
       " 'jamie letter he talked about mentioning the need to push the new g sib rules to the product and the',\n",
       " 'letter he talked about mentioning the need to push the new g sib rules to the product and the client',\n",
       " 'he talked about mentioning the need to push the new g sib rules to the product and the client level',\n",
       " 'talked about mentioning the need to push the new g sib rules to the product and the client level and',\n",
       " 'about mentioning the need to push the new g sib rules to the product and the client level and it',\n",
       " 'mentioning the need to push the new g sib rules to the product and the client level and it piqued',\n",
       " 'the need to push the new g sib rules to the product and the client level and it piqued my',\n",
       " 'need to push the new g sib rules to the product and the client level and it piqued my curiosity',\n",
       " 'to push the new g sib rules to the product and the client level and it piqued my curiosity i',\n",
       " 'push the new g sib rules to the product and the client level and it piqued my curiosity i just',\n",
       " 'the new g sib rules to the product and the client level and it piqued my curiosity i just curious',\n",
       " 'new g sib rules to the product and the client level and it piqued my curiosity i just curious how',\n",
       " 'g sib rules to the product and the client level and it piqued my curiosity i just curious how different',\n",
       " 'sib rules to the product and the client level and it piqued my curiosity i just curious how different is',\n",
       " 'rules to the product and the client level and it piqued my curiosity i just curious how different is that',\n",
       " 'to the product and the client level and it piqued my curiosity i just curious how different is that from',\n",
       " 'the product and the client level and it piqued my curiosity i just curious how different is that from what',\n",
       " 'product and the client level and it piqued my curiosity i just curious how different is that from what you',\n",
       " 'and the client level and it piqued my curiosity i just curious how different is that from what you already',\n",
       " 'the client level and it piqued my curiosity i just curious how different is that from what you already done',\n",
       " 'client level and it piqued my curiosity i just curious how different is that from what you already done in',\n",
       " 'level and it piqued my curiosity i just curious how different is that from what you already done in other',\n",
       " 'and it piqued my curiosity i just curious how different is that from what you already done in other words',\n",
       " 'it piqued my curiosity i just curious how different is that from what you already done in other words each',\n",
       " 'piqued my curiosity i just curious how different is that from what you already done in other words each step',\n",
       " 'my curiosity i just curious how different is that from what you already done in other words each step of',\n",
       " 'curiosity i just curious how different is that from what you already done in other words each step of the',\n",
       " 'i just curious how different is that from what you already done in other words each step of the way',\n",
       " 'just curious how different is that from what you already done in other words each step of the way you',\n",
       " 'curious how different is that from what you already done in other words each step of the way you been',\n",
       " 'how different is that from what you already done in other words each step of the way you been early',\n",
       " 'different is that from what you already done in other words each step of the way you been early in',\n",
       " 'is that from what you already done in other words each step of the way you been early in adopting',\n",
       " 'that from what you already done in other words each step of the way you been early in adopting and',\n",
       " 'from what you already done in other words each step of the way you been early in adopting and pushing',\n",
       " 'what you already done in other words each step of the way you been early in adopting and pushing out',\n",
       " 'you already done in other words each step of the way you been early in adopting and pushing out to',\n",
       " 'already done in other words each step of the way you been early in adopting and pushing out to the',\n",
       " 'done in other words each step of the way you been early in adopting and pushing out to the desk',\n",
       " 'in other words each step of the way you been early in adopting and pushing out to the desk level',\n",
       " 'other words each step of the way you been early in adopting and pushing out to the desk level higher',\n",
       " 'words each step of the way you been early in adopting and pushing out to the desk level higher capital',\n",
       " 'each step of the way you been early in adopting and pushing out to the desk level higher capital charges',\n",
       " 'step of the way you been early in adopting and pushing out to the desk level higher capital charges does',\n",
       " 'of the way you been early in adopting and pushing out to the desk level higher capital charges does this',\n",
       " 'the way you been early in adopting and pushing out to the desk level higher capital charges does this just',\n",
       " 'way you been early in adopting and pushing out to the desk level higher capital charges does this just mean',\n",
       " 'you been early in adopting and pushing out to the desk level higher capital charges does this just mean more',\n",
       " 'been early in adopting and pushing out to the desk level higher capital charges does this just mean more of',\n",
       " 'early in adopting and pushing out to the desk level higher capital charges does this just mean more of the',\n",
       " 'in adopting and pushing out to the desk level higher capital charges does this just mean more of the same',\n",
       " 'adopting and pushing out to the desk level higher capital charges does this just mean more of the same meaning',\n",
       " 'and pushing out to the desk level higher capital charges does this just mean more of the same meaning higher',\n",
       " 'pushing out to the desk level higher capital charges does this just mean more of the same meaning higher capital',\n",
       " 'out to the desk level higher capital charges does this just mean more of the same meaning higher capital charge',\n",
       " 'to the desk level higher capital charges does this just mean more of the same meaning higher capital charge higher',\n",
       " 'the desk level higher capital charges does this just mean more of the same meaning higher capital charge higher capital',\n",
       " 'desk level higher capital charges does this just mean more of the same meaning higher capital charge higher capital charge',\n",
       " 'level higher capital charges does this just mean more of the same meaning higher capital charge higher capital charge or',\n",
       " 'higher capital charges does this just mean more of the same meaning higher capital charge higher capital charge or is',\n",
       " 'capital charges does this just mean more of the same meaning higher capital charge higher capital charge or is there',\n",
       " 'charges does this just mean more of the same meaning higher capital charge higher capital charge or is there something',\n",
       " 'does this just mean more of the same meaning higher capital charge higher capital charge or is there something different',\n",
       " 'this just mean more of the same meaning higher capital charge higher capital charge or is there something different there',\n",
       " 'just mean more of the same meaning higher capital charge higher capital charge or is there something different there that',\n",
       " 'mean more of the same meaning higher capital charge higher capital charge or is there something different there that you',\n",
       " 'more of the same meaning higher capital charge higher capital charge or is there something different there that you need',\n",
       " 'of the same meaning higher capital charge higher capital charge or is there something different there that you need to',\n",
       " 'the same meaning higher capital charge higher capital charge or is there something different there that you need to do',\n",
       " 'same meaning higher capital charge higher capital charge or is there something different there that you need to do a',\n",
       " 'meaning higher capital charge higher capital charge or is there something different there that you need to do a quick',\n",
       " 'higher capital charge higher capital charge or is there something different there that you need to do a quick question',\n",
       " 'capital charge higher capital charge or is there something different there that you need to do a quick question on',\n",
       " 'charge higher capital charge or is there something different there that you need to do a quick question on proposal',\n",
       " 'higher capital charge or is there something different there that you need to do a quick question on proposal eight',\n",
       " 'capital charge or is there something different there that you need to do a quick question on proposal eight in',\n",
       " 'charge or is there something different there that you need to do a quick question on proposal eight in the',\n",
       " 'or is there something different there that you need to do a quick question on proposal eight in the proxy',\n",
       " 'is there something different there that you need to do a quick question on proposal eight in the proxy i',\n",
       " 'there something different there that you need to do a quick question on proposal eight in the proxy i mean',\n",
       " 'something different there that you need to do a quick question on proposal eight in the proxy i mean i',\n",
       " 'different there that you need to do a quick question on proposal eight in the proxy i mean i do',\n",
       " 'there that you need to do a quick question on proposal eight in the proxy i mean i do think',\n",
       " 'that you need to do a quick question on proposal eight in the proxy i mean i do think the',\n",
       " 'you need to do a quick question on proposal eight in the proxy i mean i do think the regulators',\n",
       " 'need to do a quick question on proposal eight in the proxy i mean i do think the regulators want',\n",
       " 'to do a quick question on proposal eight in the proxy i mean i do think the regulators want it',\n",
       " 'do a quick question on proposal eight in the proxy i mean i do think the regulators want it to',\n",
       " 'a quick question on proposal eight in the proxy i mean i do think the regulators want it to happen',\n",
       " 'quick question on proposal eight in the proxy i mean i do think the regulators want it to happen i',\n",
       " 'question on proposal eight in the proxy i mean i do think the regulators want it to happen i do',\n",
       " 'on proposal eight in the proxy i mean i do think the regulators want it to happen i do think',\n",
       " 'proposal eight in the proxy i mean i do think the regulators want it to happen i do think it',\n",
       " 'eight in the proxy i mean i do think the regulators want it to happen i do think it should',\n",
       " 'in the proxy i mean i do think the regulators want it to happen i do think it should happen',\n",
       " 'the proxy i mean i do think the regulators want it to happen i do think it should happen i',\n",
       " 'proxy i mean i do think the regulators want it to happen i do think it should happen i assume',\n",
       " 'i mean i do think the regulators want it to happen i do think it should happen i assume e',\n",
       " 'mean i do think the regulators want it to happen i do think it should happen i assume e been',\n",
       " 'i do think the regulators want it to happen i do think it should happen i assume e been doing',\n",
       " 'do think the regulators want it to happen i do think it should happen i assume e been doing a',\n",
       " 'think the regulators want it to happen i do think it should happen i assume e been doing a lot',\n",
       " 'the regulators want it to happen i do think it should happen i assume e been doing a lot of',\n",
       " 'regulators want it to happen i do think it should happen i assume e been doing a lot of the',\n",
       " 'want it to happen i do think it should happen i assume e been doing a lot of the work',\n",
       " 'it to happen i do think it should happen i assume e been doing a lot of the work on',\n",
       " 'to happen i do think it should happen i assume e been doing a lot of the work on this',\n",
       " 'happen i do think it should happen i assume e been doing a lot of the work on this along',\n",
       " 'i do think it should happen i assume e been doing a lot of the work on this along the',\n",
       " 'do think it should happen i assume e been doing a lot of the work on this along the way',\n",
       " 'think it should happen i assume e been doing a lot of the work on this along the way for',\n",
       " 'it should happen i assume e been doing a lot of the work on this along the way for the',\n",
       " 'should happen i assume e been doing a lot of the work on this along the way for the last',\n",
       " 'happen i assume e been doing a lot of the work on this along the way for the last couple',\n",
       " 'i assume e been doing a lot of the work on this along the way for the last couple of',\n",
       " 'assume e been doing a lot of the work on this along the way for the last couple of y',\n",
       " 'e been doing a lot of the work on this along the way for the last couple of y ears',\n",
       " 'been doing a lot of the work on this along the way for the last couple of y ears but',\n",
       " 'doing a lot of the work on this along the way for the last couple of y ears but curious',\n",
       " 'a lot of the work on this along the way for the last couple of y ears but curious why',\n",
       " 'lot of the work on this along the way for the last couple of y ears but curious why the',\n",
       " 'of the work on this along the way for the last couple of y ears but curious why the board',\n",
       " 'the work on this along the way for the last couple of y ears but curious why the board is',\n",
       " 'work on this along the way for the last couple of y ears but curious why the board is so',\n",
       " 'on this along the way for the last couple of y ears but curious why the board is so against',\n",
       " 'this along the way for the last couple of y ears but curious why the board is so against shareholders',\n",
       " 'along the way for the last couple of y ears but curious why the board is so against shareholders v',\n",
       " 'the way for the last couple of y ears but curious why the board is so against shareholders v oting',\n",
       " 'way for the last couple of y ears but curious why the board is so against shareholders v oting for',\n",
       " 'for the last couple of y ears but curious why the board is so against shareholders v oting for it',\n",
       " 'the last couple of y ears but curious why the board is so against shareholders v oting for it and',\n",
       " 'last couple of y ears but curious why the board is so against shareholders v oting for it and what',\n",
       " 'couple of y ears but curious why the board is so against shareholders v oting for it and what you',\n",
       " 'of y ears but curious why the board is so against shareholders v oting for it and what you actually',\n",
       " 'y ears but curious why the board is so against shareholders v oting for it and what you actually have',\n",
       " 'ears but curious why the board is so against shareholders v oting for it and what you actually have to',\n",
       " 'but curious why the board is so against shareholders v oting for it and what you actually have to do',\n",
       " 'curious why the board is so against shareholders v oting for it and what you actually have to do if',\n",
       " 'why the board is so against shareholders v oting for it and what you actually have to do if it',\n",
       " 'the board is so against shareholders v oting for it and what you actually have to do if it does',\n",
       " 'board is so against shareholders v oting for it and what you actually have to do if it does get',\n",
       " 'is so against shareholders v oting for it and what you actually have to do if it does get the',\n",
       " 'so against shareholders v oting for it and what you actually have to do if it does get the y',\n",
       " 'against shareholders v oting for it and what you actually have to do if it does get the y es',\n",
       " 'shareholders v oting for it and what you actually have to do if it does get the y es v',\n",
       " 'v oting for it and what you actually have to do if it does get the y es v ote',\n",
       " 'oting for it and what you actually have to do if it does get the y es v ote on',\n",
       " 'for it and what you actually have to do if it does get the y es v ote on the',\n",
       " 'it and what you actually have to do if it does get the y es v ote on the increased',\n",
       " 'and what you actually have to do if it does get the y es v ote on the increased asset',\n",
       " 'what you actually have to do if it does get the y es v ote on the increased asset sensitivity',\n",
       " 'you actually have to do if it does get the y es v ote on the increased asset sensitivity it',\n",
       " 'actually have to do if it does get the y es v ote on the increased asset sensitivity it sounds',\n",
       " 'have to do if it does get the y es v ote on the increased asset sensitivity it sounds like',\n",
       " 'to do if it does get the y es v ote on the increased asset sensitivity it sounds like more',\n",
       " 'do if it does get the y es v ote on the increased asset sensitivity it sounds like more of',\n",
       " 'if it does get the y es v ote on the increased asset sensitivity it sounds like more of the',\n",
       " 'it does get the y es v ote on the increased asset sensitivity it sounds like more of the increased',\n",
       " 'does get the y es v ote on the increased asset sensitivity it sounds like more of the increased sensitivity',\n",
       " 'get the y es v ote on the increased asset sensitivity it sounds like more of the increased sensitivity came',\n",
       " 'the y es v ote on the increased asset sensitivity it sounds like more of the increased sensitivity came out',\n",
       " 'y es v ote on the increased asset sensitivity it sounds like more of the increased sensitivity came out on',\n",
       " 'es v ote on the increased asset sensitivity it sounds like more of the increased sensitivity came out on the',\n",
       " 'v ote on the increased asset sensitivity it sounds like more of the increased sensitivity came out on the long',\n",
       " 'ote on the increased asset sensitivity it sounds like more of the increased sensitivity came out on the long end',\n",
       " 'on the increased asset sensitivity it sounds like more of the increased sensitivity came out on the long end it',\n",
       " 'the increased asset sensitivity it sounds like more of the increased sensitivity came out on the long end it great',\n",
       " 'increased asset sensitivity it sounds like more of the increased sensitivity came out on the long end it great you',\n",
       " 'asset sensitivity it sounds like more of the increased sensitivity came out on the long end it great you make',\n",
       " 'sensitivity it sounds like more of the increased sensitivity came out on the long end it great you make a',\n",
       " 'it sounds like more of the increased sensitivity came out on the long end it great you make a lot',\n",
       " 'sounds like more of the increased sensitivity came out on the long end it great you make a lot more',\n",
       " 'like more of the increased sensitivity came out on the long end it great you make a lot more money',\n",
       " 'more of the increased sensitivity came out on the long end it great you make a lot more money if',\n",
       " 'of the increased sensitivity came out on the long end it great you make a lot more money if the',\n",
       " 'the increased sensitivity came out on the long end it great you make a lot more money if the curve',\n",
       " 'increased sensitivity came out on the long end it great you make a lot more money if the curve shifts',\n",
       " 'sensitivity came out on the long end it great you make a lot more money if the curve shifts up',\n",
       " 'came out on the long end it great you make a lot more money if the curve shifts up basis',\n",
       " 'out on the long end it great you make a lot more money if the curve shifts up basis points',\n",
       " 'on the long end it great you make a lot more money if the curve shifts up basis points but',\n",
       " 'the long end it great you make a lot more money if the curve shifts up basis points but a',\n",
       " 'long end it great you make a lot more money if the curve shifts up basis points but a lot',\n",
       " 'end it great you make a lot more money if the curve shifts up basis points but a lot of',\n",
       " 'it great you make a lot more money if the curve shifts up basis points but a lot of people',\n",
       " 'great you make a lot more money if the curve shifts up basis points but a lot of people are',\n",
       " 'you make a lot more money if the curve shifts up basis points but a lot of people are more',\n",
       " 'make a lot more money if the curve shifts up basis points but a lot of people are more fearful',\n",
       " 'a lot more money if the curve shifts up basis points but a lot of people are more fearful of',\n",
       " 'lot more money if the curve shifts up basis points but a lot of people are more fearful of what',\n",
       " 'more money if the curve shifts up basis points but a lot of people are more fearful of what happens',\n",
       " 'money if the curve shifts up basis points but a lot of people are more fearful of what happens if',\n",
       " 'if the curve shifts up basis points but a lot of people are more fearful of what happens if we',\n",
       " 'the curve shifts up basis points but a lot of people are more fearful of what happens if we get',\n",
       " 'curve shifts up basis points but a lot of people are more fearful of what happens if we get into',\n",
       " 'shifts up basis points but a lot of people are more fearful of what happens if we get into a',\n",
       " 'up basis points but a lot of people are more fearful of what happens if we get into a flattener',\n",
       " 'basis points but a lot of people are more fearful of what happens if we get into a flattener environment',\n",
       " 'points but a lot of people are more fearful of what happens if we get into a flattener environment is',\n",
       " 'but a lot of people are more fearful of what happens if we get into a flattener environment is it',\n",
       " 'a lot of people are more fearful of what happens if we get into a flattener environment is it as',\n",
       " 'lot of people are more fearful of what happens if we get into a flattener environment is it as simple',\n",
       " 'of people are more fearful of what happens if we get into a flattener environment is it as simple as',\n",
       " 'people are more fearful of what happens if we get into a flattener environment is it as simple as you',\n",
       " 'are more fearful of what happens if we get into a flattener environment is it as simple as you capture',\n",
       " 'more fearful of what happens if we get into a flattener environment is it as simple as you capture at',\n",
       " 'fearful of what happens if we get into a flattener environment is it as simple as you capture at least',\n",
       " 'of what happens if we get into a flattener environment is it as simple as you capture at least half',\n",
       " 'what happens if we get into a flattener environment is it as simple as you capture at least half the',\n",
       " 'happens if we get into a flattener environment is it as simple as you capture at least half the benefit',\n",
       " 'if we get into a flattener environment is it as simple as you capture at least half the benefit by',\n",
       " 'we get into a flattener environment is it as simple as you capture at least half the benefit by short',\n",
       " 'get into a flattener environment is it as simple as you capture at least half the benefit by short rates',\n",
       " 'into a flattener environment is it as simple as you capture at least half the benefit by short rates going',\n",
       " 'a flattener environment is it as simple as you capture at least half the benefit by short rates going up',\n",
       " 'flattener environment is it as simple as you capture at least half the benefit by short rates going up and',\n",
       " 'environment is it as simple as you capture at least half the benefit by short rates going up and if',\n",
       " 'is it as simple as you capture at least half the benefit by short rates going up and if we',\n",
       " 'it as simple as you capture at least half the benefit by short rates going up and if we in',\n",
       " 'as simple as you capture at least half the benefit by short rates going up and if we in a',\n",
       " 'simple as you capture at least half the benefit by short rates going up and if we in a flattener',\n",
       " 'as you capture at least half the benefit by short rates going up and if we in a flattener that',\n",
       " 'you capture at least half the benefit by short rates going up and if we in a flattener that where',\n",
       " 'capture at least half the benefit by short rates going up and if we in a flattener that where it',\n",
       " 'at least half the benefit by short rates going up and if we in a flattener that where it ends',\n",
       " 'least half the benefit by short rates going up and if we in a flattener that where it ends so',\n",
       " 'half the benefit by short rates going up and if we in a flattener that where it ends so i',\n",
       " 'the benefit by short rates going up and if we in a flattener that where it ends so i liked',\n",
       " 'benefit by short rates going up and if we in a flattener that where it ends so i liked the',\n",
       " 'by short rates going up and if we in a flattener that where it ends so i liked the improvement',\n",
       " 'short rates going up and if we in a flattener that where it ends so i liked the improvement on',\n",
       " 'rates going up and if we in a flattener that where it ends so i liked the improvement on the',\n",
       " 'going up and if we in a flattener that where it ends so i liked the improvement on the efficiency',\n",
       " 'up and if we in a flattener that where it ends so i liked the improvement on the efficiency ratio',\n",
       " 'and if we in a flattener that where it ends so i liked the improvement on the efficiency ratio targets',\n",
       " 'if we in a flattener that where it ends so i liked the improvement on the efficiency ratio targets and',\n",
       " 'we in a flattener that where it ends so i liked the improvement on the efficiency ratio targets and i',\n",
       " 'in a flattener that where it ends so i liked the improvement on the efficiency ratio targets and i liked',\n",
       " 'a flattener that where it ends so i liked the improvement on the efficiency ratio targets and i liked the',\n",
       " 'flattener that where it ends so i liked the improvement on the efficiency ratio targets and i liked the commitmentto',\n",
       " 'that where it ends so i liked the improvement on the efficiency ratio targets and i liked the commitmentto the',\n",
       " 'where it ends so i liked the improvement on the efficiency ratio targets and i liked the commitmentto the target',\n",
       " 'it ends so i liked the improvement on the efficiency ratio targets and i liked the commitmentto the target ranges',\n",
       " 'ends so i liked the improvement on the efficiency ratio targets and i liked the commitmentto the target ranges just',\n",
       " 'so i liked the improvement on the efficiency ratio targets and i liked the commitmentto the target ranges just a',\n",
       " 'i liked the improvement on the efficiency ratio targets and i liked the commitmentto the target ranges just a quick',\n",
       " 'liked the improvement on the efficiency ratio targets and i liked the commitmentto the target ranges just a quick curious',\n",
       " 'the improvement on the efficiency ratio targets and i liked the commitmentto the target ranges just a quick curious question',\n",
       " 'improvement on the efficiency ratio targets and i liked the commitmentto the target ranges just a quick curious question on',\n",
       " 'on the efficiency ratio targets and i liked the commitmentto the target ranges just a quick curious question on i',\n",
       " 'the efficiency ratio targets and i liked the commitmentto the target ranges just a quick curious question on i do',\n",
       " 'efficiency ratio targets and i liked the commitmentto the target ranges just a quick curious question on i do think',\n",
       " 'ratio targets and i liked the commitmentto the target ranges just a quick curious question on i do think of',\n",
       " 'targets and i liked the commitmentto the target ranges just a quick curious question on i do think of y',\n",
       " 'and i liked the commitmentto the target ranges just a quick curious question on i do think of y ou',\n",
       " 'i liked the commitmentto the target ranges just a quick curious question on i do think of y ou as',\n",
       " 'liked the commitmentto the target ranges just a quick curious question on i do think of y ou as the',\n",
       " 'the commitmentto the target ranges just a quick curious question on i do think of y ou as the biggest',\n",
       " 'commitmentto the target ranges just a quick curious question on i do think of y ou as the biggest rate',\n",
       " 'the target ranges just a quick curious question on i do think of y ou as the biggest rate beneficiary',\n",
       " 'target ranges just a quick curious question on i do think of y ou as the biggest rate beneficiary but',\n",
       " 'ranges just a quick curious question on i do think of y ou as the biggest rate beneficiary but it',\n",
       " 'just a quick curious question on i do think of y ou as the biggest rate beneficiary but it helps',\n",
       " 'a quick curious question on i do think of y ou as the biggest rate beneficiary but it helps but',\n",
       " 'quick curious question on i do think of y ou as the biggest rate beneficiary but it helps but what',\n",
       " 'curious question on i do think of y ou as the biggest rate beneficiary but it helps but what ty',\n",
       " 'question on i do think of y ou as the biggest rate beneficiary but it helps but what ty pe',\n",
       " 'on i do think of y ou as the biggest rate beneficiary but it helps but what ty pe of',\n",
       " 'i do think of y ou as the biggest rate beneficiary but it helps but what ty pe of rate',\n",
       " 'do think of y ou as the biggest rate beneficiary but it helps but what ty pe of rate scenario',\n",
       " 'think of y ou as the biggest rate beneficiary but it helps but what ty pe of rate scenario do',\n",
       " 'of y ou as the biggest rate beneficiary but it helps but what ty pe of rate scenario do y',\n",
       " 'y ou as the biggest rate beneficiary but it helps but what ty pe of rate scenario do y ou',\n",
       " 'ou as the biggest rate beneficiary but it helps but what ty pe of rate scenario do y ou have',\n",
       " 'as the biggest rate beneficiary but it helps but what ty pe of rate scenario do y ou have embedded',\n",
       " 'the biggest rate beneficiary but it helps but what ty pe of rate scenario do y ou have embedded in',\n",
       " 'biggest rate beneficiary but it helps but what ty pe of rate scenario do y ou have embedded in there',\n",
       " 'rate beneficiary but it helps but what ty pe of rate scenario do y ou have embedded in there or',\n",
       " 'beneficiary but it helps but what ty pe of rate scenario do y ou have embedded in there or i',\n",
       " 'but it helps but what ty pe of rate scenario do y ou have embedded in there or i kind',\n",
       " 'it helps but what ty pe of rate scenario do y ou have embedded in there or i kind of',\n",
       " 'helps but what ty pe of rate scenario do y ou have embedded in there or i kind of liked',\n",
       " 'but what ty pe of rate scenario do y ou have embedded in there or i kind of liked y',\n",
       " 'what ty pe of rate scenario do y ou have embedded in there or i kind of liked y our',\n",
       " 'ty pe of rate scenario do y ou have embedded in there or i kind of liked y our tone',\n",
       " 'pe of rate scenario do y ou have embedded in there or i kind of liked y our tone from',\n",
       " 'of rate scenario do y ou have embedded in there or i kind of liked y our tone from last',\n",
       " 'rate scenario do y ou have embedded in there or i kind of liked y our tone from last quarter',\n",
       " 'scenario do y ou have embedded in there or i kind of liked y our tone from last quarter of',\n",
       " 'do y ou have embedded in there or i kind of liked y our tone from last quarter of we',\n",
       " 'y ou have embedded in there or i kind of liked y our tone from last quarter of we going',\n",
       " 'ou have embedded in there or i kind of liked y our tone from last quarter of we going to',\n",
       " 'have embedded in there or i kind of liked y our tone from last quarter of we going to hit',\n",
       " 'embedded in there or i kind of liked y our tone from last quarter of we going to hit them',\n",
       " 'in there or i kind of liked y our tone from last quarter of we going to hit them one',\n",
       " 'there or i kind of liked y our tone from last quarter of we going to hit them one way',\n",
       " 'or i kind of liked y our tone from last quarter of we going to hit them one way or',\n",
       " 'i kind of liked y our tone from last quarter of we going to hit them one way or another',\n",
       " 'kind of liked y our tone from last quarter of we going to hit them one way or another so',\n",
       " 'of liked y our tone from last quarter of we going to hit them one way or another so i',\n",
       " 'liked y our tone from last quarter of we going to hit them one way or another so i liked',\n",
       " 'y our tone from last quarter of we going to hit them one way or another so i liked the',\n",
       " 'our tone from last quarter of we going to hit them one way or another so i liked the improvement',\n",
       " 'tone from last quarter of we going to hit them one way or another so i liked the improvement on',\n",
       " 'from last quarter of we going to hit them one way or another so i liked the improvement on the',\n",
       " 'last quarter of we going to hit them one way or another so i liked the improvement on the efficiency',\n",
       " 'quarter of we going to hit them one way or another so i liked the improvement on the efficiency ratio',\n",
       " 'of we going to hit them one way or another so i liked the improvement on the efficiency ratio targets',\n",
       " 'we going to hit them one way or another so i liked the improvement on the efficiency ratio targets and',\n",
       " 'going to hit them one way or another so i liked the improvement on the efficiency ratio targets and i',\n",
       " 'to hit them one way or another so i liked the improvement on the efficiency ratio targets and i liked',\n",
       " 'hit them one way or another so i liked the improvement on the efficiency ratio targets and i liked the',\n",
       " 'them one way or another so i liked the improvement on the efficiency ratio targets and i liked the commitmentto',\n",
       " 'one way or another so i liked the improvement on the efficiency ratio targets and i liked the commitmentto the',\n",
       " 'way or another so i liked the improvement on the efficiency ratio targets and i liked the commitmentto the target',\n",
       " 'or another so i liked the improvement on the efficiency ratio targets and i liked the commitmentto the target ranges',\n",
       " 'another so i liked the improvement on the efficiency ratio targets and i liked the commitmentto the target ranges just',\n",
       " 'so i liked the improvement on the efficiency ratio targets and i liked the commitmentto the target ranges just a',\n",
       " 'i liked the improvement on the efficiency ratio targets and i liked the commitmentto the target ranges just a quick',\n",
       " 'liked the improvement on the efficiency ratio targets and i liked the commitmentto the target ranges just a quick curious',\n",
       " 'the improvement on the efficiency ratio targets and i liked the commitmentto the target ranges just a quick curious question',\n",
       " 'improvement on the efficiency ratio targets and i liked the commitmentto the target ranges just a quick curious question on',\n",
       " 'on the efficiency ratio targets and i liked the commitmentto the target ranges just a quick curious question on i',\n",
       " 'the efficiency ratio targets and i liked the commitmentto the target ranges just a quick curious question on i do',\n",
       " 'efficiency ratio targets and i liked the commitmentto the target ranges just a quick curious question on i do think',\n",
       " 'ratio targets and i liked the commitmentto the target ranges just a quick curious question on i do think of',\n",
       " 'targets and i liked the commitmentto the target ranges just a quick curious question on i do think of y',\n",
       " 'and i liked the commitmentto the target ranges just a quick curious question on i do think of y ou',\n",
       " 'i liked the commitmentto the target ranges just a quick curious question on i do think of y ou as',\n",
       " 'liked the commitmentto the target ranges just a quick curious question on i do think of y ou as the',\n",
       " 'the commitmentto the target ranges just a quick curious question on i do think of y ou as the biggest',\n",
       " 'commitmentto the target ranges just a quick curious question on i do think of y ou as the biggest rate',\n",
       " 'the target ranges just a quick curious question on i do think of y ou as the biggest rate beneficiary',\n",
       " 'target ranges just a quick curious question on i do think of y ou as the biggest rate beneficiary but',\n",
       " 'ranges just a quick curious question on i do think of y ou as the biggest rate beneficiary but it',\n",
       " 'just a quick curious question on i do think of y ou as the biggest rate beneficiary but it helps',\n",
       " 'a quick curious question on i do think of y ou as the biggest rate beneficiary but it helps but',\n",
       " 'quick curious question on i do think of y ou as the biggest rate beneficiary but it helps but what',\n",
       " 'curious question on i do think of y ou as the biggest rate beneficiary but it helps but what ty',\n",
       " 'question on i do think of y ou as the biggest rate beneficiary but it helps but what ty pe',\n",
       " 'on i do think of y ou as the biggest rate beneficiary but it helps but what ty pe of',\n",
       " 'i do think of y ou as the biggest rate beneficiary but it helps but what ty pe of rate',\n",
       " 'do think of y ou as the biggest rate beneficiary but it helps but what ty pe of rate scenario',\n",
       " 'think of y ou as the biggest rate beneficiary but it helps but what ty pe of rate scenario do',\n",
       " 'of y ou as the biggest rate beneficiary but it helps but what ty pe of rate scenario do y',\n",
       " 'y ou as the biggest rate beneficiary but it helps but what ty pe of rate scenario do y ou',\n",
       " 'ou as the biggest rate beneficiary but it helps but what ty pe of rate scenario do y ou have',\n",
       " 'as the biggest rate beneficiary but it helps but what ty pe of rate scenario do y ou have embedded',\n",
       " 'the biggest rate beneficiary but it helps but what ty pe of rate scenario do y ou have embedded in',\n",
       " 'biggest rate beneficiary but it helps but what ty pe of rate scenario do y ou have embedded in there',\n",
       " 'rate beneficiary but it helps but what ty pe of rate scenario do y ou have embedded in there or',\n",
       " 'beneficiary but it helps but what ty pe of rate scenario do y ou have embedded in there or i',\n",
       " 'but it helps but what ty pe of rate scenario do y ou have embedded in there or i kind',\n",
       " 'it helps but what ty pe of rate scenario do y ou have embedded in there or i kind of',\n",
       " 'helps but what ty pe of rate scenario do y ou have embedded in there or i kind of liked',\n",
       " 'but what ty pe of rate scenario do y ou have embedded in there or i kind of liked y',\n",
       " 'what ty pe of rate scenario do y ou have embedded in there or i kind of liked y our',\n",
       " 'ty pe of rate scenario do y ou have embedded in there or i kind of liked y our tone',\n",
       " 'pe of rate scenario do y ou have embedded in there or i kind of liked y our tone from',\n",
       " 'of rate scenario do y ou have embedded in there or i kind of liked y our tone from last',\n",
       " 'rate scenario do y ou have embedded in there or i kind of liked y our tone from last quarter',\n",
       " 'scenario do y ou have embedded in there or i kind of liked y our tone from last quarter of',\n",
       " 'do y ou have embedded in there or i kind of liked y our tone from last quarter of we',\n",
       " 'y ou have embedded in there or i kind of liked y our tone from last quarter of we going',\n",
       " 'ou have embedded in there or i kind of liked y our tone from last quarter of we going to',\n",
       " 'have embedded in there or i kind of liked y our tone from last quarter of we going to hit',\n",
       " 'embedded in there or i kind of liked y our tone from last quarter of we going to hit them',\n",
       " 'in there or i kind of liked y our tone from last quarter of we going to hit them one',\n",
       " 'there or i kind of liked y our tone from last quarter of we going to hit them one way',\n",
       " 'or i kind of liked y our tone from last quarter of we going to hit them one way or',\n",
       " 'i kind of liked y our tone from last quarter of we going to hit them one way or another',\n",
       " 'kind of liked y our tone from last quarter of we going to hit them one way or another strong',\n",
       " 'of liked y our tone from last quarter of we going to hit them one way or another strong dollar',\n",
       " 'liked y our tone from last quarter of we going to hit them one way or another strong dollar did',\n",
       " 'y our tone from last quarter of we going to hit them one way or another strong dollar did seem',\n",
       " 'our tone from last quarter of we going to hit them one way or another strong dollar did seem to',\n",
       " 'tone from last quarter of we going to hit them one way or another strong dollar did seem to have',\n",
       " 'from last quarter of we going to hit them one way or another strong dollar did seem to have a',\n",
       " 'last quarter of we going to hit them one way or another strong dollar did seem to have a huge',\n",
       " 'quarter of we going to hit them one way or another strong dollar did seem to have a huge impact',\n",
       " 'of we going to hit them one way or another strong dollar did seem to have a huge impact and',\n",
       " 'we going to hit them one way or another strong dollar did seem to have a huge impact and y',\n",
       " 'going to hit them one way or another strong dollar did seem to have a huge impact and y ou',\n",
       " 'to hit them one way or another strong dollar did seem to have a huge impact and y ou broke',\n",
       " 'hit them one way or another strong dollar did seem to have a huge impact and y ou broke out',\n",
       " 'them one way or another strong dollar did seem to have a huge impact and y ou broke out for',\n",
       " 'one way or another strong dollar did seem to have a huge impact and y ou broke out for us',\n",
       " 'way or another strong dollar did seem to have a huge impact and y ou broke out for us all',\n",
       " 'or another strong dollar did seem to have a huge impact and y ou broke out for us all the',\n",
       " 'another strong dollar did seem to have a huge impact and y ou broke out for us all the places',\n",
       " 'strong dollar did seem to have a huge impact and y ou broke out for us all the places it',\n",
       " 'dollar did seem to have a huge impact and y ou broke out for us all the places it hit',\n",
       " 'did seem to have a huge impact and y ou broke out for us all the places it hit both',\n",
       " 'seem to have a huge impact and y ou broke out for us all the places it hit both revenues',\n",
       " 'to have a huge impact and y ou broke out for us all the places it hit both revenues and',\n",
       " 'have a huge impact and y ou broke out for us all the places it hit both revenues and expenses',\n",
       " 'a huge impact and y ou broke out for us all the places it hit both revenues and expenses it',\n",
       " 'huge impact and y ou broke out for us all the places it hit both revenues and expenses it also',\n",
       " 'impact and y ou broke out for us all the places it hit both revenues and expenses it also impacts',\n",
       " 'and y ou broke out for us all the places it hit both revenues and expenses it also impacts the',\n",
       " 'y ou broke out for us all the places it hit both revenues and expenses it also impacts the g',\n",
       " 'ou broke out for us all the places it hit both revenues and expenses it also impacts the g sibbuffer',\n",
       " 'broke out for us all the places it hit both revenues and expenses it also impacts the g sibbuffer and',\n",
       " 'out for us all the places it hit both revenues and expenses it also impacts the g sibbuffer and i',\n",
       " 'for us all the places it hit both revenues and expenses it also impacts the g sibbuffer and i know',\n",
       " 'us all the places it hit both revenues and expenses it also impacts the g sibbuffer and i know those',\n",
       " 'all the places it hit both revenues and expenses it also impacts the g sibbuffer and i know those rules',\n",
       " 'the places it hit both revenues and expenses it also impacts the g sibbuffer and i know those rules are',\n",
       " 'places it hit both revenues and expenses it also impacts the g sibbuffer and i know those rules are finalized',\n",
       " 'it hit both revenues and expenses it also impacts the g sibbuffer and i know those rules are finalized but',\n",
       " 'hit both revenues and expenses it also impacts the g sibbuffer and i know those rules are finalized but is',\n",
       " 'both revenues and expenses it also impacts the g sibbuffer and i know those rules are finalized but is there',\n",
       " 'revenues and expenses it also impacts the g sibbuffer and i know those rules are finalized but is there a',\n",
       " 'and expenses it also impacts the g sibbuffer and i know those rules are finalized but is there a way',\n",
       " 'expenses it also impacts the g sibbuffer and i know those rules are finalized but is there a way to',\n",
       " 'it also impacts the g sibbuffer and i know those rules are finalized but is there a way to hedge',\n",
       " 'also impacts the g sibbuffer and i know those rules are finalized but is there a way to hedge that',\n",
       " 'impacts the g sibbuffer and i know those rules are finalized but is there a way to hedge that or',\n",
       " 'the g sibbuffer and i know those rules are finalized but is there a way to hedge that or do',\n",
       " 'g sibbuffer and i know those rules are finalized but is there a way to hedge that or do y',\n",
       " 'sibbuffer and i know those rules are finalized but is there a way to hedge that or do y ou',\n",
       " 'and i know those rules are finalized but is there a way to hedge that or do y ou give',\n",
       " 'i know those rules are finalized but is there a way to hedge that or do y ou give up',\n",
       " 'know those rules are finalized but is there a way to hedge that or do y ou give up neutrality',\n",
       " 'those rules are finalized but is there a way to hedge that or do y ou give up neutrality if',\n",
       " 'rules are finalized but is there a way to hedge that or do y ou give up neutrality if y',\n",
       " 'are finalized but is there a way to hedge that or do y ou give up neutrality if y ou',\n",
       " 'finalized but is there a way to hedge that or do y ou give up neutrality if y ou wind',\n",
       " 'but is there a way to hedge that or do y ou give up neutrality if y ou wind up',\n",
       " 'is there a way to hedge that or do y ou give up neutrality if y ou wind up hedging',\n",
       " 'there a way to hedge that or do y ou give up neutrality if y ou wind up hedging that',\n",
       " 'a way to hedge that or do y ou give up neutrality if y ou wind up hedging that is',\n",
       " 'way to hedge that or do y ou give up neutrality if y ou wind up hedging that is it',\n",
       " 'to hedge that or do y ou give up neutrality if y ou wind up hedging that is it an',\n",
       " 'hedge that or do y ou give up neutrality if y ou wind up hedging that is it an either',\n",
       " 'that or do y ou give up neutrality if y ou wind up hedging that is it an either or',\n",
       " 'or do y ou give up neutrality if y ou wind up hedging that is it an either or scenario',\n",
       " 'do y ou give up neutrality if y ou wind up hedging that is it an either or scenario last',\n",
       " 'y ou give up neutrality if y ou wind up hedging that is it an either or scenario last one',\n",
       " 'ou give up neutrality if y ou wind up hedging that is it an either or scenario last one on',\n",
       " 'give up neutrality if y ou wind up hedging that is it an either or scenario last one on credit',\n",
       " 'up neutrality if y ou wind up hedging that is it an either or scenario last one on credit which',\n",
       " 'neutrality if y ou wind up hedging that is it an either or scenario last one on credit which has',\n",
       " 'if y ou wind up hedging that is it an either or scenario last one on credit which has also',\n",
       " 'y ou wind up hedging that is it an either or scenario last one on credit which has also been',\n",
       " 'ou wind up hedging that is it an either or scenario last one on credit which has also been great',\n",
       " 'wind up hedging that is it an either or scenario last one on credit which has also been great just',\n",
       " 'up hedging that is it an either or scenario last one on credit which has also been great just curious',\n",
       " 'hedging that is it an either or scenario last one on credit which has also been great just curious if',\n",
       " 'that is it an either or scenario last one on credit which has also been great just curious if there',\n",
       " 'is it an either or scenario last one on credit which has also been great just curious if there any',\n",
       " 'it an either or scenario last one on credit which has also been great just curious if there any thing',\n",
       " 'an either or scenario last one on credit which has also been great just curious if there any thing within',\n",
       " 'either or scenario last one on credit which has also been great just curious if there any thing within whether',\n",
       " 'or scenario last one on credit which has also been great just curious if there any thing within whether it',\n",
       " 'scenario last one on credit which has also been great just curious if there any thing within whether it be',\n",
       " 'last one on credit which has also been great just curious if there any thing within whether it be russia',\n",
       " 'one on credit which has also been great just curious if there any thing within whether it be russia ukraine',\n",
       " 'on credit which has also been great just curious if there any thing within whether it be russia ukraine exposure',\n",
       " 'credit which has also been great just curious if there any thing within whether it be russia ukraine exposure or',\n",
       " 'which has also been great just curious if there any thing within whether it be russia ukraine exposure or energy',\n",
       " 'has also been great just curious if there any thing within whether it be russia ukraine exposure or energy exposure',\n",
       " 'also been great just curious if there any thing within whether it be russia ukraine exposure or energy exposure thatis',\n",
       " 'been great just curious if there any thing within whether it be russia ukraine exposure or energy exposure thatis getting',\n",
       " 'great just curious if there any thing within whether it be russia ukraine exposure or energy exposure thatis getting internal',\n",
       " 'just curious if there any thing within whether it be russia ukraine exposure or energy exposure thatis getting internal rankings',\n",
       " 'curious if there any thing within whether it be russia ukraine exposure or energy exposure thatis getting internal rankings going',\n",
       " 'if there any thing within whether it be russia ukraine exposure or energy exposure thatis getting internal rankings going higher',\n",
       " 'there any thing within whether it be russia ukraine exposure or energy exposure thatis getting internal rankings going higher or',\n",
       " 'any thing within whether it be russia ukraine exposure or energy exposure thatis getting internal rankings going higher or should',\n",
       " 'thing within whether it be russia ukraine exposure or energy exposure thatis getting internal rankings going higher or should i',\n",
       " 'within whether it be russia ukraine exposure or energy exposure thatis getting internal rankings going higher or should i say',\n",
       " 'whether it be russia ukraine exposure or energy exposure thatis getting internal rankings going higher or should i say internal',\n",
       " 'it be russia ukraine exposure or energy exposure thatis getting internal rankings going higher or should i say internal downgrades',\n",
       " 'be russia ukraine exposure or energy exposure thatis getting internal rankings going higher or should i say internal downgrades that',\n",
       " 'russia ukraine exposure or energy exposure thatis getting internal rankings going higher or should i say internal downgrades that we',\n",
       " 'ukraine exposure or energy exposure thatis getting internal rankings going higher or should i say internal downgrades that we should',\n",
       " 'exposure or energy exposure thatis getting internal rankings going higher or should i say internal downgrades that we should be',\n",
       " 'or energy exposure thatis getting internal rankings going higher or should i say internal downgrades that we should be thinking',\n",
       " 'energy exposure thatis getting internal rankings going higher or should i say internal downgrades that we should be thinking about',\n",
       " 'exposure thatis getting internal rankings going higher or should i say internal downgrades that we should be thinking about throughout',\n",
       " 'thatis getting internal rankings going higher or should i say internal downgrades that we should be thinking about throughout this',\n",
       " 'getting internal rankings going higher or should i say internal downgrades that we should be thinking about throughout this y',\n",
       " 'internal rankings going higher or should i say internal downgrades that we should be thinking about throughout this y ear',\n",
       " 'rankings going higher or should i say internal downgrades that we should be thinking about throughout this y ear investing',\n",
       " 'going higher or should i say internal downgrades that we should be thinking about throughout this y ear investing lending',\n",
       " 'higher or should i say internal downgrades that we should be thinking about throughout this y ear investing lending was',\n",
       " 'or should i say internal downgrades that we should be thinking about throughout this y ear investing lending was pretty',\n",
       " 'should i say internal downgrades that we should be thinking about throughout this y ear investing lending was pretty darn',\n",
       " 'i say internal downgrades that we should be thinking about throughout this y ear investing lending was pretty darn good',\n",
       " 'say internal downgrades that we should be thinking about throughout this y ear investing lending was pretty darn good i',\n",
       " 'internal downgrades that we should be thinking about throughout this y ear investing lending was pretty darn good i curious',\n",
       " 'downgrades that we should be thinking about throughout this y ear investing lending was pretty darn good i curious on',\n",
       " 'that we should be thinking about throughout this y ear investing lending was pretty darn good i curious on the',\n",
       " 'we should be thinking about throughout this y ear investing lending was pretty darn good i curious on the equity',\n",
       " 'should be thinking about throughout this y ear investing lending was pretty darn good i curious on the equity side',\n",
       " 'be thinking about throughout this y ear investing lending was pretty darn good i curious on the equity side how',\n",
       " 'thinking about throughout this y ear investing lending was pretty darn good i curious on the equity side how much',\n",
       " 'about throughout this y ear investing lending was pretty darn good i curious on the equity side how much of',\n",
       " 'throughout this y ear investing lending was pretty darn good i curious on the equity side how much of it',\n",
       " 'this y ear investing lending was pretty darn good i curious on the equity side how much of it was',\n",
       " 'y ear investing lending was pretty darn good i curious on the equity side how much of it was actually',\n",
       " 'ear investing lending was pretty darn good i curious on the equity side how much of it was actually realized',\n",
       " 'investing lending was pretty darn good i curious on the equity side how much of it was actually realized in',\n",
       " 'lending was pretty darn good i curious on the equity side how much of it was actually realized in asset',\n",
       " 'was pretty darn good i curious on the equity side how much of it was actually realized in asset sales',\n",
       " 'pretty darn good i curious on the equity side how much of it was actually realized in asset sales because',\n",
       " 'darn good i curious on the equity side how much of it was actually realized in asset sales because it',\n",
       " 'good i curious on the equity side how much of it was actually realized in asset sales because it a',\n",
       " 'i curious on the equity side how much of it was actually realized in asset sales because it a pretty',\n",
       " 'curious on the equity side how much of it was actually realized in asset sales because it a pretty active',\n",
       " 'on the equity side how much of it was actually realized in asset sales because it a pretty active quarter',\n",
       " 'the equity side how much of it was actually realized in asset sales because it a pretty active quarter and',\n",
       " 'equity side how much of it was actually realized in asset sales because it a pretty active quarter and then',\n",
       " 'side how much of it was actually realized in asset sales because it a pretty active quarter and then maybe',\n",
       " 'how much of it was actually realized in asset sales because it a pretty active quarter and then maybe related',\n",
       " 'much of it was actually realized in asset sales because it a pretty active quarter and then maybe related to',\n",
       " 'of it was actually realized in asset sales because it a pretty active quarter and then maybe related to that',\n",
       " 'it was actually realized in asset sales because it a pretty active quarter and then maybe related to that if',\n",
       " 'was actually realized in asset sales because it a pretty active quarter and then maybe related to that if you',\n",
       " 'actually realized in asset sales because it a pretty active quarter and then maybe related to that if you could',\n",
       " 'realized in asset sales because it a pretty active quarter and then maybe related to that if you could give',\n",
       " 'in asset sales because it a pretty active quarter and then maybe related to that if you could give the',\n",
       " 'asset sales because it a pretty active quarter and then maybe related to that if you could give the portfolio',\n",
       " 'sales because it a pretty active quarter and then maybe related to that if you could give the portfolio breakdown',\n",
       " 'because it a pretty active quarter and then maybe related to that if you could give the portfolio breakdown in',\n",
       " 'it a pretty active quarter and then maybe related to that if you could give the portfolio breakdown in terms',\n",
       " 'a pretty active quarter and then maybe related to that if you could give the portfolio breakdown in terms of',\n",
       " 'pretty active quarter and then maybe related to that if you could give the portfolio breakdown in terms of equity',\n",
       " 'active quarter and then maybe related to that if you could give the portfolio breakdown in terms of equity debt',\n",
       " 'quarter and then maybe related to that if you could give the portfolio breakdown in terms of equity debt and',\n",
       " 'and then maybe related to that if you could give the portfolio breakdown in terms of equity debt and lending',\n",
       " 'then maybe related to that if you could give the portfolio breakdown in terms of equity debt and lending assets',\n",
       " 'maybe related to that if you could give the portfolio breakdown in terms of equity debt and lending assets and',\n",
       " 'related to that if you could give the portfolio breakdown in terms of equity debt and lending assets and what',\n",
       " 'to that if you could give the portfolio breakdown in terms of equity debt and lending assets and what the',\n",
       " 'that if you could give the portfolio breakdown in terms of equity debt and lending assets and what the disallowed',\n",
       " 'if you could give the portfolio breakdown in terms of equity debt and lending assets and what the disallowed portion',\n",
       " 'you could give the portfolio breakdown in terms of equity debt and lending assets and what the disallowed portion or',\n",
       " 'could give the portfolio breakdown in terms of equity debt and lending assets and what the disallowed portion or what',\n",
       " 'give the portfolio breakdown in terms of equity debt and lending assets and what the disallowed portion or what the',\n",
       " 'the portfolio breakdown in terms of equity debt and lending assets and what the disallowed portion or what the best',\n",
       " 'portfolio breakdown in terms of equity debt and lending assets and what the disallowed portion or what the best way',\n",
       " 'breakdown in terms of equity debt and lending assets and what the disallowed portion or what the best way to',\n",
       " 'in terms of equity debt and lending assets and what the disallowed portion or what the best way to ask',\n",
       " 'terms of equity debt and lending assets and what the disallowed portion or what the best way to ask how',\n",
       " 'of equity debt and lending assets and what the disallowed portion or what the best way to ask how much',\n",
       " 'equity debt and lending assets and what the disallowed portion or what the best way to ask how much goldman',\n",
       " 'debt and lending assets and what the disallowed portion or what the best way to ask how much goldman still',\n",
       " 'and lending assets and what the disallowed portion or what the best way to ask how much goldman still has',\n",
       " 'lending assets and what the disallowed portion or what the best way to ask how much goldman still has in',\n",
       " 'assets and what the disallowed portion or what the best way to ask how much goldman still has in the',\n",
       " 'and what the disallowed portion or what the best way to ask how much goldman still has in the funds',\n",
       " 'what the disallowed portion or what the best way to ask how much goldman still has in the funds that',\n",
       " 'the disallowed portion or what the best way to ask how much goldman still has in the funds that you',\n",
       " 'disallowed portion or what the best way to ask how much goldman still has in the funds that you would',\n",
       " 'portion or what the best way to ask how much goldman still has in the funds that you would need',\n",
       " 'or what the best way to ask how much goldman still has in the funds that you would need to',\n",
       " 'what the best way to ask how much goldman still has in the funds that you would need to eventually',\n",
       " 'the best way to ask how much goldman still has in the funds that you would need to eventually liquidate',\n",
       " 'best way to ask how much goldman still has in the funds that you would need to eventually liquidate on',\n",
       " 'way to ask how much goldman still has in the funds that you would need to eventually liquidate on ficc',\n",
       " 'to ask how much goldman still has in the funds that you would need to eventually liquidate on ficc the',\n",
       " 'ask how much goldman still has in the funds that you would need to eventually liquidate on ficc the macro',\n",
       " 'how much goldman still has in the funds that you would need to eventually liquidate on ficc the macro products',\n",
       " 'much goldman still has in the funds that you would need to eventually liquidate on ficc the macro products are',\n",
       " 'goldman still has in the funds that you would need to eventually liquidate on ficc the macro products are great',\n",
       " 'still has in the funds that you would need to eventually liquidate on ficc the macro products are great but',\n",
       " 'has in the funds that you would need to eventually liquidate on ficc the macro products are great but credit',\n",
       " 'in the funds that you would need to eventually liquidate on ficc the macro products are great but credit and',\n",
       " 'the funds that you would need to eventually liquidate on ficc the macro products are great but credit and mortgage',\n",
       " 'funds that you would need to eventually liquidate on ficc the macro products are great but credit and mortgage was',\n",
       " 'that you would need to eventually liquidate on ficc the macro products are great but credit and mortgage was so',\n",
       " 'you would need to eventually liquidate on ficc the macro products are great but credit and mortgage was so not',\n",
       " 'would need to eventually liquidate on ficc the macro products are great but credit and mortgage was so not exactly',\n",
       " 'need to eventually liquidate on ficc the macro products are great but credit and mortgage was so not exactly hitting',\n",
       " 'to eventually liquidate on ficc the macro products are great but credit and mortgage was so not exactly hitting on',\n",
       " 'eventually liquidate on ficc the macro products are great but credit and mortgage was so not exactly hitting on all',\n",
       " 'liquidate on ficc the macro products are great but credit and mortgage was so not exactly hitting on all cylinders',\n",
       " 'on ficc the macro products are great but credit and mortgage was so not exactly hitting on all cylinders as',\n",
       " 'ficc the macro products are great but credit and mortgage was so not exactly hitting on all cylinders as a',\n",
       " 'the macro products are great but credit and mortgage was so not exactly hitting on all cylinders as a whole',\n",
       " 'macro products are great but credit and mortgage was so not exactly hitting on all cylinders as a whole but',\n",
       " 'products are great but credit and mortgage was so not exactly hitting on all cylinders as a whole but it',\n",
       " 'are great but credit and mortgage was so not exactly hitting on all cylinders as a whole but it certainly',\n",
       " 'great but credit and mortgage was so not exactly hitting on all cylinders as a whole but it certainly seems',\n",
       " 'but credit and mortgage was so not exactly hitting on all cylinders as a whole but it certainly seems like',\n",
       " 'credit and mortgage was so not exactly hitting on all cylinders as a whole but it certainly seems like maybe',\n",
       " 'and mortgage was so not exactly hitting on all cylinders as a whole but it certainly seems like maybe some',\n",
       " 'mortgage was so not exactly hitting on all cylinders as a whole but it certainly seems like maybe some of',\n",
       " 'was so not exactly hitting on all cylinders as a whole but it certainly seems like maybe some of the',\n",
       " 'so not exactly hitting on all cylinders as a whole but it certainly seems like maybe some of the reduced',\n",
       " 'not exactly hitting on all cylinders as a whole but it certainly seems like maybe some of the reduced capacity',\n",
       " 'exactly hitting on all cylinders as a whole but it certainly seems like maybe some of the reduced capacity in',\n",
       " 'hitting on all cylinders as a whole but it certainly seems like maybe some of the reduced capacity in the',\n",
       " 'on all cylinders as a whole but it certainly seems like maybe some of the reduced capacity in the industry',\n",
       " 'all cylinders as a whole but it certainly seems like maybe some of the reduced capacity in the industry is',\n",
       " 'cylinders as a whole but it certainly seems like maybe some of the reduced capacity in the industry is helping',\n",
       " 'as a whole but it certainly seems like maybe some of the reduced capacity in the industry is helping you',\n",
       " 'a whole but it certainly seems like maybe some of the reduced capacity in the industry is helping you and',\n",
       " 'whole but it certainly seems like maybe some of the reduced capacity in the industry is helping you and so',\n",
       " 'but it certainly seems like maybe some of the reduced capacity in the industry is helping you and so your',\n",
       " 'it certainly seems like maybe some of the reduced capacity in the industry is helping you and so your long',\n",
       " 'certainly seems like maybe some of the reduced capacity in the industry is helping you and so your long term',\n",
       " 'seems like maybe some of the reduced capacity in the industry is helping you and so your long term strategy',\n",
       " 'like maybe some of the reduced capacity in the industry is helping you and so your long term strategy that',\n",
       " 'maybe some of the reduced capacity in the industry is helping you and so your long term strategy that we',\n",
       " 'some of the reduced capacity in the industry is helping you and so your long term strategy that we all',\n",
       " 'of the reduced capacity in the industry is helping you and so your long term strategy that we all beat',\n",
       " 'the reduced capacity in the industry is helping you and so your long term strategy that we all beat you',\n",
       " 'reduced capacity in the industry is helping you and so your long term strategy that we all beat you up',\n",
       " 'capacity in the industry is helping you and so your long term strategy that we all beat you up for',\n",
       " 'in the industry is helping you and so your long term strategy that we all beat you up for for',\n",
       " 'the industry is helping you and so your long term strategy that we all beat you up for for the',\n",
       " 'industry is helping you and so your long term strategy that we all beat you up for for the last',\n",
       " 'is helping you and so your long term strategy that we all beat you up for for the last five',\n",
       " 'helping you and so your long term strategy that we all beat you up for for the last five years',\n",
       " 'you and so your long term strategy that we all beat you up for for the last five years might',\n",
       " 'and so your long term strategy that we all beat you up for for the last five years might be',\n",
       " 'so your long term strategy that we all beat you up for for the last five years might be working',\n",
       " 'your long term strategy that we all beat you up for for the last five years might be working could',\n",
       " 'long term strategy that we all beat you up for for the last five years might be working could you',\n",
       " 'term strategy that we all beat you up for for the last five years might be working could you talk',\n",
       " 'strategy that we all beat you up for for the last five years might be working could you talk towards',\n",
       " 'that we all beat you up for for the last five years might be working could you talk towards what',\n",
       " 'we all beat you up for for the last five years might be working could you talk towards what you',\n",
       " 'all beat you up for for the last five years might be working could you talk towards what you seeing',\n",
       " 'beat you up for for the last five years might be working could you talk towards what you seeing in',\n",
       " 'you up for for the last five years might be working could you talk towards what you seeing in terms',\n",
       " 'up for for the last five years might be working could you talk towards what you seeing in terms of',\n",
       " 'for for the last five years might be working could you talk towards what you seeing in terms of capacity',\n",
       " 'for the last five years might be working could you talk towards what you seeing in terms of capacity and',\n",
       " 'the last five years might be working could you talk towards what you seeing in terms of capacity and now',\n",
       " 'last five years might be working could you talk towards what you seeing in terms of capacity and now that',\n",
       " 'five years might be working could you talk towards what you seeing in terms of capacity and now that you',\n",
       " 'years might be working could you talk towards what you seeing in terms of capacity and now that you seen',\n",
       " 'might be working could you talk towards what you seeing in terms of capacity and now that you seen a',\n",
       " 'be working could you talk towards what you seeing in terms of capacity and now that you seen a pickup',\n",
       " 'working could you talk towards what you seeing in terms of capacity and now that you seen a pickup in',\n",
       " 'could you talk towards what you seeing in terms of capacity and now that you seen a pickup in volatility',\n",
       " 'you talk towards what you seeing in terms of capacity and now that you seen a pickup in volatility what',\n",
       " 'talk towards what you seeing in terms of capacity and now that you seen a pickup in volatility what it',\n",
       " 'towards what you seeing in terms of capacity and now that you seen a pickup in volatility what it means',\n",
       " 'what you seeing in terms of capacity and now that you seen a pickup in volatility what it means in',\n",
       " 'you seeing in terms of capacity and now that you seen a pickup in volatility what it means in your',\n",
       " 'seeing in terms of capacity and now that you seen a pickup in volatility what it means in your commentary',\n",
       " 'in terms of capacity and now that you seen a pickup in volatility what it means in your commentary on',\n",
       " 'terms of capacity and now that you seen a pickup in volatility what it means in your commentary on fic',\n",
       " 'of capacity and now that you seen a pickup in volatility what it means in your commentary on fic about',\n",
       " 'capacity and now that you seen a pickup in volatility what it means in your commentary on fic about structured',\n",
       " 'and now that you seen a pickup in volatility what it means in your commentary on fic about structured transactions',\n",
       " 'now that you seen a pickup in volatility what it means in your commentary on fic about structured transactions being',\n",
       " 'that you seen a pickup in volatility what it means in your commentary on fic about structured transactions being a',\n",
       " 'you seen a pickup in volatility what it means in your commentary on fic about structured transactions being a contributor',\n",
       " 'seen a pickup in volatility what it means in your commentary on fic about structured transactions being a contributor i',\n",
       " 'a pickup in volatility what it means in your commentary on fic about structured transactions being a contributor i curious',\n",
       " 'pickup in volatility what it means in your commentary on fic about structured transactions being a contributor i curious if',\n",
       " 'in volatility what it means in your commentary on fic about structured transactions being a contributor i curious if that',\n",
       " 'volatility what it means in your commentary on fic about structured transactions being a contributor i curious if that is',\n",
       " 'what it means in your commentary on fic about structured transactions being a contributor i curious if that is only',\n",
       " 'it means in your commentary on fic about structured transactions being a contributor i curious if that is only in',\n",
       " 'means in your commentary on fic about structured transactions being a contributor i curious if that is only in commodities',\n",
       " 'in your commentary on fic about structured transactions being a contributor i curious if that is only in commodities or',\n",
       " 'your commentary on fic about structured transactions being a contributor i curious if that is only in commodities or if',\n",
       " 'commentary on fic about structured transactions being a contributor i curious if that is only in commodities or if that',\n",
       " 'on fic about structured transactions being a contributor i curious if that is only in commodities or if that was',\n",
       " 'fic about structured transactions being a contributor i curious if that is only in commodities or if that was in',\n",
       " 'about structured transactions being a contributor i curious if that is only in commodities or if that was in other',\n",
       " 'structured transactions being a contributor i curious if that is only in commodities or if that was in other parts',\n",
       " 'transactions being a contributor i curious if that is only in commodities or if that was in other parts of',\n",
       " 'being a contributor i curious if that is only in commodities or if that was in other parts of the',\n",
       " 'a contributor i curious if that is only in commodities or if that was in other parts of the business',\n",
       " 'contributor i curious if that is only in commodities or if that was in other parts of the business and',\n",
       " 'i curious if that is only in commodities or if that was in other parts of the business and how',\n",
       " 'curious if that is only in commodities or if that was in other parts of the business and how big',\n",
       " 'if that is only in commodities or if that was in other parts of the business and how big of',\n",
       " 'that is only in commodities or if that was in other parts of the business and how big of a',\n",
       " 'is only in commodities or if that was in other parts of the business and how big of a driver',\n",
       " 'only in commodities or if that was in other parts of the business and how big of a driver and',\n",
       " 'in commodities or if that was in other parts of the business and how big of a driver and more',\n",
       " 'commodities or if that was in other parts of the business and how big of a driver and more importantly',\n",
       " 'or if that was in other parts of the business and how big of a driver and more importantly is',\n",
       " 'if that was in other parts of the business and how big of a driver and more importantly is that',\n",
       " 'that was in other parts of the business and how big of a driver and more importantly is that a',\n",
       " 'was in other parts of the business and how big of a driver and more importantly is that a sustainable',\n",
       " 'in other parts of the business and how big of a driver and more importantly is that a sustainable appetite',\n",
       " 'other parts of the business and how big of a driver and more importantly is that a sustainable appetite by',\n",
       " 'parts of the business and how big of a driver and more importantly is that a sustainable appetite by clients',\n",
       " 'of the business and how big of a driver and more importantly is that a sustainable appetite by clients because',\n",
       " 'the business and how big of a driver and more importantly is that a sustainable appetite by clients because we',\n",
       " 'business and how big of a driver and more importantly is that a sustainable appetite by clients because we kind',\n",
       " 'and how big of a driver and more importantly is that a sustainable appetite by clients because we kind of',\n",
       " 'how big of a driver and more importantly is that a sustainable appetite by clients because we kind of all',\n",
       " 'big of a driver and more importantly is that a sustainable appetite by clients because we kind of all been',\n",
       " 'of a driver and more importantly is that a sustainable appetite by clients because we kind of all been waiting',\n",
       " 'a driver and more importantly is that a sustainable appetite by clients because we kind of all been waiting for',\n",
       " 'driver and more importantly is that a sustainable appetite by clients because we kind of all been waiting for that',\n",
       " 'and more importantly is that a sustainable appetite by clients because we kind of all been waiting for that for',\n",
       " 'more importantly is that a sustainable appetite by clients because we kind of all been waiting for that for a',\n",
       " 'importantly is that a sustainable appetite by clients because we kind of all been waiting for that for a while',\n",
       " 'is that a sustainable appetite by clients because we kind of all been waiting for that for a while okay',\n",
       " 'that a sustainable appetite by clients because we kind of all been waiting for that for a while okay and',\n",
       " 'a sustainable appetite by clients because we kind of all been waiting for that for a while okay and does',\n",
       " 'sustainable appetite by clients because we kind of all been waiting for that for a while okay and does that',\n",
       " 'appetite by clients because we kind of all been waiting for that for a while okay and does that mean',\n",
       " 'by clients because we kind of all been waiting for that for a while okay and does that mean i',\n",
       " 'clients because we kind of all been waiting for that for a while okay and does that mean i do',\n",
       " 'because we kind of all been waiting for that for a while okay and does that mean i do want',\n",
       " 'we kind of all been waiting for that for a while okay and does that mean i do want to',\n",
       " 'kind of all been waiting for that for a while okay and does that mean i do want to put',\n",
       " 'of all been waiting for that for a while okay and does that mean i do want to put words',\n",
       " 'all been waiting for that for a while okay and does that mean i do want to put words in',\n",
       " 'been waiting for that for a while okay and does that mean i do want to put words in your',\n",
       " 'waiting for that for a while okay and does that mean i do want to put words in your mouth',\n",
       " 'for that for a while okay and does that mean i do want to put words in your mouth but',\n",
       " 'that for a while okay and does that mean i do want to put words in your mouth but was',\n",
       " 'for a while okay and does that mean i do want to put words in your mouth but was the',\n",
       " 'a while okay and does that mean i do want to put words in your mouth but was the commodities',\n",
       " 'while okay and does that mean i do want to put words in your mouth but was the commodities contribution',\n",
       " 'okay and does that mean i do want to put words in your mouth but was the commodities contribution of',\n",
       " 'and does that mean i do want to put words in your mouth but was the commodities contribution of the',\n",
       " 'does that mean i do want to put words in your mouth but was the commodities contribution of the quarter',\n",
       " 'that mean i do want to put words in your mouth but was the commodities contribution of the quarter larger',\n",
       " 'mean i do want to put words in your mouth but was the commodities contribution of the quarter larger than',\n",
       " 'i do want to put words in your mouth but was the commodities contribution of the quarter larger than normal',\n",
       " 'do want to put words in your mouth but was the commodities contribution of the quarter larger than normal times',\n",
       " 'want to put words in your mouth but was the commodities contribution of the quarter larger than normal times just',\n",
       " 'to put words in your mouth but was the commodities contribution of the quarter larger than normal times just because',\n",
       " 'put words in your mouth but was the commodities contribution of the quarter larger than normal times just because some',\n",
       " 'words in your mouth but was the commodities contribution of the quarter larger than normal times just because some of',\n",
       " 'in your mouth but was the commodities contribution of the quarter larger than normal times just because some of the',\n",
       " 'your mouth but was the commodities contribution of the quarter larger than normal times just because some of the things',\n",
       " 'mouth but was the commodities contribution of the quarter larger than normal times just because some of the things you',\n",
       " 'but was the commodities contribution of the quarter larger than normal times just because some of the things you mentioned',\n",
       " 'was the commodities contribution of the quarter larger than normal times just because some of the things you mentioned plus',\n",
       " 'the commodities contribution of the quarter larger than normal times just because some of the things you mentioned plus the',\n",
       " 'commodities contribution of the quarter larger than normal times just because some of the things you mentioned plus the weather',\n",
       " 'contribution of the quarter larger than normal times just because some of the things you mentioned plus the weather and',\n",
       " 'of the quarter larger than normal times just because some of the things you mentioned plus the weather and then',\n",
       " 'the quarter larger than normal times just because some of the things you mentioned plus the weather and then the',\n",
       " 'quarter larger than normal times just because some of the things you mentioned plus the weather and then the rwa',\n",
       " 'larger than normal times just because some of the things you mentioned plus the weather and then the rwa down',\n",
       " 'than normal times just because some of the things you mentioned plus the weather and then the rwa down for',\n",
       " 'normal times just because some of the things you mentioned plus the weather and then the rwa down for the',\n",
       " 'times just because some of the things you mentioned plus the weather and then the rwa down for the firm',\n",
       " 'just because some of the things you mentioned plus the weather and then the rwa down for the firm sequentially',\n",
       " 'because some of the things you mentioned plus the weather and then the rwa down for the firm sequentially i',\n",
       " 'some of the things you mentioned plus the weather and then the rwa down for the firm sequentially i think',\n",
       " 'of the things you mentioned plus the weather and then the rwa down for the firm sequentially i think you',\n",
       " 'the things you mentioned plus the weather and then the rwa down for the firm sequentially i think you said',\n",
       " 'things you mentioned plus the weather and then the rwa down for the firm sequentially i think you said in',\n",
       " 'you mentioned plus the weather and then the rwa down for the firm sequentially i think you said in fic',\n",
       " 'mentioned plus the weather and then the rwa down for the firm sequentially i think you said in fic it',\n",
       " 'plus the weather and then the rwa down for the firm sequentially i think you said in fic it good',\n",
       " 'the weather and then the rwa down for the firm sequentially i think you said in fic it good it',\n",
       " 'weather and then the rwa down for the firm sequentially i think you said in fic it good it welcome',\n",
       " 'and then the rwa down for the firm sequentially i think you said in fic it good it welcome is',\n",
       " 'then the rwa down for the firm sequentially i think you said in fic it good it welcome is that',\n",
       " 'the rwa down for the firm sequentially i think you said in fic it good it welcome is that not',\n",
       " 'rwa down for the firm sequentially i think you said in fic it good it welcome is that not a',\n",
       " 'down for the firm sequentially i think you said in fic it good it welcome is that not a number',\n",
       " 'for the firm sequentially i think you said in fic it good it welcome is that not a number that',\n",
       " 'the firm sequentially i think you said in fic it good it welcome is that not a number that we',\n",
       " 'firm sequentially i think you said in fic it good it welcome is that not a number that we want',\n",
       " 'sequentially i think you said in fic it good it welcome is that not a number that we want to',\n",
       " 'i think you said in fic it good it welcome is that not a number that we want to straight',\n",
       " 'think you said in fic it good it welcome is that not a number that we want to straight line',\n",
       " 'you said in fic it good it welcome is that not a number that we want to straight line right',\n",
       " 'said in fic it good it welcome is that not a number that we want to straight line right can',\n",
       " 'in fic it good it welcome is that not a number that we want to straight line right can it',\n",
       " 'fic it good it welcome is that not a number that we want to straight line right can it fall',\n",
       " 'it good it welcome is that not a number that we want to straight line right can it fall at',\n",
       " 'good it welcome is that not a number that we want to straight line right can it fall at that',\n",
       " 'it welcome is that not a number that we want to straight line right can it fall at that pace',\n",
       " 'welcome is that not a number that we want to straight line right can it fall at that pace on',\n",
       " 'is that not a number that we want to straight line right can it fall at that pace on an',\n",
       " 'that not a number that we want to straight line right can it fall at that pace on an annualized',\n",
       " 'not a number that we want to straight line right can it fall at that pace on an annualized basis',\n",
       " 'a number that we want to straight line right can it fall at that pace on an annualized basis share',\n",
       " 'number that we want to straight line right can it fall at that pace on an annualized basis share count',\n",
       " 'that we want to straight line right can it fall at that pace on an annualized basis share count was',\n",
       " 'we want to straight line right can it fall at that pace on an annualized basis share count was flat',\n",
       " 'want to straight line right can it fall at that pace on an annualized basis share count was flat year',\n",
       " 'to straight line right can it fall at that pace on an annualized basis share count was flat year on',\n",
       " 'straight line right can it fall at that pace on an annualized basis share count was flat year on year',\n",
       " 'line right can it fall at that pace on an annualized basis share count was flat year on year but',\n",
       " 'right can it fall at that pace on an annualized basis share count was flat year on year but you',\n",
       " 'can it fall at that pace on an annualized basis share count was flat year on year but you got',\n",
       " 'it fall at that pace on an annualized basis share count was flat year on year but you got a',\n",
       " 'fall at that pace on an annualized basis share count was flat year on year but you got a higher',\n",
       " 'at that pace on an annualized basis share count was flat year on year but you got a higher authorization',\n",
       " 'that pace on an annualized basis share count was flat year on year but you got a higher authorization that',\n",
       " 'pace on an annualized basis share count was flat year on year but you got a higher authorization that just',\n",
       " 'on an annualized basis share count was flat year on year but you got a higher authorization that just math',\n",
       " 'an annualized basis share count was flat year on year but you got a higher authorization that just math right',\n",
       " 'annualized basis share count was flat year on year but you got a higher authorization that just math right we',\n",
       " 'basis share count was flat year on year but you got a higher authorization that just math right we can',\n",
       " 'share count was flat year on year but you got a higher authorization that just math right we can count',\n",
       " 'count was flat year on year but you got a higher authorization that just math right we can count on',\n",
       " 'was flat year on year but you got a higher authorization that just math right we can count on maybe',\n",
       " 'flat year on year but you got a higher authorization that just math right we can count on maybe shrinkage',\n",
       " 'year on year but you got a higher authorization that just math right we can count on maybe shrinkage this',\n",
       " 'on year but you got a higher authorization that just math right we can count on maybe shrinkage this year',\n",
       " 'year but you got a higher authorization that just math right we can count on maybe shrinkage this year in',\n",
       " 'but you got a higher authorization that just math right we can count on maybe shrinkage this year in the',\n",
       " 'you got a higher authorization that just math right we can count on maybe shrinkage this year in the share',\n",
       " 'got a higher authorization that just math right we can count on maybe shrinkage this year in the share count',\n",
       " 'a higher authorization that just math right we can count on maybe shrinkage this year in the share count on',\n",
       " 'higher authorization that just math right we can count on maybe shrinkage this year in the share count on fixed',\n",
       " 'authorization that just math right we can count on maybe shrinkage this year in the share count on fixed income',\n",
       " 'that just math right we can count on maybe shrinkage this year in the share count on fixed income it',\n",
       " 'just math right we can count on maybe shrinkage this year in the share count on fixed income it seems',\n",
       " 'math right we can count on maybe shrinkage this year in the share count on fixed income it seems like',\n",
       " 'right we can count on maybe shrinkage this year in the share count on fixed income it seems like you',\n",
       " 'we can count on maybe shrinkage this year in the share count on fixed income it seems like you weathered',\n",
       " 'can count on maybe shrinkage this year in the share count on fixed income it seems like you weathered the',\n",
       " 'count on maybe shrinkage this year in the share count on fixed income it seems like you weathered the whole',\n",
       " 'on maybe shrinkage this year in the share count on fixed income it seems like you weathered the whole greece',\n",
       " 'maybe shrinkage this year in the share count on fixed income it seems like you weathered the whole greece and',\n",
       " 'shrinkage this year in the share count on fixed income it seems like you weathered the whole greece and china',\n",
       " 'this year in the share count on fixed income it seems like you weathered the whole greece and china storm',\n",
       " 'year in the share count on fixed income it seems like you weathered the whole greece and china storm pretty',\n",
       " 'in the share count on fixed income it seems like you weathered the whole greece and china storm pretty well',\n",
       " 'the share count on fixed income it seems like you weathered the whole greece and china storm pretty well the',\n",
       " 'share count on fixed income it seems like you weathered the whole greece and china storm pretty well the thought',\n",
       " 'count on fixed income it seems like you weathered the whole greece and china storm pretty well the thought on',\n",
       " 'on fixed income it seems like you weathered the whole greece and china storm pretty well the thought on the',\n",
       " 'fixed income it seems like you weathered the whole greece and china storm pretty well the thought on the lack',\n",
       " 'income it seems like you weathered the whole greece and china storm pretty well the thought on the lack of',\n",
       " 'it seems like you weathered the whole greece and china storm pretty well the thought on the lack of liquidity',\n",
       " 'seems like you weathered the whole greece and china storm pretty well the thought on the lack of liquidity in',\n",
       " 'like you weathered the whole greece and china storm pretty well the thought on the lack of liquidity in fixed',\n",
       " 'you weathered the whole greece and china storm pretty well the thought on the lack of liquidity in fixed income',\n",
       " 'weathered the whole greece and china storm pretty well the thought on the lack of liquidity in fixed income markets',\n",
       " 'the whole greece and china storm pretty well the thought on the lack of liquidity in fixed income markets gets',\n",
       " 'whole greece and china storm pretty well the thought on the lack of liquidity in fixed income markets gets a',\n",
       " 'greece and china storm pretty well the thought on the lack of liquidity in fixed income markets gets a lot',\n",
       " 'and china storm pretty well the thought on the lack of liquidity in fixed income markets gets a lot of',\n",
       " 'china storm pretty well the thought on the lack of liquidity in fixed income markets gets a lot of attention',\n",
       " 'storm pretty well the thought on the lack of liquidity in fixed income markets gets a lot of attention you',\n",
       " 'pretty well the thought on the lack of liquidity in fixed income markets gets a lot of attention you guys',\n",
       " 'well the thought on the lack of liquidity in fixed income markets gets a lot of attention you guys have',\n",
       " 'the thought on the lack of liquidity in fixed income markets gets a lot of attention you guys have the',\n",
       " 'thought on the lack of liquidity in fixed income markets gets a lot of attention you guys have the most',\n",
       " 'on the lack of liquidity in fixed income markets gets a lot of attention you guys have the most market',\n",
       " 'the lack of liquidity in fixed income markets gets a lot of attention you guys have the most market share',\n",
       " 'lack of liquidity in fixed income markets gets a lot of attention you guys have the most market share have',\n",
       " 'of liquidity in fixed income markets gets a lot of attention you guys have the most market share have the',\n",
       " 'liquidity in fixed income markets gets a lot of attention you guys have the most market share have the lowest',\n",
       " 'in fixed income markets gets a lot of attention you guys have the most market share have the lowest standard',\n",
       " 'fixed income markets gets a lot of attention you guys have the most market share have the lowest standard deviation',\n",
       " 'income markets gets a lot of attention you guys have the most market share have the lowest standard deviation in',\n",
       " 'markets gets a lot of attention you guys have the most market share have the lowest standard deviation in the',\n",
       " 'gets a lot of attention you guys have the most market share have the lowest standard deviation in the business',\n",
       " 'a lot of attention you guys have the most market share have the lowest standard deviation in the business as',\n",
       " 'lot of attention you guys have the most market share have the lowest standard deviation in the business as a',\n",
       " 'of attention you guys have the most market share have the lowest standard deviation in the business as a liquidity',\n",
       " 'attention you guys have the most market share have the lowest standard deviation in the business as a liquidity provider',\n",
       " 'you guys have the most market share have the lowest standard deviation in the business as a liquidity provider that',\n",
       " 'guys have the most market share have the lowest standard deviation in the business as a liquidity provider that a',\n",
       " 'have the most market share have the lowest standard deviation in the business as a liquidity provider that a good',\n",
       " 'the most market share have the lowest standard deviation in the business as a liquidity provider that a good thing',\n",
       " 'most market share have the lowest standard deviation in the business as a liquidity provider that a good thing for',\n",
       " 'market share have the lowest standard deviation in the business as a liquidity provider that a good thing for you',\n",
       " 'share have the lowest standard deviation in the business as a liquidity provider that a good thing for you but',\n",
       " 'have the lowest standard deviation in the business as a liquidity provider that a good thing for you but i',\n",
       " 'the lowest standard deviation in the business as a liquidity provider that a good thing for you but i curious',\n",
       " 'lowest standard deviation in the business as a liquidity provider that a good thing for you but i curious on',\n",
       " 'standard deviation in the business as a liquidity provider that a good thing for you but i curious on how',\n",
       " 'deviation in the business as a liquidity provider that a good thing for you but i curious on how you',\n",
       " 'in the business as a liquidity provider that a good thing for you but i curious on how you thinking',\n",
       " 'the business as a liquidity provider that a good thing for you but i curious on how you thinking about',\n",
       " 'business as a liquidity provider that a good thing for you but i curious on how you thinking about preparing',\n",
       " 'as a liquidity provider that a good thing for you but i curious on how you thinking about preparing for',\n",
       " 'a liquidity provider that a good thing for you but i curious on how you thinking about preparing for what',\n",
       " 'liquidity provider that a good thing for you but i curious on how you thinking about preparing for what seems',\n",
       " 'provider that a good thing for you but i curious on how you thinking about preparing for what seems to',\n",
       " 'that a good thing for you but i curious on how you thinking about preparing for what seems to be',\n",
       " 'a good thing for you but i curious on how you thinking about preparing for what seems to be a',\n",
       " 'good thing for you but i curious on how you thinking about preparing for what seems to be a pretty',\n",
       " 'thing for you but i curious on how you thinking about preparing for what seems to be a pretty serious',\n",
       " 'for you but i curious on how you thinking about preparing for what seems to be a pretty serious issue',\n",
       " 'you but i curious on how you thinking about preparing for what seems to be a pretty serious issue and',\n",
       " 'but i curious on how you thinking about preparing for what seems to be a pretty serious issue and how',\n",
       " 'i curious on how you thinking about preparing for what seems to be a pretty serious issue and how serious',\n",
       " 'curious on how you thinking about preparing for what seems to be a pretty serious issue and how serious of',\n",
       " 'on how you thinking about preparing for what seems to be a pretty serious issue and how serious of an',\n",
       " 'how you thinking about preparing for what seems to be a pretty serious issue and how serious of an issue',\n",
       " 'you thinking about preparing for what seems to be a pretty serious issue and how serious of an issue do',\n",
       " 'thinking about preparing for what seems to be a pretty serious issue and how serious of an issue do you',\n",
       " 'about preparing for what seems to be a pretty serious issue and how serious of an issue do you think',\n",
       " 'preparing for what seems to be a pretty serious issue and how serious of an issue do you think it',\n",
       " 'for what seems to be a pretty serious issue and how serious of an issue do you think it is',\n",
       " 'what seems to be a pretty serious issue and how serious of an issue do you think it is in',\n",
       " 'seems to be a pretty serious issue and how serious of an issue do you think it is in terms',\n",
       " 'to be a pretty serious issue and how serious of an issue do you think it is in terms of',\n",
       " 'be a pretty serious issue and how serious of an issue do you think it is in terms of the',\n",
       " 'a pretty serious issue and how serious of an issue do you think it is in terms of the potential',\n",
       " 'pretty serious issue and how serious of an issue do you think it is in terms of the potential disruption',\n",
       " 'serious issue and how serious of an issue do you think it is in terms of the potential disruption on',\n",
       " 'issue and how serious of an issue do you think it is in terms of the potential disruption on living',\n",
       " 'and how serious of an issue do you think it is in terms of the potential disruption on living wills',\n",
       " 'how serious of an issue do you think it is in terms of the potential disruption on living wills if',\n",
       " 'serious of an issue do you think it is in terms of the potential disruption on living wills if you',\n",
       " 'of an issue do you think it is in terms of the potential disruption on living wills if you look',\n",
       " 'an issue do you think it is in terms of the potential disruption on living wills if you look at',\n",
       " 'issue do you think it is in terms of the potential disruption on living wills if you look at the',\n",
       " 'do you think it is in terms of the potential disruption on living wills if you look at the comments',\n",
       " 'you think it is in terms of the potential disruption on living wills if you look at the comments from',\n",
       " 'think it is in terms of the potential disruption on living wills if you look at the comments from the',\n",
       " 'it is in terms of the potential disruption on living wills if you look at the comments from the previous',\n",
       " 'is in terms of the potential disruption on living wills if you look at the comments from the previous year',\n",
       " 'in terms of the potential disruption on living wills if you look at the comments from the previous year what',\n",
       " 'terms of the potential disruption on living wills if you look at the comments from the previous year what they',\n",
       " 'of the potential disruption on living wills if you look at the comments from the previous year what they wanted',\n",
       " 'the potential disruption on living wills if you look at the comments from the previous year what they wanted you',\n",
       " 'potential disruption on living wills if you look at the comments from the previous year what they wanted you all',\n",
       " 'disruption on living wills if you look at the comments from the previous year what they wanted you all to',\n",
       " 'on living wills if you look at the comments from the previous year what they wanted you all to address',\n",
       " 'living wills if you look at the comments from the previous year what they wanted you all to address it',\n",
       " 'wills if you look at the comments from the previous year what they wanted you all to address it seems',\n",
       " 'if you look at the comments from the previous year what they wanted you all to address it seems like',\n",
       " 'you look at the comments from the previous year what they wanted you all to address it seems like there',\n",
       " 'look at the comments from the previous year what they wanted you all to address it seems like there was',\n",
       " 'at the comments from the previous year what they wanted you all to address it seems like there was a',\n",
       " 'the comments from the previous year what they wanted you all to address it seems like there was a massive',\n",
       " 'comments from the previous year what they wanted you all to address it seems like there was a massive amount',\n",
       " ...]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len_list = []\n",
    "for x in lines:\n",
    "    len_list.append(len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(len_list)[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 300\n",
    "MAX_SEQUENCE_LENGTH = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_11 (Embedding)     (None, 19, 300)           15000600  \n",
      "_________________________________________________________________\n",
      "bidirectional_11 (Bidirectio (None, 19, 200)           240600    \n",
      "_________________________________________________________________\n",
      "bidirectional_12 (Bidirectio (None, 200)               180600    \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 50002)             5050202   \n",
      "=================================================================\n",
      "Total params: 20,492,102\n",
      "Trainable params: 5,491,502\n",
      "Non-trainable params: 15,000,600\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "12637/12637 [==============================] - 48s 4ms/step - loss: 7.4284 - acc: 0.0494\n",
      "\n",
      "Epoch 00001: loss improved from inf to 7.42839, saving model to LSTM_basline.hdf5\n",
      "Epoch 2/100\n",
      "12637/12637 [==============================] - 43s 3ms/step - loss: 5.9424 - acc: 0.0545\n",
      "\n",
      "Epoch 00002: loss improved from 7.42839 to 5.94244, saving model to LSTM_basline.hdf5\n",
      "Epoch 3/100\n",
      "12637/12637 [==============================] - 44s 3ms/step - loss: 5.8696 - acc: 0.0545\n",
      "\n",
      "Epoch 00003: loss improved from 5.94244 to 5.86963, saving model to LSTM_basline.hdf5\n",
      "Epoch 4/100\n",
      "12637/12637 [==============================] - 43s 3ms/step - loss: 5.8214 - acc: 0.0545\n",
      "\n",
      "Epoch 00004: loss improved from 5.86963 to 5.82137, saving model to LSTM_basline.hdf5\n",
      "Epoch 5/100\n",
      "12637/12637 [==============================] - 42s 3ms/step - loss: 5.7404 - acc: 0.0569\n",
      "\n",
      "Epoch 00005: loss improved from 5.82137 to 5.74040, saving model to LSTM_basline.hdf5\n",
      "Epoch 6/100\n",
      "12637/12637 [==============================] - 42s 3ms/step - loss: 5.6423 - acc: 0.0588\n",
      "\n",
      "Epoch 00006: loss improved from 5.74040 to 5.64226, saving model to LSTM_basline.hdf5\n",
      "Epoch 7/100\n",
      "12637/12637 [==============================] - 43s 3ms/step - loss: 5.5452 - acc: 0.0641\n",
      "\n",
      "Epoch 00007: loss improved from 5.64226 to 5.54524, saving model to LSTM_basline.hdf5\n",
      "Epoch 8/100\n",
      "12637/12637 [==============================] - 42s 3ms/step - loss: 5.4625 - acc: 0.0672\n",
      "\n",
      "Epoch 00008: loss improved from 5.54524 to 5.46254, saving model to LSTM_basline.hdf5\n",
      "Epoch 9/100\n",
      "12637/12637 [==============================] - 42s 3ms/step - loss: 5.3963 - acc: 0.0686\n",
      "\n",
      "Epoch 00009: loss improved from 5.46254 to 5.39626, saving model to LSTM_basline.hdf5\n",
      "Epoch 10/100\n",
      "12637/12637 [==============================] - 42s 3ms/step - loss: 5.3295 - acc: 0.0696\n",
      "\n",
      "Epoch 00010: loss improved from 5.39626 to 5.32946, saving model to LSTM_basline.hdf5\n",
      "Epoch 11/100\n",
      "12637/12637 [==============================] - 42s 3ms/step - loss: 5.2662 - acc: 0.0748\n",
      "\n",
      "Epoch 00011: loss improved from 5.32946 to 5.26622, saving model to LSTM_basline.hdf5\n",
      "Epoch 12/100\n",
      "12637/12637 [==============================] - 42s 3ms/step - loss: 5.1908 - acc: 0.0833\n",
      "\n",
      "Epoch 00012: loss improved from 5.26622 to 5.19077, saving model to LSTM_basline.hdf5\n",
      "Epoch 13/100\n",
      "12637/12637 [==============================] - 42s 3ms/step - loss: 5.1050 - acc: 0.0909\n",
      "\n",
      "Epoch 00013: loss improved from 5.19077 to 5.10496, saving model to LSTM_basline.hdf5\n",
      "Epoch 14/100\n",
      "12637/12637 [==============================] - 41s 3ms/step - loss: 5.0245 - acc: 0.0939\n",
      "\n",
      "Epoch 00014: loss improved from 5.10496 to 5.02447, saving model to LSTM_basline.hdf5\n",
      "Epoch 15/100\n",
      "12637/12637 [==============================] - 41s 3ms/step - loss: 4.9398 - acc: 0.0986\n",
      "\n",
      "Epoch 00015: loss improved from 5.02447 to 4.93982, saving model to LSTM_basline.hdf5\n",
      "Epoch 16/100\n",
      "12637/12637 [==============================] - 41s 3ms/step - loss: 4.8482 - acc: 0.1059\n",
      "\n",
      "Epoch 00016: loss improved from 4.93982 to 4.84819, saving model to LSTM_basline.hdf5\n",
      "Epoch 17/100\n",
      "12637/12637 [==============================] - 42s 3ms/step - loss: 4.7555 - acc: 0.1140\n",
      "\n",
      "Epoch 00017: loss improved from 4.84819 to 4.75552, saving model to LSTM_basline.hdf5\n",
      "Epoch 18/100\n",
      "12637/12637 [==============================] - 41s 3ms/step - loss: 4.6611 - acc: 0.1204\n",
      "\n",
      "Epoch 00018: loss improved from 4.75552 to 4.66109, saving model to LSTM_basline.hdf5\n",
      "Epoch 19/100\n",
      "12637/12637 [==============================] - 42s 3ms/step - loss: 4.5647 - acc: 0.1270\n",
      "\n",
      "Epoch 00019: loss improved from 4.66109 to 4.56474, saving model to LSTM_basline.hdf5\n",
      "Epoch 20/100\n",
      "12637/12637 [==============================] - 41s 3ms/step - loss: 4.4643 - acc: 0.1382\n",
      "\n",
      "Epoch 00020: loss improved from 4.56474 to 4.46425, saving model to LSTM_basline.hdf5\n",
      "Epoch 21/100\n",
      "12637/12637 [==============================] - 41s 3ms/step - loss: 4.3648 - acc: 0.1473\n",
      "\n",
      "Epoch 00021: loss improved from 4.46425 to 4.36483, saving model to LSTM_basline.hdf5\n",
      "Epoch 22/100\n",
      "12637/12637 [==============================] - 41s 3ms/step - loss: 4.2568 - acc: 0.1563\n",
      "\n",
      "Epoch 00022: loss improved from 4.36483 to 4.25683, saving model to LSTM_basline.hdf5\n",
      "Epoch 23/100\n",
      "12637/12637 [==============================] - 40s 3ms/step - loss: 4.1496 - acc: 0.1631\n",
      "\n",
      "Epoch 00023: loss improved from 4.25683 to 4.14964, saving model to LSTM_basline.hdf5\n",
      "Epoch 24/100\n",
      "12637/12637 [==============================] - 40s 3ms/step - loss: 4.0368 - acc: 0.1755\n",
      "\n",
      "Epoch 00024: loss improved from 4.14964 to 4.03684, saving model to LSTM_basline.hdf5\n",
      "Epoch 25/100\n",
      "12637/12637 [==============================] - 40s 3ms/step - loss: 3.9275 - acc: 0.1857\n",
      "\n",
      "Epoch 00025: loss improved from 4.03684 to 3.92753, saving model to LSTM_basline.hdf5\n",
      "Epoch 26/100\n",
      "12637/12637 [==============================] - 40s 3ms/step - loss: 3.8130 - acc: 0.1953\n",
      "\n",
      "Epoch 00026: loss improved from 3.92753 to 3.81302, saving model to LSTM_basline.hdf5\n",
      "Epoch 27/100\n",
      "12637/12637 [==============================] - 41s 3ms/step - loss: 3.6963 - acc: 0.2081\n",
      "\n",
      "Epoch 00027: loss improved from 3.81302 to 3.69629, saving model to LSTM_basline.hdf5\n",
      "Epoch 28/100\n",
      "12637/12637 [==============================] - 784s 62ms/step - loss: 3.5875 - acc: 0.2241\n",
      "\n",
      "Epoch 00028: loss improved from 3.69629 to 3.58746, saving model to LSTM_basline.hdf5\n",
      "Epoch 29/100\n",
      "12637/12637 [==============================] - 42s 3ms/step - loss: 3.4581 - acc: 0.2383\n",
      "\n",
      "Epoch 00029: loss improved from 3.58746 to 3.45808, saving model to LSTM_basline.hdf5\n",
      "Epoch 30/100\n",
      "12637/12637 [==============================] - 41s 3ms/step - loss: 3.3363 - acc: 0.2540\n",
      "\n",
      "Epoch 00030: loss improved from 3.45808 to 3.33630, saving model to LSTM_basline.hdf5\n",
      "Epoch 31/100\n",
      "12637/12637 [==============================] - 41s 3ms/step - loss: 3.2281 - acc: 0.2743\n",
      "\n",
      "Epoch 00031: loss improved from 3.33630 to 3.22806, saving model to LSTM_basline.hdf5\n",
      "Epoch 32/100\n",
      "12637/12637 [==============================] - 40s 3ms/step - loss: 3.1009 - acc: 0.2939\n",
      "\n",
      "Epoch 00032: loss improved from 3.22806 to 3.10091, saving model to LSTM_basline.hdf5\n",
      "Epoch 33/100\n",
      "12637/12637 [==============================] - 41s 3ms/step - loss: 2.9701 - acc: 0.3141\n",
      "\n",
      "Epoch 00033: loss improved from 3.10091 to 2.97007, saving model to LSTM_basline.hdf5\n",
      "Epoch 34/100\n",
      "12637/12637 [==============================] - 40s 3ms/step - loss: 2.8376 - acc: 0.3388\n",
      "\n",
      "Epoch 00034: loss improved from 2.97007 to 2.83756, saving model to LSTM_basline.hdf5\n",
      "Epoch 35/100\n",
      "12637/12637 [==============================] - 40s 3ms/step - loss: 2.7246 - acc: 0.3606\n",
      "\n",
      "Epoch 00035: loss improved from 2.83756 to 2.72455, saving model to LSTM_basline.hdf5\n",
      "Epoch 36/100\n",
      "12637/12637 [==============================] - 40s 3ms/step - loss: 2.5994 - acc: 0.3879\n",
      "\n",
      "Epoch 00036: loss improved from 2.72455 to 2.59936, saving model to LSTM_basline.hdf5\n",
      "Epoch 37/100\n",
      "12637/12637 [==============================] - 40s 3ms/step - loss: 2.4735 - acc: 0.4137\n",
      "\n",
      "Epoch 00037: loss improved from 2.59936 to 2.47347, saving model to LSTM_basline.hdf5\n",
      "Epoch 38/100\n",
      "12637/12637 [==============================] - 40s 3ms/step - loss: 2.3608 - acc: 0.4289\n",
      "\n",
      "Epoch 00038: loss improved from 2.47347 to 2.36079, saving model to LSTM_basline.hdf5\n",
      "Epoch 39/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12637/12637 [==============================] - 41s 3ms/step - loss: 2.2386 - acc: 0.4609\n",
      "\n",
      "Epoch 00039: loss improved from 2.36079 to 2.23857, saving model to LSTM_basline.hdf5\n",
      "Epoch 40/100\n",
      "12637/12637 [==============================] - 40s 3ms/step - loss: 2.1035 - acc: 0.4958\n",
      "\n",
      "Epoch 00040: loss improved from 2.23857 to 2.10347, saving model to LSTM_basline.hdf5\n",
      "Epoch 41/100\n",
      "12637/12637 [==============================] - 40s 3ms/step - loss: 2.0063 - acc: 0.5118\n",
      "\n",
      "Epoch 00041: loss improved from 2.10347 to 2.00631, saving model to LSTM_basline.hdf5\n",
      "Epoch 42/100\n",
      "12637/12637 [==============================] - 40s 3ms/step - loss: 1.8799 - acc: 0.5425\n",
      "\n",
      "Epoch 00042: loss improved from 2.00631 to 1.87989, saving model to LSTM_basline.hdf5\n",
      "Epoch 43/100\n",
      "12637/12637 [==============================] - 41s 3ms/step - loss: 1.7701 - acc: 0.5739\n",
      "\n",
      "Epoch 00043: loss improved from 1.87989 to 1.77014, saving model to LSTM_basline.hdf5\n",
      "Epoch 44/100\n",
      "12637/12637 [==============================] - 42s 3ms/step - loss: 1.6447 - acc: 0.6074\n",
      "\n",
      "Epoch 00044: loss improved from 1.77014 to 1.64470, saving model to LSTM_basline.hdf5\n",
      "Epoch 45/100\n",
      "12637/12637 [==============================] - 41s 3ms/step - loss: 1.5724 - acc: 0.6188\n",
      "\n",
      "Epoch 00045: loss improved from 1.64470 to 1.57237, saving model to LSTM_basline.hdf5\n",
      "Epoch 46/100\n",
      "12637/12637 [==============================] - 41s 3ms/step - loss: 1.4325 - acc: 0.6636\n",
      "\n",
      "Epoch 00046: loss improved from 1.57237 to 1.43255, saving model to LSTM_basline.hdf5\n",
      "Epoch 47/100\n",
      "12637/12637 [==============================] - 380s 30ms/step - loss: 1.3283 - acc: 0.6852\n",
      "\n",
      "Epoch 00047: loss improved from 1.43255 to 1.32829, saving model to LSTM_basline.hdf5\n",
      "Epoch 48/100\n",
      "12637/12637 [==============================] - 41s 3ms/step - loss: 1.2356 - acc: 0.7129\n",
      "\n",
      "Epoch 00048: loss improved from 1.32829 to 1.23555, saving model to LSTM_basline.hdf5\n",
      "Epoch 49/100\n",
      "12637/12637 [==============================] - 40s 3ms/step - loss: 1.1601 - acc: 0.7324\n",
      "\n",
      "Epoch 00049: loss improved from 1.23555 to 1.16009, saving model to LSTM_basline.hdf5\n",
      "Epoch 50/100\n",
      "12637/12637 [==============================] - 40s 3ms/step - loss: 1.0907 - acc: 0.7502\n",
      "\n",
      "Epoch 00050: loss improved from 1.16009 to 1.09074, saving model to LSTM_basline.hdf5\n",
      "Epoch 51/100\n",
      "12637/12637 [==============================] - 40s 3ms/step - loss: 0.9997 - acc: 0.7714\n",
      "\n",
      "Epoch 00051: loss improved from 1.09074 to 0.99971, saving model to LSTM_basline.hdf5\n",
      "Epoch 52/100\n",
      "12637/12637 [==============================] - 40s 3ms/step - loss: 0.8980 - acc: 0.8028\n",
      "\n",
      "Epoch 00052: loss improved from 0.99971 to 0.89804, saving model to LSTM_basline.hdf5\n",
      "Epoch 53/100\n",
      "12637/12637 [==============================] - 40s 3ms/step - loss: 0.8113 - acc: 0.8271\n",
      "\n",
      "Epoch 00053: loss improved from 0.89804 to 0.81125, saving model to LSTM_basline.hdf5\n",
      "Epoch 54/100\n",
      "12637/12637 [==============================] - 41s 3ms/step - loss: 0.7420 - acc: 0.8437\n",
      "\n",
      "Epoch 00054: loss improved from 0.81125 to 0.74202, saving model to LSTM_basline.hdf5\n",
      "Epoch 55/100\n",
      "12637/12637 [==============================] - 40s 3ms/step - loss: 0.6729 - acc: 0.8631\n",
      "\n",
      "Epoch 00055: loss improved from 0.74202 to 0.67285, saving model to LSTM_basline.hdf5\n",
      "Epoch 56/100\n",
      "12637/12637 [==============================] - 40s 3ms/step - loss: 0.6135 - acc: 0.8800\n",
      "\n",
      "Epoch 00056: loss improved from 0.67285 to 0.61347, saving model to LSTM_basline.hdf5\n",
      "Epoch 57/100\n",
      "12637/12637 [==============================] - 40s 3ms/step - loss: 0.5602 - acc: 0.8938\n",
      "\n",
      "Epoch 00057: loss improved from 0.61347 to 0.56020, saving model to LSTM_basline.hdf5\n",
      "Epoch 58/100\n",
      "12637/12637 [==============================] - 40s 3ms/step - loss: 0.5194 - acc: 0.8990\n",
      "\n",
      "Epoch 00058: loss improved from 0.56020 to 0.51940, saving model to LSTM_basline.hdf5\n",
      "Epoch 59/100\n",
      "12637/12637 [==============================] - 40s 3ms/step - loss: 0.4561 - acc: 0.9206\n",
      "\n",
      "Epoch 00059: loss improved from 0.51940 to 0.45608, saving model to LSTM_basline.hdf5\n",
      "Epoch 60/100\n",
      "12637/12637 [==============================] - 40s 3ms/step - loss: 0.4142 - acc: 0.9295\n",
      "\n",
      "Epoch 00060: loss improved from 0.45608 to 0.41423, saving model to LSTM_basline.hdf5\n",
      "Epoch 61/100\n",
      "12637/12637 [==============================] - 40s 3ms/step - loss: 0.3869 - acc: 0.9356\n",
      "\n",
      "Epoch 00061: loss improved from 0.41423 to 0.38690, saving model to LSTM_basline.hdf5\n",
      "Epoch 62/100\n",
      "12637/12637 [==============================] - 41s 3ms/step - loss: 0.3491 - acc: 0.9464\n",
      "\n",
      "Epoch 00062: loss improved from 0.38690 to 0.34912, saving model to LSTM_basline.hdf5\n",
      "Epoch 63/100\n",
      "12637/12637 [==============================] - 2242s 177ms/step - loss: 0.3186 - acc: 0.9513\n",
      "\n",
      "Epoch 00063: loss improved from 0.34912 to 0.31858, saving model to LSTM_basline.hdf5\n",
      "Epoch 64/100\n",
      "12637/12637 [==============================] - 67s 5ms/step - loss: 0.2915 - acc: 0.9560\n",
      "\n",
      "Epoch 00064: loss improved from 0.31858 to 0.29149, saving model to LSTM_basline.hdf5\n",
      "Epoch 65/100\n",
      "12637/12637 [==============================] - 65s 5ms/step - loss: 0.2636 - acc: 0.9605\n",
      "\n",
      "Epoch 00065: loss improved from 0.29149 to 0.26364, saving model to LSTM_basline.hdf5\n",
      "Epoch 66/100\n",
      "12637/12637 [==============================] - 65s 5ms/step - loss: 0.2418 - acc: 0.9657\n",
      "\n",
      "Epoch 00066: loss improved from 0.26364 to 0.24183, saving model to LSTM_basline.hdf5\n",
      "Epoch 67/100\n",
      "12637/12637 [==============================] - 64s 5ms/step - loss: 0.2139 - acc: 0.9711\n",
      "\n",
      "Epoch 00067: loss improved from 0.24183 to 0.21391, saving model to LSTM_basline.hdf5\n",
      "Epoch 68/100\n",
      "12637/12637 [==============================] - 62s 5ms/step - loss: 0.1836 - acc: 0.9761\n",
      "\n",
      "Epoch 00068: loss improved from 0.21391 to 0.18363, saving model to LSTM_basline.hdf5\n",
      "Epoch 69/100\n",
      "12637/12637 [==============================] - 59s 5ms/step - loss: 0.1634 - acc: 0.9794\n",
      "\n",
      "Epoch 00069: loss improved from 0.18363 to 0.16339, saving model to LSTM_basline.hdf5\n",
      "Epoch 70/100\n",
      "12637/12637 [==============================] - 59s 5ms/step - loss: 0.1428 - acc: 0.9824\n",
      "\n",
      "Epoch 00070: loss improved from 0.16339 to 0.14276, saving model to LSTM_basline.hdf5\n",
      "Epoch 71/100\n",
      "12637/12637 [==============================] - 59s 5ms/step - loss: 0.1311 - acc: 0.9854\n",
      "\n",
      "Epoch 00071: loss improved from 0.14276 to 0.13112, saving model to LSTM_basline.hdf5\n",
      "Epoch 72/100\n",
      "12637/12637 [==============================] - 59s 5ms/step - loss: 0.1240 - acc: 0.9854\n",
      "\n",
      "Epoch 00072: loss improved from 0.13112 to 0.12400, saving model to LSTM_basline.hdf5\n",
      "Epoch 73/100\n",
      "12637/12637 [==============================] - 59s 5ms/step - loss: 0.1081 - acc: 0.9874\n",
      "\n",
      "Epoch 00073: loss improved from 0.12400 to 0.10809, saving model to LSTM_basline.hdf5\n",
      "Epoch 74/100\n",
      "12637/12637 [==============================] - 60s 5ms/step - loss: 0.0982 - acc: 0.9895\n",
      "\n",
      "Epoch 00074: loss improved from 0.10809 to 0.09824, saving model to LSTM_basline.hdf5\n",
      "Epoch 75/100\n",
      "12637/12637 [==============================] - 212s 17ms/step - loss: 0.0875 - acc: 0.9918\n",
      "\n",
      "Epoch 00075: loss improved from 0.09824 to 0.08753, saving model to LSTM_basline.hdf5\n",
      "Epoch 76/100\n",
      "12637/12637 [==============================] - 164s 13ms/step - loss: 0.0778 - acc: 0.9928\n",
      "\n",
      "Epoch 00076: loss improved from 0.08753 to 0.07780, saving model to LSTM_basline.hdf5\n",
      "Epoch 77/100\n",
      "12637/12637 [==============================] - 94s 7ms/step - loss: 0.0692 - acc: 0.9940\n",
      "\n",
      "Epoch 00077: loss improved from 0.07780 to 0.06917, saving model to LSTM_basline.hdf5\n",
      "Epoch 78/100\n",
      "12637/12637 [==============================] - 130s 10ms/step - loss: 0.0655 - acc: 0.9953\n",
      "\n",
      "Epoch 00078: loss improved from 0.06917 to 0.06545, saving model to LSTM_basline.hdf5\n",
      "Epoch 79/100\n",
      "12637/12637 [==============================] - 133s 11ms/step - loss: 0.0572 - acc: 0.9958\n",
      "\n",
      "Epoch 00079: loss improved from 0.06545 to 0.05718, saving model to LSTM_basline.hdf5\n",
      "Epoch 80/100\n",
      "12637/12637 [==============================] - 104s 8ms/step - loss: 0.0511 - acc: 0.9967\n",
      "\n",
      "Epoch 00080: loss improved from 0.05718 to 0.05111, saving model to LSTM_basline.hdf5\n",
      "Epoch 81/100\n",
      "12637/12637 [==============================] - 116s 9ms/step - loss: 0.0459 - acc: 0.9978\n",
      "\n",
      "Epoch 00081: loss improved from 0.05111 to 0.04590, saving model to LSTM_basline.hdf5\n",
      "Epoch 82/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12637/12637 [==============================] - 133s 10ms/step - loss: 0.0460 - acc: 0.9973\n",
      "\n",
      "Epoch 00082: loss did not improve from 0.04590\n",
      "Epoch 83/100\n",
      "12637/12637 [==============================] - 117s 9ms/step - loss: 0.0407 - acc: 0.9984\n",
      "\n",
      "Epoch 00083: loss improved from 0.04590 to 0.04075, saving model to LSTM_basline.hdf5\n",
      "Epoch 84/100\n",
      "12637/12637 [==============================] - 115s 9ms/step - loss: 0.0345 - acc: 0.9990\n",
      "\n",
      "Epoch 00084: loss improved from 0.04075 to 0.03447, saving model to LSTM_basline.hdf5\n",
      "Epoch 85/100\n",
      "12637/12637 [==============================] - 124s 10ms/step - loss: 0.0309 - acc: 0.9992\n",
      "\n",
      "Epoch 00085: loss improved from 0.03447 to 0.03094, saving model to LSTM_basline.hdf5\n",
      "Epoch 86/100\n",
      "12637/12637 [==============================] - 120s 10ms/step - loss: 0.0271 - acc: 0.9996\n",
      "\n",
      "Epoch 00086: loss improved from 0.03094 to 0.02711, saving model to LSTM_basline.hdf5\n",
      "Epoch 87/100\n",
      "12637/12637 [==============================] - 115s 9ms/step - loss: 0.0263 - acc: 0.9996\n",
      "\n",
      "Epoch 00087: loss improved from 0.02711 to 0.02630, saving model to LSTM_basline.hdf5\n",
      "Epoch 88/100\n",
      "12637/12637 [==============================] - 127s 10ms/step - loss: 0.0234 - acc: 0.9996\n",
      "\n",
      "Epoch 00088: loss improved from 0.02630 to 0.02336, saving model to LSTM_basline.hdf5\n",
      "Epoch 89/100\n",
      "12637/12637 [==============================] - 124s 10ms/step - loss: 0.0204 - acc: 0.9998\n",
      "\n",
      "Epoch 00089: loss improved from 0.02336 to 0.02038, saving model to LSTM_basline.hdf5\n",
      "Epoch 90/100\n",
      "12637/12637 [==============================] - 115s 9ms/step - loss: 0.0676 - acc: 0.9869\n",
      "\n",
      "Epoch 00090: loss did not improve from 0.02038\n",
      "Epoch 91/100\n",
      "12637/12637 [==============================] - 126s 10ms/step - loss: 1.3489 - acc: 0.6539\n",
      "\n",
      "Epoch 00091: loss did not improve from 0.02038\n",
      "Epoch 92/100\n",
      "12637/12637 [==============================] - 128s 10ms/step - loss: 0.7436 - acc: 0.7885\n",
      "\n",
      "Epoch 00092: loss did not improve from 0.02038\n",
      "Epoch 93/100\n",
      "12637/12637 [==============================] - 120s 9ms/step - loss: 0.2586 - acc: 0.9367\n",
      "\n",
      "Epoch 00093: loss did not improve from 0.02038\n",
      "Epoch 94/100\n",
      "12637/12637 [==============================] - 122s 10ms/step - loss: 0.0970 - acc: 0.9899\n",
      "\n",
      "Epoch 00094: loss did not improve from 0.02038\n",
      "Epoch 95/100\n",
      "12637/12637 [==============================] - 123s 10ms/step - loss: 0.0458 - acc: 0.9989\n",
      "\n",
      "Epoch 00095: loss did not improve from 0.02038\n",
      "Epoch 96/100\n",
      "12637/12637 [==============================] - 122s 10ms/step - loss: 0.0315 - acc: 0.9995\n",
      "\n",
      "Epoch 00096: loss did not improve from 0.02038\n",
      "Epoch 97/100\n",
      "12637/12637 [==============================] - 122s 10ms/step - loss: 0.0269 - acc: 0.9997\n",
      "\n",
      "Epoch 00097: loss did not improve from 0.02038\n",
      "Epoch 98/100\n",
      "12637/12637 [==============================] - 127s 10ms/step - loss: 0.0234 - acc: 0.9996\n",
      "\n",
      "Epoch 00098: loss did not improve from 0.02038\n",
      "Epoch 99/100\n",
      "12637/12637 [==============================] - 122s 10ms/step - loss: 0.0210 - acc: 0.9997\n",
      "\n",
      "Epoch 00099: loss did not improve from 0.02038\n",
      "Epoch 100/100\n",
      "12637/12637 [==============================] - 120s 9ms/step - loss: 0.0192 - acc: 0.9998\n",
      "\n",
      "Epoch 00100: loss improved from 0.02038 to 0.01916, saving model to LSTM_basline.hdf5\n"
     ]
    }
   ],
   "source": [
    "# integer encode sequences of words\n",
    "# tokenizer = Tokenizer()\n",
    "# tokenizer.fit_on_texts(lines) #fit on texts\n",
    "# sequences = tokenizer.texts_to_sequences(lines)\n",
    "# vocab_size = len(tokenizer.word_index) + 1\n",
    "MAX_NUM_WORDS = words_to_load\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(lines)\n",
    "sequences = tokenizer.texts_to_sequences(lines)\n",
    "word_index = tokenizer.word_index\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "X, y = data[:,:-1], data[:,-1]\n",
    "vocab_size = len(words_ft)\n",
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "seq_length = X.shape[1]\n",
    " \n",
    "# define model\n",
    "model = Sequential()\n",
    "embedding_layer = Embedding(vocab_size,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[loaded_embeddings_ft],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH-1,\n",
    "                            trainable=False)\n",
    "model.add(embedding_layer)\n",
    "model.add(Bidirectional(GRU(100, return_sequences=True)))\n",
    "model.add(Bidirectional(GRU(100)))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "\n",
    "#import the checkpoint to save current model\n",
    "filepath=\"GRU_0.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "# compile model\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "# fit the model\n",
    "model.fit(X, y, batch_size=200, epochs=100, callbacks=callbacks_list)\n",
    "\n",
    "# categorical_crossentropy\n",
    " \n",
    "# save the model to file\n",
    "model.save('model.h5')\n",
    "# save the tokenizer\n",
    "plk.dump(tokenizer, open('tokenizer.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    " \n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    file = open(filename, 'r')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text\n",
    " \n",
    "# generate a sequence from a language model\n",
    "def generate_seq(model, tokenizer, seq_length, seed_text, n_words):\n",
    "    result = list()\n",
    "    in_text = seed_text\n",
    "    for _ in range(n_words):\n",
    "    # encode the text as integer\n",
    "        encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        # truncate sequences to a fixed length\n",
    "        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
    "        # predict probabilities for each word\n",
    "        yhat = model.predict_classes(encoded, verbose=0)\n",
    "        # map predicted word index to word\n",
    "        out_word = ''\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == yhat:\n",
    "                out_word = word\n",
    "                break\n",
    "    # append to input\n",
    "        in_text += ' ' + out_word\n",
    "        result.append(out_word)\n",
    "    return ' '.join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "but in your text you point out low rates and slow economic growth as headwinds for ficc you talked about\n",
      "\n",
      "lower market volumes and volatility and equities i just curious when you look at the quarter we just had in ficc particularly is it more like you had a nice pickup like and you setting us up for keep calm like things have returned back to normal or is this\n"
     ]
    }
   ],
   "source": [
    "#test 1\n",
    "# load cleaned text sequences\n",
    "in_filename = 'Glenn_Schorr_sequences.txt'\n",
    "doc = load_doc(in_filename)\n",
    "lines = doc.split('\\n')\n",
    "seq_length = len(lines[0].split()) - 1\n",
    " \n",
    "# load the model\n",
    "model = load_model('GRU_0.hdf5')\n",
    " \n",
    "# load the tokenizer\n",
    "tokenizer = plk.load(open('tokenizer.pkl', 'rb'))\n",
    " \n",
    "# select a seed text\n",
    "seed_text = lines[randint(0,len(lines))]\n",
    "print(seed_text + '\\n')\n",
    " \n",
    "# generate new text\n",
    "generated = generate_seq(model, tokenizer, seq_length, seed_text, 50)\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "12004/12004 [==============================] - 18s 2ms/step - loss: 3.3733 - acc: 0.2468\n",
      "\n",
      "Epoch 00001: loss did not improve from 3.25588\n",
      "Epoch 2/500\n",
      "12004/12004 [==============================] - 12s 1ms/step - loss: 3.2386 - acc: 0.2646\n",
      "\n",
      "Epoch 00002: loss improved from 3.25588 to 3.23858, saving model to LSTM_basline.hdf5\n",
      "Epoch 3/500\n",
      "12004/12004 [==============================] - 12s 1ms/step - loss: 3.2111 - acc: 0.2687\n",
      "\n",
      "Epoch 00003: loss improved from 3.23858 to 3.21107, saving model to LSTM_basline.hdf5\n",
      "Epoch 4/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 3.1937 - acc: 0.2726\n",
      "\n",
      "Epoch 00004: loss improved from 3.21107 to 3.19372, saving model to LSTM_basline.hdf5\n",
      "Epoch 5/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 3.1613 - acc: 0.2755\n",
      "\n",
      "Epoch 00005: loss improved from 3.19372 to 3.16126, saving model to LSTM_basline.hdf5\n",
      "Epoch 6/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 3.1517 - acc: 0.2795\n",
      "\n",
      "Epoch 00006: loss improved from 3.16126 to 3.15166, saving model to LSTM_basline.hdf5\n",
      "Epoch 7/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 3.1368 - acc: 0.2837\n",
      "\n",
      "Epoch 00007: loss improved from 3.15166 to 3.13680, saving model to LSTM_basline.hdf5\n",
      "Epoch 8/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 3.1277 - acc: 0.2837\n",
      "\n",
      "Epoch 00008: loss improved from 3.13680 to 3.12772, saving model to LSTM_basline.hdf5\n",
      "Epoch 9/500\n",
      "12004/12004 [==============================] - 12s 1ms/step - loss: 3.1003 - acc: 0.2884\n",
      "\n",
      "Epoch 00009: loss improved from 3.12772 to 3.10034, saving model to LSTM_basline.hdf5\n",
      "Epoch 10/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 3.0912 - acc: 0.2872\n",
      "\n",
      "Epoch 00010: loss improved from 3.10034 to 3.09116, saving model to LSTM_basline.hdf5\n",
      "Epoch 11/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 3.1042 - acc: 0.2807\n",
      "\n",
      "Epoch 00011: loss did not improve from 3.09116\n",
      "Epoch 12/500\n",
      "12004/12004 [==============================] - 12s 1ms/step - loss: 3.0543 - acc: 0.2919\n",
      "\n",
      "Epoch 00012: loss improved from 3.09116 to 3.05433, saving model to LSTM_basline.hdf5\n",
      "Epoch 13/500\n",
      "12004/12004 [==============================] - 12s 1ms/step - loss: 3.0469 - acc: 0.2962\n",
      "\n",
      "Epoch 00013: loss improved from 3.05433 to 3.04688, saving model to LSTM_basline.hdf5\n",
      "Epoch 14/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 3.0372 - acc: 0.2974\n",
      "\n",
      "Epoch 00014: loss improved from 3.04688 to 3.03725, saving model to LSTM_basline.hdf5\n",
      "Epoch 15/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 3.0147 - acc: 0.3001\n",
      "\n",
      "Epoch 00015: loss improved from 3.03725 to 3.01472, saving model to LSTM_basline.hdf5\n",
      "Epoch 16/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 2.9963 - acc: 0.3071\n",
      "\n",
      "Epoch 00016: loss improved from 3.01472 to 2.99628, saving model to LSTM_basline.hdf5\n",
      "Epoch 17/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 3.0016 - acc: 0.3018\n",
      "\n",
      "Epoch 00017: loss did not improve from 2.99628\n",
      "Epoch 18/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 2.9748 - acc: 0.3065\n",
      "\n",
      "Epoch 00018: loss improved from 2.99628 to 2.97476, saving model to LSTM_basline.hdf5\n",
      "Epoch 19/500\n",
      "12004/12004 [==============================] - 12s 1ms/step - loss: 2.9606 - acc: 0.3101\n",
      "\n",
      "Epoch 00019: loss improved from 2.97476 to 2.96064, saving model to LSTM_basline.hdf5\n",
      "Epoch 20/500\n",
      "12004/12004 [==============================] - 12s 1ms/step - loss: 2.9430 - acc: 0.3168\n",
      "\n",
      "Epoch 00020: loss improved from 2.96064 to 2.94305, saving model to LSTM_basline.hdf5\n",
      "Epoch 21/500\n",
      "12004/12004 [==============================] - 12s 1ms/step - loss: 2.9430 - acc: 0.3156\n",
      "\n",
      "Epoch 00021: loss improved from 2.94305 to 2.94303, saving model to LSTM_basline.hdf5\n",
      "Epoch 22/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 2.9247 - acc: 0.3151\n",
      "\n",
      "Epoch 00022: loss improved from 2.94303 to 2.92467, saving model to LSTM_basline.hdf5\n",
      "Epoch 23/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 2.9150 - acc: 0.3166\n",
      "\n",
      "Epoch 00023: loss improved from 2.92467 to 2.91505, saving model to LSTM_basline.hdf5\n",
      "Epoch 24/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 2.9156 - acc: 0.3174\n",
      "\n",
      "Epoch 00024: loss did not improve from 2.91505\n",
      "Epoch 25/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 2.8777 - acc: 0.3301\n",
      "\n",
      "Epoch 00025: loss improved from 2.91505 to 2.87766, saving model to LSTM_basline.hdf5\n",
      "Epoch 26/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 2.8804 - acc: 0.3246\n",
      "\n",
      "Epoch 00026: loss did not improve from 2.87766\n",
      "Epoch 27/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 2.8787 - acc: 0.3237\n",
      "\n",
      "Epoch 00027: loss did not improve from 2.87766\n",
      "Epoch 28/500\n",
      "12004/12004 [==============================] - 12s 1ms/step - loss: 2.8493 - acc: 0.3332\n",
      "\n",
      "Epoch 00028: loss improved from 2.87766 to 2.84933, saving model to LSTM_basline.hdf5\n",
      "Epoch 29/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 2.8379 - acc: 0.3364\n",
      "\n",
      "Epoch 00029: loss improved from 2.84933 to 2.83791, saving model to LSTM_basline.hdf5\n",
      "Epoch 30/500\n",
      "12004/12004 [==============================] - 12s 1ms/step - loss: 2.8195 - acc: 0.3381\n",
      "\n",
      "Epoch 00030: loss improved from 2.83791 to 2.81951, saving model to LSTM_basline.hdf5\n",
      "Epoch 31/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 2.8242 - acc: 0.3333\n",
      "\n",
      "Epoch 00031: loss did not improve from 2.81951\n",
      "Epoch 32/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 2.8089 - acc: 0.3355\n",
      "\n",
      "Epoch 00032: loss improved from 2.81951 to 2.80893, saving model to LSTM_basline.hdf5\n",
      "Epoch 33/500\n",
      "12004/12004 [==============================] - 12s 1ms/step - loss: 2.7941 - acc: 0.3436\n",
      "\n",
      "Epoch 00033: loss improved from 2.80893 to 2.79409, saving model to LSTM_basline.hdf5\n",
      "Epoch 34/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 2.8166 - acc: 0.3373\n",
      "\n",
      "Epoch 00034: loss did not improve from 2.79409\n",
      "Epoch 35/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 2.7765 - acc: 0.3456\n",
      "\n",
      "Epoch 00035: loss improved from 2.79409 to 2.77651, saving model to LSTM_basline.hdf5\n",
      "Epoch 36/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 2.7757 - acc: 0.3468\n",
      "\n",
      "Epoch 00036: loss improved from 2.77651 to 2.77566, saving model to LSTM_basline.hdf5\n",
      "Epoch 37/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 2.7734 - acc: 0.3438\n",
      "\n",
      "Epoch 00037: loss improved from 2.77566 to 2.77344, saving model to LSTM_basline.hdf5\n",
      "Epoch 38/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 2.7388 - acc: 0.3517\n",
      "\n",
      "Epoch 00038: loss improved from 2.77344 to 2.73880, saving model to LSTM_basline.hdf5\n",
      "Epoch 39/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 2.7253 - acc: 0.3555\n",
      "\n",
      "Epoch 00039: loss improved from 2.73880 to 2.72531, saving model to LSTM_basline.hdf5\n",
      "Epoch 40/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 2.7271 - acc: 0.3551\n",
      "\n",
      "Epoch 00040: loss did not improve from 2.72531\n",
      "Epoch 41/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 2.7079 - acc: 0.3581\n",
      "\n",
      "Epoch 00041: loss improved from 2.72531 to 2.70788, saving model to LSTM_basline.hdf5\n",
      "Epoch 42/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 2.6877 - acc: 0.3598\n",
      "\n",
      "Epoch 00042: loss improved from 2.70788 to 2.68774, saving model to LSTM_basline.hdf5\n",
      "Epoch 43/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 2.6949 - acc: 0.3582\n",
      "\n",
      "Epoch 00043: loss did not improve from 2.68774\n",
      "Epoch 44/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 2.7018 - acc: 0.3564\n",
      "\n",
      "Epoch 00044: loss did not improve from 2.68774\n",
      "Epoch 45/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 2.6838 - acc: 0.3615\n",
      "\n",
      "Epoch 00045: loss improved from 2.68774 to 2.68375, saving model to LSTM_basline.hdf5\n",
      "Epoch 46/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 2.6612 - acc: 0.3691\n",
      "\n",
      "Epoch 00046: loss improved from 2.68375 to 2.66119, saving model to LSTM_basline.hdf5\n",
      "Epoch 47/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12004/12004 [==============================] - 13s 1ms/step - loss: 2.6505 - acc: 0.3688\n",
      "\n",
      "Epoch 00047: loss improved from 2.66119 to 2.65046, saving model to LSTM_basline.hdf5\n",
      "Epoch 48/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 2.6528 - acc: 0.3655\n",
      "\n",
      "Epoch 00048: loss did not improve from 2.65046\n",
      "Epoch 49/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 2.6265 - acc: 0.3700\n",
      "\n",
      "Epoch 00049: loss improved from 2.65046 to 2.62651, saving model to LSTM_basline.hdf5\n",
      "Epoch 50/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 2.6029 - acc: 0.3795\n",
      "\n",
      "Epoch 00050: loss improved from 2.62651 to 2.60288, saving model to LSTM_basline.hdf5\n",
      "Epoch 51/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 2.6177 - acc: 0.3751\n",
      "\n",
      "Epoch 00051: loss did not improve from 2.60288\n",
      "Epoch 52/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 2.6046 - acc: 0.3744\n",
      "\n",
      "Epoch 00052: loss did not improve from 2.60288\n",
      "Epoch 53/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 2.5947 - acc: 0.3798\n",
      "\n",
      "Epoch 00053: loss improved from 2.60288 to 2.59475, saving model to LSTM_basline.hdf5\n",
      "Epoch 54/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 2.5941 - acc: 0.3804\n",
      "\n",
      "Epoch 00054: loss improved from 2.59475 to 2.59408, saving model to LSTM_basline.hdf5\n",
      "Epoch 55/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 2.5795 - acc: 0.3839\n",
      "\n",
      "Epoch 00055: loss improved from 2.59408 to 2.57952, saving model to LSTM_basline.hdf5\n",
      "Epoch 56/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 2.5650 - acc: 0.3817\n",
      "\n",
      "Epoch 00056: loss improved from 2.57952 to 2.56503, saving model to LSTM_basline.hdf5\n",
      "Epoch 57/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 2.5676 - acc: 0.3813\n",
      "\n",
      "Epoch 00057: loss did not improve from 2.56503\n",
      "Epoch 58/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 2.5442 - acc: 0.3880\n",
      "\n",
      "Epoch 00058: loss improved from 2.56503 to 2.54420, saving model to LSTM_basline.hdf5\n",
      "Epoch 59/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 2.5174 - acc: 0.3933\n",
      "\n",
      "Epoch 00059: loss improved from 2.54420 to 2.51744, saving model to LSTM_basline.hdf5\n",
      "Epoch 60/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 2.5558 - acc: 0.3854\n",
      "\n",
      "Epoch 00060: loss did not improve from 2.51744\n",
      "Epoch 61/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 2.5266 - acc: 0.3906\n",
      "\n",
      "Epoch 00061: loss did not improve from 2.51744\n",
      "Epoch 62/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 2.4966 - acc: 0.3977\n",
      "\n",
      "Epoch 00062: loss improved from 2.51744 to 2.49659, saving model to LSTM_basline.hdf5\n",
      "Epoch 63/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 2.4869 - acc: 0.4005\n",
      "\n",
      "Epoch 00063: loss improved from 2.49659 to 2.48692, saving model to LSTM_basline.hdf5\n",
      "Epoch 64/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 2.4868 - acc: 0.3997\n",
      "\n",
      "Epoch 00064: loss improved from 2.48692 to 2.48677, saving model to LSTM_basline.hdf5\n",
      "Epoch 65/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 2.4711 - acc: 0.4044\n",
      "\n",
      "Epoch 00065: loss improved from 2.48677 to 2.47112, saving model to LSTM_basline.hdf5\n",
      "Epoch 66/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 2.4646 - acc: 0.4013\n",
      "\n",
      "Epoch 00066: loss improved from 2.47112 to 2.46458, saving model to LSTM_basline.hdf5\n",
      "Epoch 67/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 2.4460 - acc: 0.4045\n",
      "\n",
      "Epoch 00067: loss improved from 2.46458 to 2.44605, saving model to LSTM_basline.hdf5\n",
      "Epoch 68/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 2.4450 - acc: 0.4034\n",
      "\n",
      "Epoch 00068: loss improved from 2.44605 to 2.44501, saving model to LSTM_basline.hdf5\n",
      "Epoch 69/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 2.4400 - acc: 0.4107\n",
      "\n",
      "Epoch 00069: loss improved from 2.44501 to 2.44000, saving model to LSTM_basline.hdf5\n",
      "Epoch 70/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 2.4092 - acc: 0.4167\n",
      "\n",
      "Epoch 00070: loss improved from 2.44000 to 2.40922, saving model to LSTM_basline.hdf5\n",
      "Epoch 71/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 2.4075 - acc: 0.4150\n",
      "\n",
      "Epoch 00071: loss improved from 2.40922 to 2.40751, saving model to LSTM_basline.hdf5\n",
      "Epoch 72/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 2.3879 - acc: 0.4204\n",
      "\n",
      "Epoch 00072: loss improved from 2.40751 to 2.38792, saving model to LSTM_basline.hdf5\n",
      "Epoch 73/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 2.3870 - acc: 0.4188\n",
      "\n",
      "Epoch 00073: loss improved from 2.38792 to 2.38703, saving model to LSTM_basline.hdf5\n",
      "Epoch 74/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 2.3878 - acc: 0.4231\n",
      "\n",
      "Epoch 00074: loss did not improve from 2.38703\n",
      "Epoch 75/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 2.3815 - acc: 0.4194\n",
      "\n",
      "Epoch 00075: loss improved from 2.38703 to 2.38151, saving model to LSTM_basline.hdf5\n",
      "Epoch 76/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 2.3530 - acc: 0.4279\n",
      "\n",
      "Epoch 00076: loss improved from 2.38151 to 2.35302, saving model to LSTM_basline.hdf5\n",
      "Epoch 77/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 2.3778 - acc: 0.4217\n",
      "\n",
      "Epoch 00077: loss did not improve from 2.35302\n",
      "Epoch 78/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 2.3526 - acc: 0.4251\n",
      "\n",
      "Epoch 00078: loss improved from 2.35302 to 2.35257, saving model to LSTM_basline.hdf5\n",
      "Epoch 79/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 2.3572 - acc: 0.4232\n",
      "\n",
      "Epoch 00079: loss did not improve from 2.35257\n",
      "Epoch 80/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 2.3464 - acc: 0.4248\n",
      "\n",
      "Epoch 00080: loss improved from 2.35257 to 2.34636, saving model to LSTM_basline.hdf5\n",
      "Epoch 81/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 2.3269 - acc: 0.4312\n",
      "\n",
      "Epoch 00081: loss improved from 2.34636 to 2.32691, saving model to LSTM_basline.hdf5\n",
      "Epoch 82/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 2.3242 - acc: 0.4295\n",
      "\n",
      "Epoch 00082: loss improved from 2.32691 to 2.32416, saving model to LSTM_basline.hdf5\n",
      "Epoch 83/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 2.3123 - acc: 0.4336\n",
      "\n",
      "Epoch 00083: loss improved from 2.32416 to 2.31234, saving model to LSTM_basline.hdf5\n",
      "Epoch 84/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 2.2883 - acc: 0.4411\n",
      "\n",
      "Epoch 00084: loss improved from 2.31234 to 2.28833, saving model to LSTM_basline.hdf5\n",
      "Epoch 85/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 2.2763 - acc: 0.4446\n",
      "\n",
      "Epoch 00085: loss improved from 2.28833 to 2.27627, saving model to LSTM_basline.hdf5\n",
      "Epoch 86/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 2.2747 - acc: 0.4417\n",
      "\n",
      "Epoch 00086: loss improved from 2.27627 to 2.27467, saving model to LSTM_basline.hdf5\n",
      "Epoch 87/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 2.2587 - acc: 0.4414\n",
      "\n",
      "Epoch 00087: loss improved from 2.27467 to 2.25874, saving model to LSTM_basline.hdf5\n",
      "Epoch 88/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 2.2441 - acc: 0.4498\n",
      "\n",
      "Epoch 00088: loss improved from 2.25874 to 2.24408, saving model to LSTM_basline.hdf5\n",
      "Epoch 89/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 2.2250 - acc: 0.4531\n",
      "\n",
      "Epoch 00089: loss improved from 2.24408 to 2.22497, saving model to LSTM_basline.hdf5\n",
      "Epoch 90/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 2.2300 - acc: 0.4531\n",
      "\n",
      "Epoch 00090: loss did not improve from 2.22497\n",
      "Epoch 91/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 2.3297 - acc: 0.4225\n",
      "\n",
      "Epoch 00091: loss did not improve from 2.22497\n",
      "Epoch 92/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 2.2264 - acc: 0.4506\n",
      "\n",
      "Epoch 00092: loss did not improve from 2.22497\n",
      "Epoch 93/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12004/12004 [==============================] - 15s 1ms/step - loss: 2.2306 - acc: 0.4513\n",
      "\n",
      "Epoch 00093: loss did not improve from 2.22497\n",
      "Epoch 94/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 2.1885 - acc: 0.4607\n",
      "\n",
      "Epoch 00094: loss improved from 2.22497 to 2.18847, saving model to LSTM_basline.hdf5\n",
      "Epoch 95/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 2.1967 - acc: 0.4597\n",
      "\n",
      "Epoch 00095: loss did not improve from 2.18847\n",
      "Epoch 96/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 2.1658 - acc: 0.4667\n",
      "\n",
      "Epoch 00096: loss improved from 2.18847 to 2.16585, saving model to LSTM_basline.hdf5\n",
      "Epoch 97/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 2.1459 - acc: 0.4714\n",
      "\n",
      "Epoch 00097: loss improved from 2.16585 to 2.14593, saving model to LSTM_basline.hdf5\n",
      "Epoch 98/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 2.1837 - acc: 0.4608\n",
      "\n",
      "Epoch 00098: loss did not improve from 2.14593\n",
      "Epoch 99/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 2.1540 - acc: 0.4678\n",
      "\n",
      "Epoch 00099: loss did not improve from 2.14593\n",
      "Epoch 100/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 2.1255 - acc: 0.4766\n",
      "\n",
      "Epoch 00100: loss improved from 2.14593 to 2.12553, saving model to LSTM_basline.hdf5\n",
      "Epoch 101/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 2.1343 - acc: 0.4723\n",
      "\n",
      "Epoch 00101: loss did not improve from 2.12553\n",
      "Epoch 102/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 2.1381 - acc: 0.4698\n",
      "\n",
      "Epoch 00102: loss did not improve from 2.12553\n",
      "Epoch 103/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 2.0994 - acc: 0.4803\n",
      "\n",
      "Epoch 00103: loss improved from 2.12553 to 2.09937, saving model to LSTM_basline.hdf5\n",
      "Epoch 104/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 2.0792 - acc: 0.4850\n",
      "\n",
      "Epoch 00104: loss improved from 2.09937 to 2.07917, saving model to LSTM_basline.hdf5\n",
      "Epoch 105/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 2.1004 - acc: 0.4788\n",
      "\n",
      "Epoch 00105: loss did not improve from 2.07917\n",
      "Epoch 106/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 2.0798 - acc: 0.4858\n",
      "\n",
      "Epoch 00106: loss did not improve from 2.07917\n",
      "Epoch 107/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 2.0601 - acc: 0.4897\n",
      "\n",
      "Epoch 00107: loss improved from 2.07917 to 2.06006, saving model to LSTM_basline.hdf5\n",
      "Epoch 108/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 2.0441 - acc: 0.4942\n",
      "\n",
      "Epoch 00108: loss improved from 2.06006 to 2.04408, saving model to LSTM_basline.hdf5\n",
      "Epoch 109/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 2.0517 - acc: 0.4896\n",
      "\n",
      "Epoch 00109: loss did not improve from 2.04408\n",
      "Epoch 110/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 2.0273 - acc: 0.4958\n",
      "\n",
      "Epoch 00110: loss improved from 2.04408 to 2.02726, saving model to LSTM_basline.hdf5\n",
      "Epoch 111/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 2.0203 - acc: 0.4987\n",
      "\n",
      "Epoch 00111: loss improved from 2.02726 to 2.02030, saving model to LSTM_basline.hdf5\n",
      "Epoch 112/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 2.0270 - acc: 0.4958\n",
      "\n",
      "Epoch 00112: loss did not improve from 2.02030\n",
      "Epoch 113/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 2.0108 - acc: 0.4988\n",
      "\n",
      "Epoch 00113: loss improved from 2.02030 to 2.01079, saving model to LSTM_basline.hdf5\n",
      "Epoch 114/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.9923 - acc: 0.5025\n",
      "\n",
      "Epoch 00114: loss improved from 2.01079 to 1.99226, saving model to LSTM_basline.hdf5\n",
      "Epoch 115/500\n",
      "12004/12004 [==============================] - 15s 1ms/step - loss: 2.0282 - acc: 0.4933\n",
      "\n",
      "Epoch 00115: loss did not improve from 1.99226\n",
      "Epoch 116/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.9773 - acc: 0.5087\n",
      "\n",
      "Epoch 00116: loss improved from 1.99226 to 1.97730, saving model to LSTM_basline.hdf5\n",
      "Epoch 117/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 2.0154 - acc: 0.4938\n",
      "\n",
      "Epoch 00117: loss did not improve from 1.97730\n",
      "Epoch 118/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 2.0800 - acc: 0.4754\n",
      "\n",
      "Epoch 00118: loss did not improve from 1.97730\n",
      "Epoch 119/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 2.0045 - acc: 0.4961\n",
      "\n",
      "Epoch 00119: loss did not improve from 1.97730\n",
      "Epoch 120/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 1.9959 - acc: 0.4965\n",
      "\n",
      "Epoch 00120: loss did not improve from 1.97730\n",
      "Epoch 121/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.9741 - acc: 0.5079\n",
      "\n",
      "Epoch 00121: loss improved from 1.97730 to 1.97413, saving model to LSTM_basline.hdf5\n",
      "Epoch 122/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.9678 - acc: 0.5027\n",
      "\n",
      "Epoch 00122: loss improved from 1.97413 to 1.96779, saving model to LSTM_basline.hdf5\n",
      "Epoch 123/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.9524 - acc: 0.5128\n",
      "\n",
      "Epoch 00123: loss improved from 1.96779 to 1.95240, saving model to LSTM_basline.hdf5\n",
      "Epoch 124/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.9160 - acc: 0.5223\n",
      "\n",
      "Epoch 00124: loss improved from 1.95240 to 1.91602, saving model to LSTM_basline.hdf5\n",
      "Epoch 125/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.8929 - acc: 0.5259\n",
      "\n",
      "Epoch 00125: loss improved from 1.91602 to 1.89287, saving model to LSTM_basline.hdf5\n",
      "Epoch 126/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.8939 - acc: 0.5235\n",
      "\n",
      "Epoch 00126: loss did not improve from 1.89287\n",
      "Epoch 127/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.8781 - acc: 0.5323\n",
      "\n",
      "Epoch 00127: loss improved from 1.89287 to 1.87809, saving model to LSTM_basline.hdf5\n",
      "Epoch 128/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.8575 - acc: 0.5340\n",
      "\n",
      "Epoch 00128: loss improved from 1.87809 to 1.85747, saving model to LSTM_basline.hdf5\n",
      "Epoch 129/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.8497 - acc: 0.5377\n",
      "\n",
      "Epoch 00129: loss improved from 1.85747 to 1.84974, saving model to LSTM_basline.hdf5\n",
      "Epoch 130/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.8816 - acc: 0.5242\n",
      "\n",
      "Epoch 00130: loss did not improve from 1.84974\n",
      "Epoch 131/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.8366 - acc: 0.5385\n",
      "\n",
      "Epoch 00131: loss improved from 1.84974 to 1.83664, saving model to LSTM_basline.hdf5\n",
      "Epoch 132/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.8491 - acc: 0.5344\n",
      "\n",
      "Epoch 00132: loss did not improve from 1.83664\n",
      "Epoch 133/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.8401 - acc: 0.5378\n",
      "\n",
      "Epoch 00133: loss did not improve from 1.83664\n",
      "Epoch 134/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.8288 - acc: 0.5417\n",
      "\n",
      "Epoch 00134: loss improved from 1.83664 to 1.82882, saving model to LSTM_basline.hdf5\n",
      "Epoch 135/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.8244 - acc: 0.5392\n",
      "\n",
      "Epoch 00135: loss improved from 1.82882 to 1.82442, saving model to LSTM_basline.hdf5\n",
      "Epoch 136/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.8037 - acc: 0.5513\n",
      "\n",
      "Epoch 00136: loss improved from 1.82442 to 1.80373, saving model to LSTM_basline.hdf5\n",
      "Epoch 137/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.7796 - acc: 0.5575\n",
      "\n",
      "Epoch 00137: loss improved from 1.80373 to 1.77963, saving model to LSTM_basline.hdf5\n",
      "Epoch 138/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.7581 - acc: 0.5617\n",
      "\n",
      "Epoch 00138: loss improved from 1.77963 to 1.75811, saving model to LSTM_basline.hdf5\n",
      "Epoch 139/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.7585 - acc: 0.5588\n",
      "\n",
      "Epoch 00139: loss did not improve from 1.75811\n",
      "Epoch 140/500\n",
      "12004/12004 [==============================] - 15s 1ms/step - loss: 1.8255 - acc: 0.5381\n",
      "\n",
      "Epoch 00140: loss did not improve from 1.75811\n",
      "Epoch 141/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 1.7580 - acc: 0.5584\n",
      "\n",
      "Epoch 00141: loss improved from 1.75811 to 1.75800, saving model to LSTM_basline.hdf5\n",
      "Epoch 142/500\n",
      "12004/12004 [==============================] - 15s 1ms/step - loss: 1.7543 - acc: 0.5617\n",
      "\n",
      "Epoch 00142: loss improved from 1.75800 to 1.75430, saving model to LSTM_basline.hdf5\n",
      "Epoch 143/500\n",
      "12004/12004 [==============================] - 15s 1ms/step - loss: 1.7451 - acc: 0.5626\n",
      "\n",
      "Epoch 00143: loss improved from 1.75430 to 1.74511, saving model to LSTM_basline.hdf5\n",
      "Epoch 144/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.7682 - acc: 0.5536\n",
      "\n",
      "Epoch 00144: loss did not improve from 1.74511\n",
      "Epoch 145/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.7500 - acc: 0.5581\n",
      "\n",
      "Epoch 00145: loss did not improve from 1.74511\n",
      "Epoch 146/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.7661 - acc: 0.5516\n",
      "\n",
      "Epoch 00146: loss did not improve from 1.74511\n",
      "Epoch 147/500\n",
      "12004/12004 [==============================] - 15s 1ms/step - loss: 1.8425 - acc: 0.5294\n",
      "\n",
      "Epoch 00147: loss did not improve from 1.74511\n",
      "Epoch 148/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.7604 - acc: 0.5546\n",
      "\n",
      "Epoch 00148: loss did not improve from 1.74511\n",
      "Epoch 149/500\n",
      "12004/12004 [==============================] - 15s 1ms/step - loss: 1.7190 - acc: 0.5646\n",
      "\n",
      "Epoch 00149: loss improved from 1.74511 to 1.71899, saving model to LSTM_basline.hdf5\n",
      "Epoch 150/500\n",
      "12004/12004 [==============================] - 15s 1ms/step - loss: 1.6664 - acc: 0.5817\n",
      "\n",
      "Epoch 00150: loss improved from 1.71899 to 1.66637, saving model to LSTM_basline.hdf5\n",
      "Epoch 151/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.6641 - acc: 0.5829\n",
      "\n",
      "Epoch 00151: loss improved from 1.66637 to 1.66413, saving model to LSTM_basline.hdf5\n",
      "Epoch 152/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.6688 - acc: 0.5806\n",
      "\n",
      "Epoch 00152: loss did not improve from 1.66413\n",
      "Epoch 153/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.6488 - acc: 0.5861\n",
      "\n",
      "Epoch 00153: loss improved from 1.66413 to 1.64877, saving model to LSTM_basline.hdf5\n",
      "Epoch 154/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.6486 - acc: 0.5871\n",
      "\n",
      "Epoch 00154: loss improved from 1.64877 to 1.64858, saving model to LSTM_basline.hdf5\n",
      "Epoch 155/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.6537 - acc: 0.5821\n",
      "\n",
      "Epoch 00155: loss did not improve from 1.64858\n",
      "Epoch 156/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.6134 - acc: 0.5991\n",
      "\n",
      "Epoch 00156: loss improved from 1.64858 to 1.61344, saving model to LSTM_basline.hdf5\n",
      "Epoch 157/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.6091 - acc: 0.5986\n",
      "\n",
      "Epoch 00157: loss improved from 1.61344 to 1.60914, saving model to LSTM_basline.hdf5\n",
      "Epoch 158/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.6423 - acc: 0.5836\n",
      "\n",
      "Epoch 00158: loss did not improve from 1.60914\n",
      "Epoch 159/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.6151 - acc: 0.5930\n",
      "\n",
      "Epoch 00159: loss did not improve from 1.60914\n",
      "Epoch 160/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.6028 - acc: 0.5987\n",
      "\n",
      "Epoch 00160: loss improved from 1.60914 to 1.60277, saving model to LSTM_basline.hdf5\n",
      "Epoch 161/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.5789 - acc: 0.6025\n",
      "\n",
      "Epoch 00161: loss improved from 1.60277 to 1.57888, saving model to LSTM_basline.hdf5\n",
      "Epoch 162/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.5560 - acc: 0.6107\n",
      "\n",
      "Epoch 00162: loss improved from 1.57888 to 1.55596, saving model to LSTM_basline.hdf5\n",
      "Epoch 163/500\n",
      "12004/12004 [==============================] - 15s 1ms/step - loss: 1.5731 - acc: 0.6002\n",
      "\n",
      "Epoch 00163: loss did not improve from 1.55596\n",
      "Epoch 164/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.5642 - acc: 0.6043\n",
      "\n",
      "Epoch 00164: loss did not improve from 1.55596\n",
      "Epoch 165/500\n",
      "12004/12004 [==============================] - 15s 1ms/step - loss: 1.5755 - acc: 0.6039\n",
      "\n",
      "Epoch 00165: loss did not improve from 1.55596\n",
      "Epoch 166/500\n",
      "12004/12004 [==============================] - 15s 1ms/step - loss: 2.3681 - acc: 0.4102\n",
      "\n",
      "Epoch 00166: loss did not improve from 1.55596\n",
      "Epoch 167/500\n",
      "12004/12004 [==============================] - 15s 1ms/step - loss: 1.9354 - acc: 0.4973\n",
      "\n",
      "Epoch 00167: loss did not improve from 1.55596\n",
      "Epoch 168/500\n",
      "12004/12004 [==============================] - 15s 1ms/step - loss: 1.7945 - acc: 0.5327\n",
      "\n",
      "Epoch 00168: loss did not improve from 1.55596\n",
      "Epoch 169/500\n",
      "12004/12004 [==============================] - 15s 1ms/step - loss: 1.6837 - acc: 0.5678\n",
      "\n",
      "Epoch 00169: loss did not improve from 1.55596\n",
      "Epoch 170/500\n",
      "12004/12004 [==============================] - 15s 1ms/step - loss: 1.6470 - acc: 0.5806\n",
      "\n",
      "Epoch 00170: loss did not improve from 1.55596\n",
      "Epoch 171/500\n",
      "12004/12004 [==============================] - 15s 1ms/step - loss: 1.5955 - acc: 0.5946\n",
      "\n",
      "Epoch 00171: loss did not improve from 1.55596\n",
      "Epoch 172/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.5548 - acc: 0.6043\n",
      "\n",
      "Epoch 00172: loss improved from 1.55596 to 1.55482, saving model to LSTM_basline.hdf5\n",
      "Epoch 173/500\n",
      "12004/12004 [==============================] - 15s 1ms/step - loss: 1.5407 - acc: 0.6099\n",
      "\n",
      "Epoch 00173: loss improved from 1.55482 to 1.54067, saving model to LSTM_basline.hdf5\n",
      "Epoch 174/500\n",
      "12004/12004 [==============================] - 15s 1ms/step - loss: 1.5272 - acc: 0.6177\n",
      "\n",
      "Epoch 00174: loss improved from 1.54067 to 1.52719, saving model to LSTM_basline.hdf5\n",
      "Epoch 175/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.5106 - acc: 0.6207\n",
      "\n",
      "Epoch 00175: loss improved from 1.52719 to 1.51061, saving model to LSTM_basline.hdf5\n",
      "Epoch 176/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.4989 - acc: 0.6217\n",
      "\n",
      "Epoch 00176: loss improved from 1.51061 to 1.49887, saving model to LSTM_basline.hdf5\n",
      "Epoch 177/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.4617 - acc: 0.6356\n",
      "\n",
      "Epoch 00177: loss improved from 1.49887 to 1.46171, saving model to LSTM_basline.hdf5\n",
      "Epoch 178/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.4506 - acc: 0.6337\n",
      "\n",
      "Epoch 00178: loss improved from 1.46171 to 1.45061, saving model to LSTM_basline.hdf5\n",
      "Epoch 179/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.4601 - acc: 0.6330\n",
      "\n",
      "Epoch 00179: loss did not improve from 1.45061\n",
      "Epoch 180/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.4749 - acc: 0.6300\n",
      "\n",
      "Epoch 00180: loss did not improve from 1.45061\n",
      "Epoch 181/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.4445 - acc: 0.6330\n",
      "\n",
      "Epoch 00181: loss improved from 1.45061 to 1.44447, saving model to LSTM_basline.hdf5\n",
      "Epoch 182/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.4579 - acc: 0.6354\n",
      "\n",
      "Epoch 00182: loss did not improve from 1.44447\n",
      "Epoch 183/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.4270 - acc: 0.6413\n",
      "\n",
      "Epoch 00183: loss improved from 1.44447 to 1.42702, saving model to LSTM_basline.hdf5\n",
      "Epoch 184/500\n",
      "12004/12004 [==============================] - 15s 1ms/step - loss: 1.4182 - acc: 0.6431\n",
      "\n",
      "Epoch 00184: loss improved from 1.42702 to 1.41817, saving model to LSTM_basline.hdf5\n",
      "Epoch 185/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.4020 - acc: 0.6485\n",
      "\n",
      "Epoch 00185: loss improved from 1.41817 to 1.40204, saving model to LSTM_basline.hdf5\n",
      "Epoch 186/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.4477 - acc: 0.6321\n",
      "\n",
      "Epoch 00186: loss did not improve from 1.40204\n",
      "Epoch 187/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.4043 - acc: 0.6480\n",
      "\n",
      "Epoch 00187: loss did not improve from 1.40204\n",
      "Epoch 188/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.3776 - acc: 0.6558\n",
      "\n",
      "Epoch 00188: loss improved from 1.40204 to 1.37761, saving model to LSTM_basline.hdf5\n",
      "Epoch 189/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.3983 - acc: 0.6526\n",
      "\n",
      "Epoch 00189: loss did not improve from 1.37761\n",
      "Epoch 190/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.4129 - acc: 0.6432\n",
      "\n",
      "Epoch 00190: loss did not improve from 1.37761\n",
      "Epoch 191/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.3716 - acc: 0.6526\n",
      "\n",
      "Epoch 00191: loss improved from 1.37761 to 1.37159, saving model to LSTM_basline.hdf5\n",
      "Epoch 192/500\n",
      "12004/12004 [==============================] - 92s 8ms/step - loss: 1.3663 - acc: 0.6541\n",
      "\n",
      "Epoch 00192: loss improved from 1.37159 to 1.36629, saving model to LSTM_basline.hdf5\n",
      "Epoch 193/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 1.3432 - acc: 0.6625\n",
      "\n",
      "Epoch 00193: loss improved from 1.36629 to 1.34323, saving model to LSTM_basline.hdf5\n",
      "Epoch 194/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.3512 - acc: 0.6555\n",
      "\n",
      "Epoch 00194: loss did not improve from 1.34323\n",
      "Epoch 195/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.3333 - acc: 0.6689\n",
      "\n",
      "Epoch 00195: loss improved from 1.34323 to 1.33325, saving model to LSTM_basline.hdf5\n",
      "Epoch 196/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.3366 - acc: 0.6636\n",
      "\n",
      "Epoch 00196: loss did not improve from 1.33325\n",
      "Epoch 197/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.3284 - acc: 0.6689\n",
      "\n",
      "Epoch 00197: loss improved from 1.33325 to 1.32840, saving model to LSTM_basline.hdf5\n",
      "Epoch 198/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.3174 - acc: 0.6709\n",
      "\n",
      "Epoch 00198: loss improved from 1.32840 to 1.31741, saving model to LSTM_basline.hdf5\n",
      "Epoch 199/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.3216 - acc: 0.6679\n",
      "\n",
      "Epoch 00199: loss did not improve from 1.31741\n",
      "Epoch 200/500\n",
      "12004/12004 [==============================] - 15s 1ms/step - loss: 1.2829 - acc: 0.6782\n",
      "\n",
      "Epoch 00200: loss improved from 1.31741 to 1.28293, saving model to LSTM_basline.hdf5\n",
      "Epoch 201/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.3151 - acc: 0.6644\n",
      "\n",
      "Epoch 00201: loss did not improve from 1.28293\n",
      "Epoch 202/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.3230 - acc: 0.6637\n",
      "\n",
      "Epoch 00202: loss did not improve from 1.28293\n",
      "Epoch 203/500\n",
      "12004/12004 [==============================] - 472s 39ms/step - loss: 1.3068 - acc: 0.6703\n",
      "\n",
      "Epoch 00203: loss did not improve from 1.28293\n",
      "Epoch 204/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 1.2923 - acc: 0.6715\n",
      "\n",
      "Epoch 00204: loss did not improve from 1.28293\n",
      "Epoch 205/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 1.2642 - acc: 0.6860\n",
      "\n",
      "Epoch 00205: loss improved from 1.28293 to 1.26424, saving model to LSTM_basline.hdf5\n",
      "Epoch 206/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.2733 - acc: 0.6797\n",
      "\n",
      "Epoch 00206: loss did not improve from 1.26424\n",
      "Epoch 207/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.2532 - acc: 0.6829\n",
      "\n",
      "Epoch 00207: loss improved from 1.26424 to 1.25320, saving model to LSTM_basline.hdf5\n",
      "Epoch 208/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.2485 - acc: 0.6886\n",
      "\n",
      "Epoch 00208: loss improved from 1.25320 to 1.24854, saving model to LSTM_basline.hdf5\n",
      "Epoch 209/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.2588 - acc: 0.6815\n",
      "\n",
      "Epoch 00209: loss did not improve from 1.24854\n",
      "Epoch 210/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.2064 - acc: 0.7006\n",
      "\n",
      "Epoch 00210: loss improved from 1.24854 to 1.20635, saving model to LSTM_basline.hdf5\n",
      "Epoch 211/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.1955 - acc: 0.7038\n",
      "\n",
      "Epoch 00211: loss improved from 1.20635 to 1.19546, saving model to LSTM_basline.hdf5\n",
      "Epoch 212/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.2106 - acc: 0.6955\n",
      "\n",
      "Epoch 00212: loss did not improve from 1.19546\n",
      "Epoch 213/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.2247 - acc: 0.6934\n",
      "\n",
      "Epoch 00213: loss did not improve from 1.19546\n",
      "Epoch 214/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 1.1773 - acc: 0.7048\n",
      "\n",
      "Epoch 00214: loss improved from 1.19546 to 1.17727, saving model to LSTM_basline.hdf5\n",
      "Epoch 215/500\n",
      "12004/12004 [==============================] - 576s 48ms/step - loss: 1.1887 - acc: 0.6991\n",
      "\n",
      "Epoch 00215: loss did not improve from 1.17727\n",
      "Epoch 216/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 1.1623 - acc: 0.7064\n",
      "\n",
      "Epoch 00216: loss improved from 1.17727 to 1.16230, saving model to LSTM_basline.hdf5\n",
      "Epoch 217/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 1.2409 - acc: 0.6808\n",
      "\n",
      "Epoch 00217: loss did not improve from 1.16230\n",
      "Epoch 218/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.1935 - acc: 0.7007\n",
      "\n",
      "Epoch 00218: loss did not improve from 1.16230\n",
      "Epoch 219/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.1541 - acc: 0.7151\n",
      "\n",
      "Epoch 00219: loss improved from 1.16230 to 1.15409, saving model to LSTM_basline.hdf5\n",
      "Epoch 220/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.1549 - acc: 0.7113\n",
      "\n",
      "Epoch 00220: loss did not improve from 1.15409\n",
      "Epoch 221/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.1689 - acc: 0.7076\n",
      "\n",
      "Epoch 00221: loss did not improve from 1.15409\n",
      "Epoch 222/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.1235 - acc: 0.7222\n",
      "\n",
      "Epoch 00222: loss improved from 1.15409 to 1.12350, saving model to LSTM_basline.hdf5\n",
      "Epoch 223/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.1501 - acc: 0.7129\n",
      "\n",
      "Epoch 00223: loss did not improve from 1.12350\n",
      "Epoch 224/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.1314 - acc: 0.7187\n",
      "\n",
      "Epoch 00224: loss did not improve from 1.12350\n",
      "Epoch 225/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.1155 - acc: 0.7222\n",
      "\n",
      "Epoch 00225: loss improved from 1.12350 to 1.11545, saving model to LSTM_basline.hdf5\n",
      "Epoch 226/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.0936 - acc: 0.7291\n",
      "\n",
      "Epoch 00226: loss improved from 1.11545 to 1.09365, saving model to LSTM_basline.hdf5\n",
      "Epoch 227/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.0802 - acc: 0.7310\n",
      "\n",
      "Epoch 00227: loss improved from 1.09365 to 1.08017, saving model to LSTM_basline.hdf5\n",
      "Epoch 228/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.0787 - acc: 0.7323\n",
      "\n",
      "Epoch 00228: loss improved from 1.08017 to 1.07873, saving model to LSTM_basline.hdf5\n",
      "Epoch 229/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.2759 - acc: 0.6738\n",
      "\n",
      "Epoch 00229: loss did not improve from 1.07873\n",
      "Epoch 230/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.1748 - acc: 0.7002\n",
      "\n",
      "Epoch 00230: loss did not improve from 1.07873\n",
      "Epoch 231/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.1318 - acc: 0.7154\n",
      "\n",
      "Epoch 00231: loss did not improve from 1.07873\n",
      "Epoch 232/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.0900 - acc: 0.7250\n",
      "\n",
      "Epoch 00232: loss did not improve from 1.07873\n",
      "Epoch 233/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.0472 - acc: 0.7395\n",
      "\n",
      "Epoch 00233: loss improved from 1.07873 to 1.04716, saving model to LSTM_basline.hdf5\n",
      "Epoch 234/500\n",
      "12004/12004 [==============================] - 901s 75ms/step - loss: 1.0285 - acc: 0.7453\n",
      "\n",
      "Epoch 00234: loss improved from 1.04716 to 1.02845, saving model to LSTM_basline.hdf5\n",
      "Epoch 235/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 1.0008 - acc: 0.7571\n",
      "\n",
      "Epoch 00235: loss improved from 1.02845 to 1.00083, saving model to LSTM_basline.hdf5\n",
      "Epoch 236/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.0090 - acc: 0.7520\n",
      "\n",
      "Epoch 00236: loss did not improve from 1.00083\n",
      "Epoch 237/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.9938 - acc: 0.7585\n",
      "\n",
      "Epoch 00237: loss improved from 1.00083 to 0.99383, saving model to LSTM_basline.hdf5\n",
      "Epoch 238/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.0087 - acc: 0.7501\n",
      "\n",
      "Epoch 00238: loss did not improve from 0.99383\n",
      "Epoch 239/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.9798 - acc: 0.7617\n",
      "\n",
      "Epoch 00239: loss improved from 0.99383 to 0.97977, saving model to LSTM_basline.hdf5\n",
      "Epoch 240/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.0141 - acc: 0.7485\n",
      "\n",
      "Epoch 00240: loss did not improve from 0.97977\n",
      "Epoch 241/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.9675 - acc: 0.7655\n",
      "\n",
      "Epoch 00241: loss improved from 0.97977 to 0.96751, saving model to LSTM_basline.hdf5\n",
      "Epoch 242/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.9836 - acc: 0.7580\n",
      "\n",
      "Epoch 00242: loss did not improve from 0.96751\n",
      "Epoch 243/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.0117 - acc: 0.7486\n",
      "\n",
      "Epoch 00243: loss did not improve from 0.96751\n",
      "Epoch 244/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.0034 - acc: 0.7480\n",
      "\n",
      "Epoch 00244: loss did not improve from 0.96751\n",
      "Epoch 245/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.9560 - acc: 0.7666\n",
      "\n",
      "Epoch 00245: loss improved from 0.96751 to 0.95598, saving model to LSTM_basline.hdf5\n",
      "Epoch 246/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.9641 - acc: 0.7636\n",
      "\n",
      "Epoch 00246: loss did not improve from 0.95598\n",
      "Epoch 247/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.9518 - acc: 0.7667\n",
      "\n",
      "Epoch 00247: loss improved from 0.95598 to 0.95181, saving model to LSTM_basline.hdf5\n",
      "Epoch 248/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.9533 - acc: 0.7611\n",
      "\n",
      "Epoch 00248: loss did not improve from 0.95181\n",
      "Epoch 249/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 0.9940 - acc: 0.7506\n",
      "\n",
      "Epoch 00249: loss did not improve from 0.95181\n",
      "Epoch 250/500\n",
      "12004/12004 [==============================] - 1142s 95ms/step - loss: 0.9561 - acc: 0.7571\n",
      "\n",
      "Epoch 00250: loss did not improve from 0.95181\n",
      "Epoch 251/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 0.9144 - acc: 0.7766\n",
      "\n",
      "Epoch 00251: loss improved from 0.95181 to 0.91444, saving model to LSTM_basline.hdf5\n",
      "Epoch 252/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 0.9428 - acc: 0.7623\n",
      "\n",
      "Epoch 00252: loss did not improve from 0.91444\n",
      "Epoch 253/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 0.9941 - acc: 0.7477\n",
      "\n",
      "Epoch 00253: loss did not improve from 0.91444\n",
      "Epoch 254/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.0055 - acc: 0.7421\n",
      "\n",
      "Epoch 00254: loss did not improve from 0.91444\n",
      "Epoch 255/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.9306 - acc: 0.7688\n",
      "\n",
      "Epoch 00255: loss did not improve from 0.91444\n",
      "Epoch 256/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.9113 - acc: 0.7737\n",
      "\n",
      "Epoch 00256: loss improved from 0.91444 to 0.91131, saving model to LSTM_basline.hdf5\n",
      "Epoch 257/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.8963 - acc: 0.7787\n",
      "\n",
      "Epoch 00257: loss improved from 0.91131 to 0.89632, saving model to LSTM_basline.hdf5\n",
      "Epoch 258/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.8598 - acc: 0.7894\n",
      "\n",
      "Epoch 00258: loss improved from 0.89632 to 0.85981, saving model to LSTM_basline.hdf5\n",
      "Epoch 259/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.8556 - acc: 0.7947\n",
      "\n",
      "Epoch 00259: loss improved from 0.85981 to 0.85555, saving model to LSTM_basline.hdf5\n",
      "Epoch 260/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.8867 - acc: 0.7779\n",
      "\n",
      "Epoch 00260: loss did not improve from 0.85555\n",
      "Epoch 261/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.8540 - acc: 0.7904\n",
      "\n",
      "Epoch 00261: loss improved from 0.85555 to 0.85399, saving model to LSTM_basline.hdf5\n",
      "Epoch 262/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.8491 - acc: 0.7932\n",
      "\n",
      "Epoch 00262: loss improved from 0.85399 to 0.84913, saving model to LSTM_basline.hdf5\n",
      "Epoch 263/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.8364 - acc: 0.7957\n",
      "\n",
      "Epoch 00263: loss improved from 0.84913 to 0.83639, saving model to LSTM_basline.hdf5\n",
      "Epoch 264/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.7953 - acc: 0.8094\n",
      "\n",
      "Epoch 00264: loss improved from 0.83639 to 0.79534, saving model to LSTM_basline.hdf5\n",
      "Epoch 265/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.8458 - acc: 0.7921\n",
      "\n",
      "Epoch 00265: loss did not improve from 0.79534\n",
      "Epoch 266/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.8920 - acc: 0.7772\n",
      "\n",
      "Epoch 00266: loss did not improve from 0.79534\n",
      "Epoch 267/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.8378 - acc: 0.7951\n",
      "\n",
      "Epoch 00267: loss did not improve from 0.79534\n",
      "Epoch 268/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.8317 - acc: 0.7946\n",
      "\n",
      "Epoch 00268: loss did not improve from 0.79534\n",
      "Epoch 269/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.8377 - acc: 0.7910\n",
      "\n",
      "Epoch 00269: loss did not improve from 0.79534\n",
      "Epoch 270/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.8029 - acc: 0.8054\n",
      "\n",
      "Epoch 00270: loss did not improve from 0.79534\n",
      "Epoch 271/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.7643 - acc: 0.8161\n",
      "\n",
      "Epoch 00271: loss improved from 0.79534 to 0.76425, saving model to LSTM_basline.hdf5\n",
      "Epoch 272/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.7991 - acc: 0.8036\n",
      "\n",
      "Epoch 00272: loss did not improve from 0.76425\n",
      "Epoch 273/500\n",
      "12004/12004 [==============================] - 15s 1ms/step - loss: 0.7792 - acc: 0.8095\n",
      "\n",
      "Epoch 00273: loss did not improve from 0.76425\n",
      "Epoch 274/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 0.7835 - acc: 0.8077\n",
      "\n",
      "Epoch 00274: loss did not improve from 0.76425\n",
      "Epoch 275/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.7820 - acc: 0.8091\n",
      "\n",
      "Epoch 00275: loss did not improve from 0.76425\n",
      "Epoch 276/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.7921 - acc: 0.8064\n",
      "\n",
      "Epoch 00276: loss did not improve from 0.76425\n",
      "Epoch 277/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 0.7498 - acc: 0.8212\n",
      "\n",
      "Epoch 00277: loss improved from 0.76425 to 0.74979, saving model to LSTM_basline.hdf5\n",
      "Epoch 278/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.7488 - acc: 0.8174\n",
      "\n",
      "Epoch 00278: loss improved from 0.74979 to 0.74875, saving model to LSTM_basline.hdf5\n",
      "Epoch 279/500\n",
      "12004/12004 [==============================] - 15s 1ms/step - loss: 0.7066 - acc: 0.8356\n",
      "\n",
      "Epoch 00279: loss improved from 0.74875 to 0.70661, saving model to LSTM_basline.hdf5\n",
      "Epoch 280/500\n",
      "12004/12004 [==============================] - 15s 1ms/step - loss: 0.7235 - acc: 0.8291\n",
      "\n",
      "Epoch 00280: loss did not improve from 0.70661\n",
      "Epoch 281/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.7413 - acc: 0.8196\n",
      "\n",
      "Epoch 00281: loss did not improve from 0.70661\n",
      "Epoch 282/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.6969 - acc: 0.8343\n",
      "\n",
      "Epoch 00282: loss improved from 0.70661 to 0.69694, saving model to LSTM_basline.hdf5\n",
      "Epoch 283/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.7053 - acc: 0.8263\n",
      "\n",
      "Epoch 00283: loss did not improve from 0.69694\n",
      "Epoch 284/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.7555 - acc: 0.8148\n",
      "\n",
      "Epoch 00284: loss did not improve from 0.69694\n",
      "Epoch 285/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.7616 - acc: 0.8085\n",
      "\n",
      "Epoch 00285: loss did not improve from 0.69694\n",
      "Epoch 286/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.7461 - acc: 0.8152\n",
      "\n",
      "Epoch 00286: loss did not improve from 0.69694\n",
      "Epoch 287/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.7726 - acc: 0.8076\n",
      "\n",
      "Epoch 00287: loss did not improve from 0.69694\n",
      "Epoch 288/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.7708 - acc: 0.8017\n",
      "\n",
      "Epoch 00288: loss did not improve from 0.69694\n",
      "Epoch 289/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.7291 - acc: 0.8201\n",
      "\n",
      "Epoch 00289: loss did not improve from 0.69694\n",
      "Epoch 290/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.7545 - acc: 0.8155\n",
      "\n",
      "Epoch 00290: loss did not improve from 0.69694\n",
      "Epoch 291/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.7521 - acc: 0.8103\n",
      "\n",
      "Epoch 00291: loss did not improve from 0.69694\n",
      "Epoch 292/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.7082 - acc: 0.8263\n",
      "\n",
      "Epoch 00292: loss did not improve from 0.69694\n",
      "Epoch 293/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.6982 - acc: 0.8336\n",
      "\n",
      "Epoch 00293: loss did not improve from 0.69694\n",
      "Epoch 294/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.6420 - acc: 0.8513\n",
      "\n",
      "Epoch 00294: loss improved from 0.69694 to 0.64199, saving model to LSTM_basline.hdf5\n",
      "Epoch 295/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.6465 - acc: 0.8489\n",
      "\n",
      "Epoch 00295: loss did not improve from 0.64199\n",
      "Epoch 296/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.6363 - acc: 0.8542\n",
      "\n",
      "Epoch 00296: loss improved from 0.64199 to 0.63629, saving model to LSTM_basline.hdf5\n",
      "Epoch 297/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.6021 - acc: 0.8615\n",
      "\n",
      "Epoch 00297: loss improved from 0.63629 to 0.60208, saving model to LSTM_basline.hdf5\n",
      "Epoch 298/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.5974 - acc: 0.8627\n",
      "\n",
      "Epoch 00298: loss improved from 0.60208 to 0.59736, saving model to LSTM_basline.hdf5\n",
      "Epoch 299/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.5933 - acc: 0.8642\n",
      "\n",
      "Epoch 00299: loss improved from 0.59736 to 0.59333, saving model to LSTM_basline.hdf5\n",
      "Epoch 300/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.5828 - acc: 0.8685\n",
      "\n",
      "Epoch 00300: loss improved from 0.59333 to 0.58277, saving model to LSTM_basline.hdf5\n",
      "Epoch 301/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.5694 - acc: 0.8728\n",
      "\n",
      "Epoch 00301: loss improved from 0.58277 to 0.56935, saving model to LSTM_basline.hdf5\n",
      "Epoch 302/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.5438 - acc: 0.8819\n",
      "\n",
      "Epoch 00302: loss improved from 0.56935 to 0.54378, saving model to LSTM_basline.hdf5\n",
      "Epoch 303/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.6333 - acc: 0.8496\n",
      "\n",
      "Epoch 00303: loss did not improve from 0.54378\n",
      "Epoch 304/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.5997 - acc: 0.8610\n",
      "\n",
      "Epoch 00304: loss did not improve from 0.54378\n",
      "Epoch 305/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.5641 - acc: 0.8742\n",
      "\n",
      "Epoch 00305: loss did not improve from 0.54378\n",
      "Epoch 306/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.5643 - acc: 0.8740\n",
      "\n",
      "Epoch 00306: loss did not improve from 0.54378\n",
      "Epoch 307/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.6052 - acc: 0.8576\n",
      "\n",
      "Epoch 00307: loss did not improve from 0.54378\n",
      "Epoch 308/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.5883 - acc: 0.8619\n",
      "\n",
      "Epoch 00308: loss did not improve from 0.54378\n",
      "Epoch 309/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.5857 - acc: 0.8627\n",
      "\n",
      "Epoch 00309: loss did not improve from 0.54378\n",
      "Epoch 310/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.5575 - acc: 0.8753\n",
      "\n",
      "Epoch 00310: loss did not improve from 0.54378\n",
      "Epoch 311/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.5644 - acc: 0.8721\n",
      "\n",
      "Epoch 00311: loss did not improve from 0.54378\n",
      "Epoch 312/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.5679 - acc: 0.8697\n",
      "\n",
      "Epoch 00312: loss did not improve from 0.54378\n",
      "Epoch 313/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.5709 - acc: 0.8642\n",
      "\n",
      "Epoch 00313: loss did not improve from 0.54378\n",
      "Epoch 314/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.5753 - acc: 0.8635\n",
      "\n",
      "Epoch 00314: loss did not improve from 0.54378\n",
      "Epoch 315/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.5390 - acc: 0.8758\n",
      "\n",
      "Epoch 00315: loss improved from 0.54378 to 0.53905, saving model to LSTM_basline.hdf5\n",
      "Epoch 316/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.6008 - acc: 0.8540\n",
      "\n",
      "Epoch 00316: loss did not improve from 0.53905\n",
      "Epoch 317/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.6118 - acc: 0.8444\n",
      "\n",
      "Epoch 00317: loss did not improve from 0.53905\n",
      "Epoch 318/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.5450 - acc: 0.8720\n",
      "\n",
      "Epoch 00318: loss did not improve from 0.53905\n",
      "Epoch 319/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.4866 - acc: 0.8945\n",
      "\n",
      "Epoch 00319: loss improved from 0.53905 to 0.48657, saving model to LSTM_basline.hdf5\n",
      "Epoch 320/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.4617 - acc: 0.9018\n",
      "\n",
      "Epoch 00320: loss improved from 0.48657 to 0.46167, saving model to LSTM_basline.hdf5\n",
      "Epoch 321/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.4765 - acc: 0.8960\n",
      "\n",
      "Epoch 00321: loss did not improve from 0.46167\n",
      "Epoch 322/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.4727 - acc: 0.8981\n",
      "\n",
      "Epoch 00322: loss did not improve from 0.46167\n",
      "Epoch 323/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.4873 - acc: 0.8918\n",
      "\n",
      "Epoch 00323: loss did not improve from 0.46167\n",
      "Epoch 324/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.4624 - acc: 0.9014\n",
      "\n",
      "Epoch 00324: loss did not improve from 0.46167\n",
      "Epoch 325/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.4335 - acc: 0.9106\n",
      "\n",
      "Epoch 00325: loss improved from 0.46167 to 0.43351, saving model to LSTM_basline.hdf5\n",
      "Epoch 326/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.4435 - acc: 0.9067\n",
      "\n",
      "Epoch 00326: loss did not improve from 0.43351\n",
      "Epoch 327/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.4454 - acc: 0.9047\n",
      "\n",
      "Epoch 00327: loss did not improve from 0.43351\n",
      "Epoch 328/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.4372 - acc: 0.9061\n",
      "\n",
      "Epoch 00328: loss did not improve from 0.43351\n",
      "Epoch 329/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.4505 - acc: 0.9010\n",
      "\n",
      "Epoch 00329: loss did not improve from 0.43351\n",
      "Epoch 330/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.4957 - acc: 0.8838\n",
      "\n",
      "Epoch 00330: loss did not improve from 0.43351\n",
      "Epoch 331/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.4646 - acc: 0.8930\n",
      "\n",
      "Epoch 00331: loss did not improve from 0.43351\n",
      "Epoch 332/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.4296 - acc: 0.9078\n",
      "\n",
      "Epoch 00332: loss improved from 0.43351 to 0.42963, saving model to LSTM_basline.hdf5\n",
      "Epoch 333/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.4911 - acc: 0.8829\n",
      "\n",
      "Epoch 00333: loss did not improve from 0.42963\n",
      "Epoch 334/500\n",
      "12004/12004 [==============================] - 2053s 171ms/step - loss: 0.4064 - acc: 0.9140\n",
      "\n",
      "Epoch 00334: loss improved from 0.42963 to 0.40635, saving model to LSTM_basline.hdf5\n",
      "Epoch 335/500\n",
      "12004/12004 [==============================] - 15s 1ms/step - loss: 0.4972 - acc: 0.8815\n",
      "\n",
      "Epoch 00335: loss did not improve from 0.40635\n",
      "Epoch 336/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11800/12004 [============================>.] - ETA: 0s - loss: 0.5160 - acc: 0.8738"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-85-cdd54980823c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mresult_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/nlps/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/envs/nlps/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlps/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlps/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlps/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1397\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1398\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1399\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1400\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.load_weights(filepath)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "result_history = model.fit(X, y, batch_size=200, epochs= 500, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# result_history.history[\"loss\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('model.h6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial text:\n",
      "competition and what we should expect on the deposit beta if there are more rate hikes and so maybe bluntly\n",
      "\n",
      "Generated text:\n",
      "if we get a couple more hate rate hikes do you capture the majority of it even without as you said a rate story do you have to hike along\n"
     ]
    }
   ],
   "source": [
    "#test 1\n",
    "# load cleaned text sequences\n",
    "in_filename = 'Glenn_Schorr_sequences.txt'\n",
    "doc = load_doc(in_filename)\n",
    "lines = doc.split('\\n')\n",
    "seq_length = len(lines[0].split()) - 1\n",
    " \n",
    "# load the model\n",
    "# model = load_model('model.h6')\n",
    "\n",
    "model.load_weights(filepath)\n",
    " \n",
    "# load the tokenizer\n",
    "tokenizer = plk.load(open('tokenizer.pkl', 'rb'))\n",
    " \n",
    "# select a seed text\n",
    "seed_text = lines[randint(0,len(lines))]\n",
    "print('Initial text:')\n",
    "print(seed_text + '\\n')\n",
    " \n",
    "# generate new text\n",
    "print('Generated text:')\n",
    "generated = generate_seq(model, tokenizer, seq_length, seed_text, 30)\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial text:\n",
      "and are you still on track in your mind for the overhead ratio goals i do want to overly focus\n",
      "\n",
      "Generated text:\n",
      "on the dollar amount okay so i wanted to ask an nii question and i saw the comments on ex global markets the net interest yield being up basis points deposit betas are good so the core business that we all focus on is good i do want to talk\n"
     ]
    }
   ],
   "source": [
    "#test 2\n",
    "\n",
    "# load the tokenizer\n",
    "tokenizer = plk.load(open('tokenizer.pkl', 'rb'))\n",
    " \n",
    "# select a seed text\n",
    "seed_text = lines[randint(0,len(lines))]\n",
    "print('Initial text:')\n",
    "print(seed_text + '\\n')\n",
    " \n",
    "# generate new text\n",
    "print('Generated text:')\n",
    "generated = generate_seq(model, tokenizer, seq_length, seed_text, 50)\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further improvement \n",
    "\n",
    "### 1. Improve the sequence build up based on each question, or expand training data to all questions from same category\n",
    "### 2. Tune parameter to see if the performace is better in generating questions (batch size, epoch time etc.)\n",
    "### 3. Research on the question generation process including tones and use of words\n",
    "### 4. Improve the dropout (0.2, 0.4 0.6 etc.) to LSTM input layer to prevent overfitting for the model and may generate a more natural language\n",
    "\n",
    "Dropout is a technique where randomly selected neurons are ignored during training. They are “dropped-out” randomly. This means that their contribution to the activation of downstream neurons is temporally removed on the forward pass and any weight updates are not applied to the neuron on the backward pass.\n",
    "paper about dropout\n",
    "http://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
