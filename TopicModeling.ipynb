{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pickle as pkl\n",
    "import re\n",
    "\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import nimfa \n",
    "\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "\n",
    "data_directory = '/'.join(os.getcwd().split(\"/\")[:-2]) + '/data/'\n",
    "\n",
    "import sys\n",
    "sys.path.insert(3, '/'.join(os.getcwd().split(\"/\")[:-2]) + '/textacy')\n",
    "\n",
    "import textacy\n",
    "from textacy import preprocess_text, Doc, Corpus\n",
    "from textacy.vsm import Vectorizer, GroupVectorizer\n",
    "from textacy.tm import TopicModel\n",
    "en = textacy.load_spacy(\"en_core_web_sm\", disable='parser')\n",
    "\n",
    "\n",
    "test_set = [173,  74,  20, 101,  83,   1,  38,  39,  72,  50,  21, 164,  57,\n",
    "       169, 8,  63, 102,  34,  80, 192, 139,  88, 112, 116,  61,  46,\n",
    "        51, 165, 135,  89, 108,   7,  25,  15, 125,  93, 130,  71]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "orig_data = pd.read_csv(data_directory + 'qaData.csv', parse_dates=['Date'])\n",
    "orig_data['Year'] = orig_data['Date'].dt.year\n",
    "orig_data['Month'] = orig_data['Date'].dt.month\n",
    "orig_data['Quarter'] = orig_data['Month'].apply(lambda x: 1 if x < 4 else 2 if x < 7 else 3 if x < 9 else 4)\n",
    "orig_data['Company'] = orig_data['Company'].str.title().str.replace(\" \", \"\")\n",
    "orig_data['AnalystName'] = orig_data['AnalystName'].str.title().str.replace(\" \", \"\")\n",
    "orig_data['Tag'] = orig_data['EarningTag2'].str.title().str.replace(\" \", \"\")\n",
    "\n",
    "orig_data = orig_data.loc[~orig_data['AnalystName'].isna()].copy()\n",
    "\n",
    "groups = []\n",
    "for i, (name, group) in enumerate(orig_data.groupby(['Company', 'Participants', 'Month', 'Year', 'Quarter', 'EventType', 'Date'])):\n",
    "    g2 = group.copy()\n",
    "    g2['EventNumber'] = i\n",
    "    g2.reset_index(drop=True, inplace=True)\n",
    "    g2.index.name = \"QuestionNumber\"\n",
    "    g2.reset_index(inplace=True)\n",
    "    groups.append(g2)\n",
    "    \n",
    "indexed_data = pd.concat(groups)\n",
    "#train_data = indexed_data.loc[~indexed_data['EventNumber'].isin(test_set)]\n",
    "q_data = indexed_data[['Date', 'EventNumber', 'QuestionNumber', 'Year', 'Quarter', 'Company', 'AnalystName', 'EventType', 'Tag', 'Question']].copy()\n",
    "\n",
    "\n",
    "docs = Corpus(lang=en, docs=q_data.apply(lambda x: Doc(content=' '.join(\n",
    "                                                        [token for token in preprocess_text(text=x['Question'], lowercase=True, no_punct=True, no_contractions=True, no_accents=True, no_currency_symbols=True, no_numbers=True).split(' ') if len(token)>2]),\n",
    "                                                    lang=en, metadata={'Year':x['Year'],\n",
    "                                                                       'Quarter':x['Quarter'],\n",
    "                                                                       'Company':x['Company'],\n",
    "                                                                       'AnalystName':x[\"AnalystName\"],\n",
    "                                                                       'Tag':x['Tag'],\n",
    "                                                                       'EventType':x['EventType'],\n",
    "                                                                       'EventNumber':x['EventNumber'],\n",
    "                                                                       'QuestionNumber':x['QuestionNumber']}),axis=1).tolist())\n",
    "\n",
    "tokenized_docs = [[list(doc.to_terms_list(ngrams=(1), as_strings=True, normalize='lemma', drop_determiners=True)), doc.metadata] for doc in docs if doc.metadata['EventNumber'] not in test_set]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_phraser = Phraser(Phrases([doc[0] for doc in tokenized_docs], min_count=10, threshold=20, delimiter=b' '))\n",
    "bigram_docs = [bigram_phraser[doc[0]] for doc in tokenized_docs] \n",
    "\n",
    "trigram_phraser = Phraser(Phrases(bigram_docs, min_count=5, threshold=10, delimiter=b' '))\n",
    "trigram_docs = [trigram_phraser[doc] for doc in bigram_docs] \n",
    "\n",
    "analysts = [d[1]['AnalystName'] for d in tokenized_docs]\n",
    "tags = [d[1]['Tag'] for d in tokenized_docs]\n",
    "companies = [d[1]['Company'] for d in tokenized_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_rank = 10\n",
    "\n",
    "analyst_vec = GroupVectorizer(tf_type='bm25', apply_idf=True, idf_type='smooth', apply_dl=True, dl_type='linear').fit(trigram_docs, analysts)\n",
    "train_doc_term_matrix = analyst_vec.transform(trigram_docs, analysts)\n",
    "\n",
    "mod = nimfa.Nmf(V=train_doc_term_matrix, max_iter=200, rank=a_rank)\n",
    "mod_fit = mod()\n",
    "\n",
    "analyst_df = pd.SparseDataFrame(normalize(mod_fit.basis()), columns = ['aTopic'+str(i) for i in range(a_rank)], index=analyst_vec.grps_list).fillna(0)**2\n",
    "analyst_df.index.name = 'AnalystName'\n",
    "analyst_df.join(analyst_df.idxmax(axis=1).rename('aTopicMax')).reset_index().to_csv(data_directory+\"analystTopic.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_rank = 2\n",
    "\n",
    "tag_vec = GroupVectorizer(tf_type='bm25', apply_idf=True, idf_type='smooth', apply_dl=True, dl_type='linear').fit(trigram_docs, tags)\n",
    "train_doc_term_matrix = tag_vec.transform(trigram_docs, tags)\n",
    "\n",
    "mod = nimfa.Nmf(V=train_doc_term_matrix, max_iter=200, rank=t_rank)\n",
    "mod_fit = mod()\n",
    "\n",
    "tag_df = pd.SparseDataFrame(normalize(mod_fit.basis()), columns = ['tTopic'+str(i) for i in range(t_rank)], index=tag_vec.grps_list).fillna(0)**2\n",
    "tag_df.index.name = 'Tag'\n",
    "tag_df.join(tag_df.idxmax(axis=1).rename('tTopicMax')).reset_index().to_csv(data_directory+\"tagTopic.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_rank = 2\n",
    "\n",
    "company_vec = GroupVectorizer(tf_type='bm25', apply_idf=True, idf_type='smooth', apply_dl=True, dl_type='linear').fit(trigram_docs, companies)\n",
    "train_doc_term_matrix = company_vec.transform(trigram_docs, companies)\n",
    "\n",
    "mod = nimfa.Nmf(V=train_doc_term_matrix, max_iter=200, rank=c_rank)\n",
    "mod_fit = mod()\n",
    "\n",
    "company_df = pd.SparseDataFrame(normalize(mod_fit.basis()), columns = ['cTopic'+str(i) for i in range(c_rank)], index=company_vec.grps_list).fillna(0)**2\n",
    "company_df.index.name = 'Company'\n",
    "company_df.join(tag_df.idxmax(axis=1).rename('cTopicMax')).reset_index().to_csv(data_directory+\"companyTopic.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyst_vec = GroupVectorizer(tf_type='bm25', apply_idf=True, idf_type='smooth', apply_dl=True, dl_type='linear').fit(trigram_docs, analysts)\n",
    "analyst_doc_term_matrix = analyst_vec.transform(trigram_docs, analysts)\n",
    "\n",
    "\n",
    "tag_vec = GroupVectorizer(tf_type='bm25', apply_idf=True, idf_type='smooth', apply_dl=True, dl_type='linear').fit(trigram_docs, tags)\n",
    "tag_doc_term_matrix = tag_vec.transform(trigram_docs, tags)\n",
    "\n",
    "question_vec = Vectorizer(tf_type='bm25', apply_idf=True, idf_type='smooth', apply_dl=True, dl_type='linear').fit(trigram_docs)\n",
    "question_doc_term_matrix = question_vec.transform(trigram_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_affinity_mat = 1- sp.spatial.distance.cdist(question_doc_term_matrix.toarray(), tag_doc_term_matrix.toarray(), 'cosine')\n",
    "tag_affinity = pd.SparseDataFrame(tag_affinity_mat, columns=tag_vec.grps_list)\n",
    "\n",
    "analyst_affinity_mat = 1- sp.spatial.distance.cdist(question_doc_term_matrix.toarray(), analyst_doc_term_matrix.toarray(), 'cosine')\n",
    "analyst_affinity = pd.SparseDataFrame(analyst_affinity_mat, columns=analyst_vec.grps_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "top3 = ((analyst_affinity['GlennSchorr'] + tag_affinity['Cib'])/2).nlargest(3).reset_index()['index'].values\n",
    "\n",
    "sample_questions = {}\n",
    "sample_questions['GlennSchorr__Cib'] = []\n",
    "\n",
    "for val in top3:\n",
    "    sample_questions['GlennSchorr__Cib'].append(indexed_data.loc[(indexed_data['EventNumber']==tokenized_docs[val][1]['EventNumber'])&\n",
    "                 (indexed_data['QuestionNumber']==tokenized_docs[val][1]['QuestionNumber']), \"Question\"].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I appreciate the color around things related to FICC, and kind of a question of overall backdrop. In Q1 2015, the Swiss re-pegged, stuff went bonkers for a couple of weeks and you made tons of money, even more than the overall industry. This quarter, it didnâ€™t seem like there was too many a-ha moments. I mean, credit spreads tightened and we had the aftermath of Brexit and stuff. But in your text, you point out low rates and slow economic growth as headwinds for FICC. Youâ€™ve talked about lower market volumes and volatility and equities. Iâ€™m just curious, when you look at the quarter we just had in FICC particularly, is it more like you had a nice pickup like Q1 2015 and youâ€™re setting us up for keep calm, like things have returned back to normal? Or is this possibly a little bit higher activity environment given the uncertainty?',\n",
       " 'So in the past, better DCM revenues eventually led to better secondary revenue. And I heard all the comments on this really low vol in just about every asset class. But Iâ€™m curious if thereâ€™s any component that itâ€™s a lot of leverage finance or thereâ€™s a lot of money going into bond funds, just buying every new issue and putting it away. It feels like the whole dynamic has changed as we sit here and wait for vol to pick up. But it seems like your revenues have fallen more than even the â€“ in the drop in vol. I appreciate that. Fair enough. Itâ€™s a little bit of a combination of both. I am asking about the geography and the main contributor to the pickup in DCM and why thereâ€™s not a follow through in FICC as there was in the past.',\n",
       " \"On the whole cyclical versus structural debate in FICC, I heard everything you said and I think you're right and I think you are who you say you are for your clients. But I look at the backdrop and I say the Europeans are actually starting to need to adjust their balance sheets and shrink parts of the business. And so the thought of Goldman maintaining its optionality all these years, this would seem like the payoff, and yet a pickup in volatility in several asset classes it feels like actually the world that was coming your way but yet the revenue reduction for Goldman relative to peers and I am not just talking this quarter, these last couple of quarters, is more pronounced. I am just trying to do the smell test of why is that? Because I actually do think the things are lining up that you'd be more important to your clients, not less. And there would be more opportunities, not less?\"]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_questions[\"GlennSchorr__Cib\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
