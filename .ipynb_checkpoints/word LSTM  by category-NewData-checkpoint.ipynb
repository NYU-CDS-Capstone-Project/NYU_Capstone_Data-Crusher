{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# we learning some concept and idea of word level text generation from: https://machinelearningmastery.com/how-to-develop-a-word-level-neural-language-model-in-keras/\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "import pickle as plk\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import np_utils\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Embedding\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/cyian/Desktop/NYU/FALL2018/DS-GA1006_Capstone'\n",
    "df = pd.read_excel(path+'/Q&A_Database_new.xlsx','QA', skiprows=3)\n",
    "df2 = pd.read_csv(path+ '/combined_csv_forecast.csv', encoding='latin-1')\n",
    "l = df['Breakout'].unique() # 79 unique analyst\n",
    "dic = {} #create dictionary for questions\n",
    "for category in l:\n",
    "    list_ = list(df.loc[df['Breakout']  == category]['Question'])\n",
    "    dic[category] = list_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_new = pd.read_csv('combined_csv_forecast.csv',header = 0, encoding='latin-1')\n",
    "l2 = df_new['sub_lvl'].unique()\n",
    "#create dictionary for questions\n",
    "for category in l2:\n",
    "    list_ = list(df_new.loc[df_new['sub_lvl']  == category]['Question'])\n",
    "    dic[category] = list_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['?', 'you', 'ficc', 'going', 'dont', 'vote', 'to', 'hit', 'them', 'one', 'way', 'or', 'another', 'strong', 'dollar', 'did', 'seem', 'to', 'have', 'a', 'huge', 'impact', 'and', 'you', ',', 'what', 'are', 'you', 'doing', '?', \"i'm\", \"'s\", 'what', 'do', 'you', 'and', 'me', 'think', 'or', 'like', 'apples', 'and', 'apple', 'is', 'looking', 'at', 'buying', 'and', 'bought', 'u', '.k', '.', 'startup', 'for', '$1', 'billion', '.', \"'\", \"'\", 'another', 'sentence']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import re\n",
    "import string\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "UNK_IDX = 0\n",
    "\n",
    "\n",
    "#tokenize sentence by sentence\n",
    "def question_split(input_):\n",
    "    list_ = []\n",
    "    for q in input_:\n",
    "        q = q.split('\\n')\n",
    "        if len(q) ==1 and len(tokenize(q[0]))>= 10:\n",
    "            list_.append(q)\n",
    "        else:\n",
    "            for i in range(len(q)):\n",
    "                if len(tokenize(q[i])) >= 10:\n",
    "                    list_.append(q[i])\n",
    "    return list_\n",
    "    \n",
    "def tokenize(sent):\n",
    "    sent = sent.lower()\n",
    "    sent = re.sub(' y ou', ' you', sent)\n",
    "    sent = re.sub(' y es', ' yes', sent)\n",
    "    sent = re.sub(' v o', ' vo', sent)\n",
    "    sent = re.sub(\"don't\", 'dont', sent)\n",
    "    sent = re.sub( r'([a-zA-Z])([,.!?])', r'\\1 \\2', sent)\n",
    "    #keep punctions\n",
    "#     sent = re.sub('[^A-Za-z&]', ' ', sent) # replace non-letter with space\n",
    "#     sent = re.sub(r'\\b[a-zA-Z]\\b', '', sent) #\n",
    "#     sent = re.sub('^[0-9]+', '', sent)\n",
    "    return sent.split()\n",
    "\n",
    "tokens = tokenize(\" ? y ou FICC going dont v ote to hit them one way or another strong dollar did seem to have a huge impact and y ou, what are you doing? I'm 's what do you and me think or like apples and Apple is looking at buying and bought U.K. startup for $1 billion. '\\n' another sentence\")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load training and validation dataset\n",
    "dic_len = len(dic['Balance sheet']) + len(dic['FIRMWIDE-BALANCE SHEET'])\n",
    "all_data = dic['Balance sheet'] + dic['FIRMWIDE-BALANCE SHEET']\n",
    "train_data = all_data[:int(dic_len*0.8)]\n",
    "val_data = all_data[int(dic_len*0.8): dic_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_tokens = []\n",
    "for q in all_data:\n",
    "    tokens = tokenize(q)\n",
    "    all_tokens += tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "max_vocab_size = 10000\n",
    "# save index 0 for unk and 1 for pad\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "\n",
    "def build_vocab(all_tokens):\n",
    "    # Returns:\n",
    "    # id2token: list of tokens, where id2token[i] returns token that corresponds to token i\n",
    "    # token2id: dictionary where keys represent tokens and corresponding values represent indices\n",
    "    token_counter = Counter(all_tokens)\n",
    "    vocab, count = zip(*token_counter.most_common(max_vocab_size))\n",
    "    id2token = list(vocab)\n",
    "    token2id = dict(zip(vocab, range(2,2+len(vocab)))) \n",
    "    id2token = ['<pad>', '<unk>'] + id2token\n",
    "    token2id['<pad>'] = PAD_IDX \n",
    "    token2id['<unk>'] = UNK_IDX\n",
    "    return token2id, id2token\n",
    "\n",
    "token2id, id2token = build_vocab(all_tokens)\n",
    "vocab = len(token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SEQUENCE_LENGTH = 10\n",
    "# organize into sequences of tokens\n",
    "def build_sequence(length, input_):\n",
    "    sequences = list()\n",
    "    for l in range(length, len(input_)+1):\n",
    "        seq = input_[l-length:l]\n",
    "        line = ' '.join(seq)\n",
    "        sequences.append(line)\n",
    "#     print('Total Sequences: {}' .format(len(sequences)))\n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_generator(dic, length):\n",
    "    out_list = []\n",
    "    raw = question_split(dic)\n",
    "    for i in range(len(raw)):\n",
    "        sent = tokenize(raw[i][0].lower())\n",
    "        if len(sent) >= 10: # screen the length of the question\n",
    "            q_sequence = build_sequence(length, sent)\n",
    "            out_list += q_sequence\n",
    "    return out_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_d = data_generator(all_data,10)\n",
    "train_d = data_generator(train_data,10)\n",
    "val_d = data_generator(val_data,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['on net interest income , do you have an outlook',\n",
       " 'net interest income , do you have an outlook for',\n",
       " 'interest income , do you have an outlook for how',\n",
       " 'income , do you have an outlook for how the',\n",
       " ', do you have an outlook for how the net',\n",
       " 'do you have an outlook for how the net interest',\n",
       " 'you have an outlook for how the net interest income',\n",
       " 'have an outlook for how the net interest income dollars',\n",
       " 'an outlook for how the net interest income dollars could',\n",
       " 'outlook for how the net interest income dollars could trend',\n",
       " 'for how the net interest income dollars could trend from',\n",
       " 'how the net interest income dollars could trend from here',\n",
       " 'the net interest income dollars could trend from here ,',\n",
       " 'net interest income dollars could trend from here , assuming',\n",
       " 'interest income dollars could trend from here , assuming that',\n",
       " 'income dollars could trend from here , assuming that you',\n",
       " 'dollars could trend from here , assuming that you don’t',\n",
       " 'could trend from here , assuming that you don’t get',\n",
       " 'trend from here , assuming that you don’t get much',\n",
       " 'from here , assuming that you don’t get much help',\n",
       " 'here , assuming that you don’t get much help from',\n",
       " ', assuming that you don’t get much help from higher',\n",
       " 'assuming that you don’t get much help from higher rates',\n",
       " 'that you don’t get much help from higher rates ,',\n",
       " 'you don’t get much help from higher rates , what',\n",
       " 'don’t get much help from higher rates , what are',\n",
       " 'get much help from higher rates , what are the',\n",
       " 'much help from higher rates , what are the key',\n",
       " 'help from higher rates , what are the key drivers',\n",
       " 'from higher rates , what are the key drivers ?',\n",
       " 'higher rates , what are the key drivers ? and',\n",
       " 'rates , what are the key drivers ? and what’s',\n",
       " ', what are the key drivers ? and what’s kind',\n",
       " 'what are the key drivers ? and what’s kind of',\n",
       " 'are the key drivers ? and what’s kind of your',\n",
       " 'the key drivers ? and what’s kind of your outlook',\n",
       " 'key drivers ? and what’s kind of your outlook for',\n",
       " 'drivers ? and what’s kind of your outlook for nim',\n",
       " '? and what’s kind of your outlook for nim and',\n",
       " 'and what’s kind of your outlook for nim and nii',\n",
       " 'what’s kind of your outlook for nim and nii dollars',\n",
       " 'kind of your outlook for nim and nii dollars for',\n",
       " 'of your outlook for nim and nii dollars for the',\n",
       " 'your outlook for nim and nii dollars for the year',\n",
       " 'outlook for nim and nii dollars for the year ?',\n",
       " 'on your deposit discussion about pushing out , i think',\n",
       " 'your deposit discussion about pushing out , i think you',\n",
       " 'deposit discussion about pushing out , i think you said',\n",
       " 'discussion about pushing out , i think you said during',\n",
       " 'about pushing out , i think you said during your',\n",
       " 'pushing out , i think you said during your investor',\n",
       " 'out , i think you said during your investor day',\n",
       " ', i think you said during your investor day that',\n",
       " 'i think you said during your investor day that you',\n",
       " 'think you said during your investor day that you would',\n",
       " 'you said during your investor day that you would like',\n",
       " 'said during your investor day that you would like to',\n",
       " 'during your investor day that you would like to get',\n",
       " 'your investor day that you would like to get about',\n",
       " 'investor day that you would like to get about $3b',\n",
       " 'day that you would like to get about $3b of',\n",
       " 'that you would like to get about $3b of non-core',\n",
       " 'you would like to get about $3b of non-core deposits',\n",
       " 'would like to get about $3b of non-core deposits off',\n",
       " 'like to get about $3b of non-core deposits off the',\n",
       " 'to get about $3b of non-core deposits off the balance',\n",
       " 'get about $3b of non-core deposits off the balance sheet',\n",
       " 'about $3b of non-core deposits off the balance sheet ,',\n",
       " '$3b of non-core deposits off the balance sheet , and',\n",
       " 'of non-core deposits off the balance sheet , and i',\n",
       " 'non-core deposits off the balance sheet , and i know',\n",
       " 'deposits off the balance sheet , and i know you',\n",
       " 'off the balance sheet , and i know you said',\n",
       " 'the balance sheet , and i know you said that',\n",
       " 'balance sheet , and i know you said that you',\n",
       " 'sheet , and i know you said that you hope',\n",
       " ', and i know you said that you hope to',\n",
       " 'and i know you said that you hope to make',\n",
       " 'i know you said that you hope to make progress',\n",
       " 'know you said that you hope to make progress in',\n",
       " 'you said that you hope to make progress in q2.',\n",
       " 'said that you hope to make progress in q2. how',\n",
       " 'that you hope to make progress in q2. how should',\n",
       " 'you hope to make progress in q2. how should we',\n",
       " 'hope to make progress in q2. how should we model',\n",
       " 'to make progress in q2. how should we model that',\n",
       " 'make progress in q2. how should we model that out',\n",
       " 'progress in q2. how should we model that out ?',\n",
       " 'in q2. how should we model that out ? and',\n",
       " 'q2. how should we model that out ? and what',\n",
       " 'how should we model that out ? and what type',\n",
       " 'should we model that out ? and what type of',\n",
       " 'we model that out ? and what type of benefit',\n",
       " 'model that out ? and what type of benefit have',\n",
       " 'that out ? and what type of benefit have you',\n",
       " 'out ? and what type of benefit have you modeled',\n",
       " '? and what type of benefit have you modeled out',\n",
       " 'and what type of benefit have you modeled out to',\n",
       " 'what type of benefit have you modeled out to the',\n",
       " 'type of benefit have you modeled out to the nim',\n",
       " 'of benefit have you modeled out to the nim with',\n",
       " 'benefit have you modeled out to the nim with that',\n",
       " 'have you modeled out to the nim with that $3b',\n",
       " 'you modeled out to the nim with that $3b of',\n",
       " 'modeled out to the nim with that $3b of deposits',\n",
       " 'out to the nim with that $3b of deposits ?',\n",
       " 'on a dynamic that we’re seeing with loan growth outpacing',\n",
       " 'a dynamic that we’re seeing with loan growth outpacing deposit',\n",
       " 'dynamic that we’re seeing with loan growth outpacing deposit growth',\n",
       " 'that we’re seeing with loan growth outpacing deposit growth across',\n",
       " 'we’re seeing with loan growth outpacing deposit growth across the',\n",
       " 'seeing with loan growth outpacing deposit growth across the banking',\n",
       " 'with loan growth outpacing deposit growth across the banking system',\n",
       " 'loan growth outpacing deposit growth across the banking system as',\n",
       " 'growth outpacing deposit growth across the banking system as a',\n",
       " 'outpacing deposit growth across the banking system as a whole',\n",
       " 'deposit growth across the banking system as a whole .',\n",
       " 'growth across the banking system as a whole . but',\n",
       " 'across the banking system as a whole . but for',\n",
       " 'the banking system as a whole . but for the',\n",
       " 'banking system as a whole . but for the larger',\n",
       " 'system as a whole . but for the larger banks',\n",
       " 'as a whole . but for the larger banks ,',\n",
       " 'a whole . but for the larger banks , including',\n",
       " 'whole . but for the larger banks , including wells',\n",
       " '. but for the larger banks , including wells fargo',\n",
       " 'but for the larger banks , including wells fargo ,',\n",
       " 'for the larger banks , including wells fargo , i',\n",
       " 'the larger banks , including wells fargo , i guess',\n",
       " 'larger banks , including wells fargo , i guess ,',\n",
       " 'banks , including wells fargo , i guess , we’re',\n",
       " ', including wells fargo , i guess , we’re still',\n",
       " 'including wells fargo , i guess , we’re still seeing',\n",
       " 'wells fargo , i guess , we’re still seeing deposit',\n",
       " 'fargo , i guess , we’re still seeing deposit growth',\n",
       " ', i guess , we’re still seeing deposit growth outpacing',\n",
       " 'i guess , we’re still seeing deposit growth outpacing loan',\n",
       " 'guess , we’re still seeing deposit growth outpacing loan growth',\n",
       " ', we’re still seeing deposit growth outpacing loan growth .',\n",
       " 'we’re still seeing deposit growth outpacing loan growth . i',\n",
       " 'still seeing deposit growth outpacing loan growth . i was',\n",
       " 'seeing deposit growth outpacing loan growth . i was hoping',\n",
       " 'deposit growth outpacing loan growth . i was hoping that',\n",
       " 'growth outpacing loan growth . i was hoping that you',\n",
       " 'outpacing loan growth . i was hoping that you could',\n",
       " 'loan growth . i was hoping that you could just',\n",
       " 'growth . i was hoping that you could just talk',\n",
       " '. i was hoping that you could just talk about',\n",
       " 'i was hoping that you could just talk about what',\n",
       " 'was hoping that you could just talk about what you',\n",
       " 'hoping that you could just talk about what you think',\n",
       " 'that you could just talk about what you think may',\n",
       " 'you could just talk about what you think may be',\n",
       " 'could just talk about what you think may be driving',\n",
       " 'just talk about what you think may be driving that',\n",
       " 'talk about what you think may be driving that ?',\n",
       " 'about what you think may be driving that ? and',\n",
       " 'what you think may be driving that ? and do',\n",
       " 'you think may be driving that ? and do you',\n",
       " 'think may be driving that ? and do you see',\n",
       " 'may be driving that ? and do you see anything',\n",
       " 'be driving that ? and do you see anything on',\n",
       " 'driving that ? and do you see anything on the',\n",
       " 'that ? and do you see anything on the horizon',\n",
       " '? and do you see anything on the horizon that',\n",
       " 'and do you see anything on the horizon that you',\n",
       " 'do you see anything on the horizon that you think',\n",
       " 'you see anything on the horizon that you think could',\n",
       " 'see anything on the horizon that you think could change',\n",
       " 'anything on the horizon that you think could change that',\n",
       " 'on the horizon that you think could change that ?',\n",
       " 'the horizon that you think could change that ? i',\n",
       " 'horizon that you think could change that ? i guess',\n",
       " 'that you think could change that ? i guess ,',\n",
       " 'you think could change that ? i guess , in',\n",
       " 'think could change that ? i guess , in other',\n",
       " 'could change that ? i guess , in other words',\n",
       " 'change that ? i guess , in other words ,',\n",
       " 'that ? i guess , in other words , just',\n",
       " '? i guess , in other words , just lead',\n",
       " 'i guess , in other words , just lead to',\n",
       " 'guess , in other words , just lead to loan',\n",
       " ', in other words , just lead to loan growth',\n",
       " 'in other words , just lead to loan growth outpacing',\n",
       " 'other words , just lead to loan growth outpacing deposit',\n",
       " 'words , just lead to loan growth outpacing deposit growth',\n",
       " ', just lead to loan growth outpacing deposit growth at',\n",
       " 'just lead to loan growth outpacing deposit growth at the',\n",
       " 'lead to loan growth outpacing deposit growth at the large',\n",
       " 'to loan growth outpacing deposit growth at the large banks',\n",
       " 'loan growth outpacing deposit growth at the large banks as',\n",
       " 'growth outpacing deposit growth at the large banks as well',\n",
       " 'outpacing deposit growth at the large banks as well ?',\n",
       " 'could you provide any color to help us understand the',\n",
       " 'you provide any color to help us understand the new',\n",
       " 'provide any color to help us understand the new loan',\n",
       " 'any color to help us understand the new loan purchases',\n",
       " 'color to help us understand the new loan purchases ?',\n",
       " 'to help us understand the new loan purchases ? is',\n",
       " 'help us understand the new loan purchases ? is there',\n",
       " 'us understand the new loan purchases ? is there any',\n",
       " 'understand the new loan purchases ? is there any way',\n",
       " 'the new loan purchases ? is there any way to',\n",
       " 'new loan purchases ? is there any way to put',\n",
       " 'loan purchases ? is there any way to put in',\n",
       " 'purchases ? is there any way to put in context',\n",
       " '? is there any way to put in context either',\n",
       " 'is there any way to put in context either the',\n",
       " 'there any way to put in context either the yield',\n",
       " 'any way to put in context either the yield of',\n",
       " 'way to put in context either the yield of the',\n",
       " 'to put in context either the yield of the portfolio',\n",
       " 'put in context either the yield of the portfolio relative',\n",
       " 'in context either the yield of the portfolio relative to',\n",
       " 'context either the yield of the portfolio relative to the',\n",
       " 'either the yield of the portfolio relative to the existing',\n",
       " 'the yield of the portfolio relative to the existing yield',\n",
       " 'yield of the portfolio relative to the existing yield of',\n",
       " 'of the portfolio relative to the existing yield of the',\n",
       " 'the portfolio relative to the existing yield of the wells',\n",
       " 'portfolio relative to the existing yield of the wells fargo',\n",
       " 'relative to the existing yield of the wells fargo book',\n",
       " 'to the existing yield of the wells fargo book ?',\n",
       " 'the existing yield of the wells fargo book ? and',\n",
       " 'existing yield of the wells fargo book ? and do',\n",
       " 'yield of the wells fargo book ? and do you',\n",
       " 'of the wells fargo book ? and do you bring',\n",
       " 'the wells fargo book ? and do you bring over',\n",
       " 'wells fargo book ? and do you bring over expenses',\n",
       " 'fargo book ? and do you bring over expenses with',\n",
       " 'book ? and do you bring over expenses with it',\n",
       " '? and do you bring over expenses with it ?',\n",
       " 'and do you bring over expenses with it ? or',\n",
       " 'do you bring over expenses with it ? or is',\n",
       " 'you bring over expenses with it ? or is it',\n",
       " 'bring over expenses with it ? or is it largely',\n",
       " 'over expenses with it ? or is it largely just',\n",
       " 'expenses with it ? or is it largely just interest',\n",
       " 'with it ? or is it largely just interest income',\n",
       " 'it ? or is it largely just interest income coming',\n",
       " '? or is it largely just interest income coming over',\n",
       " 'or is it largely just interest income coming over as',\n",
       " 'is it largely just interest income coming over as has',\n",
       " 'it largely just interest income coming over as has happened',\n",
       " 'largely just interest income coming over as has happened in',\n",
       " 'just interest income coming over as has happened in your',\n",
       " 'interest income coming over as has happened in your prior',\n",
       " 'income coming over as has happened in your prior acquisitions',\n",
       " 'coming over as has happened in your prior acquisitions of',\n",
       " 'over as has happened in your prior acquisitions of loan',\n",
       " 'as has happened in your prior acquisitions of loan books',\n",
       " 'has happened in your prior acquisitions of loan books ?',\n",
       " 'with regards to potential other portfolios that ge had talked',\n",
       " 'regards to potential other portfolios that ge had talked about',\n",
       " 'to potential other portfolios that ge had talked about ,',\n",
       " 'potential other portfolios that ge had talked about , given',\n",
       " 'other portfolios that ge had talked about , given your',\n",
       " 'portfolios that ge had talked about , given your positioning',\n",
       " 'that ge had talked about , given your positioning in',\n",
       " 'ge had talked about , given your positioning in terms',\n",
       " 'had talked about , given your positioning in terms of',\n",
       " 'talked about , given your positioning in terms of helping',\n",
       " 'about , given your positioning in terms of helping them',\n",
       " ', given your positioning in terms of helping them through',\n",
       " 'given your positioning in terms of helping them through the',\n",
       " 'your positioning in terms of helping them through the structure',\n",
       " 'positioning in terms of helping them through the structure ,',\n",
       " 'in terms of helping them through the structure , as',\n",
       " 'terms of helping them through the structure , as you',\n",
       " 'of helping them through the structure , as you mentioned',\n",
       " 'helping them through the structure , as you mentioned ,',\n",
       " 'them through the structure , as you mentioned , do',\n",
       " 'through the structure , as you mentioned , do you',\n",
       " 'the structure , as you mentioned , do you guys',\n",
       " 'structure , as you mentioned , do you guys get',\n",
       " ', as you mentioned , do you guys get a',\n",
       " 'as you mentioned , do you guys get a different',\n",
       " 'you mentioned , do you guys get a different look',\n",
       " 'mentioned , do you guys get a different look ?',\n",
       " ', do you guys get a different look ? or',\n",
       " 'do you guys get a different look ? or have',\n",
       " 'you guys get a different look ? or have you',\n",
       " 'guys get a different look ? or have you already',\n",
       " 'get a different look ? or have you already taken',\n",
       " 'a different look ? or have you already taken a',\n",
       " 'different look ? or have you already taken a look',\n",
       " 'look ? or have you already taken a look at',\n",
       " '? or have you already taken a look at other',\n",
       " 'or have you already taken a look at other parts',\n",
       " 'have you already taken a look at other parts of',\n",
       " 'you already taken a look at other parts of the',\n",
       " 'already taken a look at other parts of the book',\n",
       " 'taken a look at other parts of the book and',\n",
       " 'a look at other parts of the book and this',\n",
       " 'look at other parts of the book and this is',\n",
       " 'at other parts of the book and this is just',\n",
       " 'other parts of the book and this is just the',\n",
       " 'parts of the book and this is just the first',\n",
       " 'of the book and this is just the first tranche',\n",
       " 'the book and this is just the first tranche of',\n",
       " 'book and this is just the first tranche of what',\n",
       " 'and this is just the first tranche of what could',\n",
       " 'this is just the first tranche of what could be',\n",
       " 'is just the first tranche of what could be more',\n",
       " 'just the first tranche of what could be more ?',\n",
       " 'the first tranche of what could be more ? or',\n",
       " 'first tranche of what could be more ? or what’s',\n",
       " 'tranche of what could be more ? or what’s your',\n",
       " 'of what could be more ? or what’s your just',\n",
       " 'what could be more ? or what’s your just general',\n",
       " 'could be more ? or what’s your just general perception',\n",
       " 'be more ? or what’s your just general perception of',\n",
       " 'more ? or what’s your just general perception of the',\n",
       " '? or what’s your just general perception of the opportunity',\n",
       " 'or what’s your just general perception of the opportunity set',\n",
       " 'what’s your just general perception of the opportunity set that',\n",
       " 'your just general perception of the opportunity set that could',\n",
       " 'just general perception of the opportunity set that could come',\n",
       " 'general perception of the opportunity set that could come forth',\n",
       " 'perception of the opportunity set that could come forth for',\n",
       " 'of the opportunity set that could come forth for wells',\n",
       " 'the opportunity set that could come forth for wells fargo',\n",
       " 'opportunity set that could come forth for wells fargo ?',\n",
       " 'compared to q4, the commercial loan growth was a little',\n",
       " 'to q4, the commercial loan growth was a little slower',\n",
       " 'q4, the commercial loan growth was a little slower outside',\n",
       " 'the commercial loan growth was a little slower outside of',\n",
       " 'commercial loan growth was a little slower outside of real',\n",
       " 'loan growth was a little slower outside of real estate',\n",
       " 'growth was a little slower outside of real estate construction',\n",
       " 'was a little slower outside of real estate construction .',\n",
       " 'a little slower outside of real estate construction . was',\n",
       " 'little slower outside of real estate construction . was some',\n",
       " 'slower outside of real estate construction . was some of',\n",
       " 'outside of real estate construction . was some of that',\n",
       " 'of real estate construction . was some of that seasonal',\n",
       " 'real estate construction . was some of that seasonal or',\n",
       " 'estate construction . was some of that seasonal or were',\n",
       " 'construction . was some of that seasonal or were there',\n",
       " '. was some of that seasonal or were there other',\n",
       " 'was some of that seasonal or were there other factors',\n",
       " 'some of that seasonal or were there other factors in',\n",
       " 'of that seasonal or were there other factors in play',\n",
       " 'that seasonal or were there other factors in play there',\n",
       " 'seasonal or were there other factors in play there ?',\n",
       " 'or were there other factors in play there ? how',\n",
       " 'were there other factors in play there ? how do',\n",
       " 'there other factors in play there ? how do you',\n",
       " 'other factors in play there ? how do you feel',\n",
       " 'factors in play there ? how do you feel about',\n",
       " 'in play there ? how do you feel about the',\n",
       " 'play there ? how do you feel about the current',\n",
       " 'there ? how do you feel about the current levels',\n",
       " '? how do you feel about the current levels of',\n",
       " 'how do you feel about the current levels of demand',\n",
       " 'do you feel about the current levels of demand on',\n",
       " 'you feel about the current levels of demand on the',\n",
       " 'feel about the current levels of demand on the commercial',\n",
       " 'about the current levels of demand on the commercial side',\n",
       " 'the current levels of demand on the commercial side ?',\n",
       " 'i was curious what your early read is on the',\n",
       " 'was curious what your early read is on the spring',\n",
       " 'curious what your early read is on the spring selling',\n",
       " 'what your early read is on the spring selling season',\n",
       " 'your early read is on the spring selling season this',\n",
       " 'early read is on the spring selling season this year',\n",
       " 'read is on the spring selling season this year .',\n",
       " 'is on the spring selling season this year . i',\n",
       " 'on the spring selling season this year . i know',\n",
       " 'the spring selling season this year . i know you’re',\n",
       " 'spring selling season this year . i know you’re starting',\n",
       " 'selling season this year . i know you’re starting to',\n",
       " 'season this year . i know you’re starting to see',\n",
       " 'this year . i know you’re starting to see any',\n",
       " 'year . i know you’re starting to see any signs',\n",
       " '. i know you’re starting to see any signs for',\n",
       " 'i know you’re starting to see any signs for return',\n",
       " 'know you’re starting to see any signs for return of',\n",
       " 'you’re starting to see any signs for return of the',\n",
       " 'starting to see any signs for return of the first',\n",
       " 'to see any signs for return of the first time',\n",
       " 'see any signs for return of the first time home',\n",
       " 'any signs for return of the first time home buyer',\n",
       " 'signs for return of the first time home buyer .',\n",
       " 'can you talk a little bit more on the housing',\n",
       " 'you talk a little bit more on the housing backlog',\n",
       " 'talk a little bit more on the housing backlog ?',\n",
       " 'a little bit more on the housing backlog ? how',\n",
       " 'little bit more on the housing backlog ? how does',\n",
       " 'bit more on the housing backlog ? how does it',\n",
       " 'more on the housing backlog ? how does it look',\n",
       " 'on the housing backlog ? how does it look by',\n",
       " 'the housing backlog ? how does it look by region',\n",
       " 'housing backlog ? how does it look by region and',\n",
       " 'backlog ? how does it look by region and what',\n",
       " '? how does it look by region and what are',\n",
       " 'how does it look by region and what are the',\n",
       " 'does it look by region and what are the areas',\n",
       " 'it look by region and what are the areas of',\n",
       " 'look by region and what are the areas of strength',\n",
       " 'by region and what are the areas of strength ?',\n",
       " 'on the mortgage competitive landscape here . we saw jpmorgan',\n",
       " 'the mortgage competitive landscape here . we saw jpmorgan out',\n",
       " 'mortgage competitive landscape here . we saw jpmorgan out earlier',\n",
       " 'competitive landscape here . we saw jpmorgan out earlier today',\n",
       " 'landscape here . we saw jpmorgan out earlier today with',\n",
       " 'here . we saw jpmorgan out earlier today with a',\n",
       " '. we saw jpmorgan out earlier today with a big',\n",
       " 'we saw jpmorgan out earlier today with a big increase',\n",
       " 'saw jpmorgan out earlier today with a big increase in',\n",
       " 'jpmorgan out earlier today with a big increase in correspondent',\n",
       " 'out earlier today with a big increase in correspondent originations',\n",
       " 'earlier today with a big increase in correspondent originations vs',\n",
       " 'today with a big increase in correspondent originations vs .',\n",
       " 'with a big increase in correspondent originations vs . a',\n",
       " 'a big increase in correspondent originations vs . a year',\n",
       " 'big increase in correspondent originations vs . a year ago',\n",
       " 'increase in correspondent originations vs . a year ago .',\n",
       " 'in correspondent originations vs . a year ago . and',\n",
       " 'correspondent originations vs . a year ago . and maybe',\n",
       " 'originations vs . a year ago . and maybe you’ve',\n",
       " 'vs . a year ago . and maybe you’ve been',\n",
       " '. a year ago . and maybe you’ve been a',\n",
       " 'a year ago . and maybe you’ve been a little',\n",
       " 'year ago . and maybe you’ve been a little more',\n",
       " 'ago . and maybe you’ve been a little more consistent',\n",
       " '. and maybe you’ve been a little more consistent in',\n",
       " 'and maybe you’ve been a little more consistent in that',\n",
       " 'maybe you’ve been a little more consistent in that business',\n",
       " 'you’ve been a little more consistent in that business .',\n",
       " 'been a little more consistent in that business . but',\n",
       " 'a little more consistent in that business . but if',\n",
       " 'little more consistent in that business . but if you',\n",
       " 'more consistent in that business . but if you could',\n",
       " 'consistent in that business . but if you could just',\n",
       " 'in that business . but if you could just remind',\n",
       " 'that business . but if you could just remind us',\n",
       " 'business . but if you could just remind us on',\n",
       " '. but if you could just remind us on the',\n",
       " 'but if you could just remind us on the correspondent',\n",
       " 'if you could just remind us on the correspondent and',\n",
       " 'you could just remind us on the correspondent and talk',\n",
       " 'could just remind us on the correspondent and talk about',\n",
       " 'just remind us on the correspondent and talk about the',\n",
       " 'remind us on the correspondent and talk about the mix',\n",
       " 'us on the correspondent and talk about the mix that',\n",
       " 'on the correspondent and talk about the mix that you’re',\n",
       " 'the correspondent and talk about the mix that you’re going',\n",
       " 'correspondent and talk about the mix that you’re going after',\n",
       " 'and talk about the mix that you’re going after right',\n",
       " 'talk about the mix that you’re going after right now',\n",
       " 'about the mix that you’re going after right now ?',\n",
       " 'could you talk about loan growth expectations ? you saw',\n",
       " 'you talk about loan growth expectations ? you saw declines',\n",
       " 'talk about loan growth expectations ? you saw declines in',\n",
       " 'about loan growth expectations ? you saw declines in your',\n",
       " 'loan growth expectations ? you saw declines in your total',\n",
       " 'growth expectations ? you saw declines in your total cre',\n",
       " 'expectations ? you saw declines in your total cre book',\n",
       " '? you saw declines in your total cre book .',\n",
       " 'you saw declines in your total cre book . want',\n",
       " 'saw declines in your total cre book . want to',\n",
       " 'declines in your total cre book . want to get',\n",
       " 'in your total cre book . want to get some',\n",
       " 'your total cre book . want to get some thoughts',\n",
       " 'total cre book . want to get some thoughts around',\n",
       " 'cre book . want to get some thoughts around a',\n",
       " 'book . want to get some thoughts around a sustained',\n",
       " '. want to get some thoughts around a sustained inflection',\n",
       " 'want to get some thoughts around a sustained inflection in',\n",
       " 'to get some thoughts around a sustained inflection in that',\n",
       " 'get some thoughts around a sustained inflection in that portfolio',\n",
       " 'some thoughts around a sustained inflection in that portfolio organically',\n",
       " 'thoughts around a sustained inflection in that portfolio organically ,',\n",
       " 'around a sustained inflection in that portfolio organically , and',\n",
       " 'a sustained inflection in that portfolio organically , and then',\n",
       " 'sustained inflection in that portfolio organically , and then is',\n",
       " 'inflection in that portfolio organically , and then is it',\n",
       " 'in that portfolio organically , and then is it also',\n",
       " 'that portfolio organically , and then is it also fair',\n",
       " 'portfolio organically , and then is it also fair to',\n",
       " 'organically , and then is it also fair to assume',\n",
       " ', and then is it also fair to assume that',\n",
       " 'and then is it also fair to assume that the',\n",
       " 'then is it also fair to assume that the loan',\n",
       " 'is it also fair to assume that the loan growth',\n",
       " 'it also fair to assume that the loan growth overall',\n",
       " 'also fair to assume that the loan growth overall should',\n",
       " 'fair to assume that the loan growth overall should remain',\n",
       " 'to assume that the loan growth overall should remain in',\n",
       " 'assume that the loan growth overall should remain in the',\n",
       " 'that the loan growth overall should remain in the mid-single',\n",
       " 'the loan growth overall should remain in the mid-single digit',\n",
       " 'loan growth overall should remain in the mid-single digit range',\n",
       " 'growth overall should remain in the mid-single digit range at',\n",
       " 'overall should remain in the mid-single digit range at this',\n",
       " 'should remain in the mid-single digit range at this point',\n",
       " 'remain in the mid-single digit range at this point ?',\n",
       " 'can you guys share with us some of the color',\n",
       " 'you guys share with us some of the color –',\n",
       " 'guys share with us some of the color – you’ve',\n",
       " 'share with us some of the color – you’ve had',\n",
       " 'with us some of the color – you’ve had real',\n",
       " 'us some of the color – you’ve had real big',\n",
       " 'some of the color – you’ve had real big success',\n",
       " 'of the color – you’ve had real big success on',\n",
       " 'the color – you’ve had real big success on the',\n",
       " 'color – you’ve had real big success on the commercial',\n",
       " '– you’ve had real big success on the commercial loan',\n",
       " 'you’ve had real big success on the commercial loan growth',\n",
       " 'had real big success on the commercial loan growth on',\n",
       " 'real big success on the commercial loan growth on a',\n",
       " 'big success on the commercial loan growth on a y-over-y',\n",
       " 'success on the commercial loan growth on a y-over-y basis',\n",
       " 'on the commercial loan growth on a y-over-y basis ,',\n",
       " 'the commercial loan growth on a y-over-y basis , in',\n",
       " 'commercial loan growth on a y-over-y basis , in particular',\n",
       " 'loan growth on a y-over-y basis , in particular ,',\n",
       " 'growth on a y-over-y basis , in particular , commercial',\n",
       " 'on a y-over-y basis , in particular , commercial mortgage',\n",
       " 'a y-over-y basis , in particular , commercial mortgage is',\n",
       " 'y-over-y basis , in particular , commercial mortgage is real',\n",
       " 'basis , in particular , commercial mortgage is real strong',\n",
       " ', in particular , commercial mortgage is real strong .',\n",
       " 'in particular , commercial mortgage is real strong . could',\n",
       " 'particular , commercial mortgage is real strong . could you',\n",
       " ', commercial mortgage is real strong . could you share',\n",
       " 'commercial mortgage is real strong . could you share with',\n",
       " 'mortgage is real strong . could you share with us',\n",
       " 'is real strong . could you share with us where',\n",
       " 'real strong . could you share with us where that',\n",
       " 'strong . could you share with us where that growth',\n",
       " '. could you share with us where that growth is',\n",
       " 'could you share with us where that growth is coming',\n",
       " 'you share with us where that growth is coming from',\n",
       " 'share with us where that growth is coming from .',\n",
       " 'with us where that growth is coming from . is',\n",
       " 'us where that growth is coming from . is it',\n",
       " 'where that growth is coming from . is it geographically',\n",
       " 'that growth is coming from . is it geographically ,',\n",
       " 'growth is coming from . is it geographically , is',\n",
       " 'is coming from . is it geographically , is it',\n",
       " 'coming from . is it geographically , is it southeast',\n",
       " 'from . is it geographically , is it southeast vs',\n",
       " '. is it geographically , is it southeast vs .',\n",
       " 'is it geographically , is it southeast vs . the',\n",
       " 'it geographically , is it southeast vs . the midwest',\n",
       " 'geographically , is it southeast vs . the midwest or',\n",
       " ', is it southeast vs . the midwest or where',\n",
       " 'is it southeast vs . the midwest or where is',\n",
       " 'it southeast vs . the midwest or where is the',\n",
       " 'southeast vs . the midwest or where is the best',\n",
       " 'vs . the midwest or where is the best growth',\n",
       " '. the midwest or where is the best growth coming',\n",
       " 'the midwest or where is the best growth coming from',\n",
       " 'midwest or where is the best growth coming from ?',\n",
       " 'i see . and then can you guys give us',\n",
       " 'see . and then can you guys give us any',\n",
       " '. and then can you guys give us any color',\n",
       " 'and then can you guys give us any color on',\n",
       " 'then can you guys give us any color on the',\n",
       " 'can you guys give us any color on the utilization',\n",
       " 'you guys give us any color on the utilization rates',\n",
       " 'guys give us any color on the utilization rates of',\n",
       " 'give us any color on the utilization rates of the',\n",
       " 'us any color on the utilization rates of the commercial',\n",
       " 'any color on the utilization rates of the commercial lines',\n",
       " 'color on the utilization rates of the commercial lines ?',\n",
       " 'on the utilization rates of the commercial lines ? did',\n",
       " 'the utilization rates of the commercial lines ? did they',\n",
       " 'utilization rates of the commercial lines ? did they go',\n",
       " 'rates of the commercial lines ? did they go up',\n",
       " 'of the commercial lines ? did they go up this',\n",
       " 'the commercial lines ? did they go up this quarter',\n",
       " 'commercial lines ? did they go up this quarter ?',\n",
       " 'lines ? did they go up this quarter ? some',\n",
       " '? did they go up this quarter ? some of',\n",
       " 'did they go up this quarter ? some of your',\n",
       " 'they go up this quarter ? some of your peers',\n",
       " 'go up this quarter ? some of your peers are',\n",
       " 'up this quarter ? some of your peers are suggesting',\n",
       " 'this quarter ? some of your peers are suggesting higher',\n",
       " 'quarter ? some of your peers are suggesting higher utilization',\n",
       " '? some of your peers are suggesting higher utilization rates',\n",
       " 'some of your peers are suggesting higher utilization rates .',\n",
       " 'just a couple of rate-related questions . if we get',\n",
       " 'a couple of rate-related questions . if we get rising',\n",
       " 'couple of rate-related questions . if we get rising short-term',\n",
       " 'of rate-related questions . if we get rising short-term rates',\n",
       " 'rate-related questions . if we get rising short-term rates but',\n",
       " 'questions . if we get rising short-term rates but nothing',\n",
       " '. if we get rising short-term rates but nothing on',\n",
       " 'if we get rising short-term rates but nothing on the',\n",
       " 'we get rising short-term rates but nothing on the long',\n",
       " 'get rising short-term rates but nothing on the long end',\n",
       " 'rising short-term rates but nothing on the long end and',\n",
       " 'short-term rates but nothing on the long end and i',\n",
       " 'rates but nothing on the long end and i guess',\n",
       " 'but nothing on the long end and i guess in',\n",
       " 'nothing on the long end and i guess in between',\n",
       " 'on the long end and i guess in between things',\n",
       " 'the long end and i guess in between things go',\n",
       " 'long end and i guess in between things go up',\n",
       " 'end and i guess in between things go up a',\n",
       " 'and i guess in between things go up a little',\n",
       " 'i guess in between things go up a little bit',\n",
       " 'guess in between things go up a little bit but',\n",
       " 'in between things go up a little bit but not',\n",
       " 'between things go up a little bit but not as',\n",
       " 'things go up a little bit but not as much',\n",
       " 'go up a little bit but not as much as',\n",
       " 'up a little bit but not as much as the',\n",
       " 'a little bit but not as much as the short',\n",
       " 'little bit but not as much as the short end',\n",
       " 'bit but not as much as the short end so',\n",
       " 'but not as much as the short end so you',\n",
       " 'not as much as the short end so you get',\n",
       " 'as much as the short end so you get a',\n",
       " 'much as the short end so you get a flattening',\n",
       " 'as the short end so you get a flattening yield',\n",
       " 'the short end so you get a flattening yield curve',\n",
       " 'short end so you get a flattening yield curve .',\n",
       " 'end so you get a flattening yield curve . remind',\n",
       " 'so you get a flattening yield curve . remind us',\n",
       " 'you get a flattening yield curve . remind us how',\n",
       " 'get a flattening yield curve . remind us how sensitive',\n",
       " 'a flattening yield curve . remind us how sensitive you',\n",
       " 'flattening yield curve . remind us how sensitive you are',\n",
       " 'yield curve . remind us how sensitive you are to',\n",
       " 'curve . remind us how sensitive you are to that',\n",
       " '. remind us how sensitive you are to that scenario',\n",
       " 'remind us how sensitive you are to that scenario ?',\n",
       " 'and just one – ifi could just sneak one more',\n",
       " 'just one – ifi could just sneak one more ,',\n",
       " 'one – ifi could just sneak one more , how',\n",
       " '– ifi could just sneak one more , how should',\n",
       " 'ifi could just sneak one more , how should we',\n",
       " 'could just sneak one more , how should we read',\n",
       " 'just sneak one more , how should we read into',\n",
       " 'sneak one more , how should we read into your',\n",
       " 'one more , how should we read into your lcr',\n",
       " 'more , how should we read into your lcr becoming',\n",
       " ', how should we read into your lcr becoming –',\n",
       " 'how should we read into your lcr becoming – beingover',\n",
       " 'should we read into your lcr becoming – beingover 100%',\n",
       " 'we read into your lcr becoming – beingover 100% relative',\n",
       " 'read into your lcr becoming – beingover 100% relative to',\n",
       " 'into your lcr becoming – beingover 100% relative to the',\n",
       " 'your lcr becoming – beingover 100% relative to the 80%',\n",
       " 'lcr becoming – beingover 100% relative to the 80% 2015',\n",
       " 'becoming – beingover 100% relative to the 80% 2015 minimum',\n",
       " '– beingover 100% relative to the 80% 2015 minimum ?',\n",
       " 'beingover 100% relative to the 80% 2015 minimum ? i',\n",
       " '100% relative to the 80% 2015 minimum ? i know',\n",
       " 'relative to the 80% 2015 minimum ? i know that',\n",
       " 'to the 80% 2015 minimum ? i know that this',\n",
       " 'the 80% 2015 minimum ? i know that this ratio',\n",
       " '80% 2015 minimum ? i know that this ratio is',\n",
       " '2015 minimum ? i know that this ratio is quite',\n",
       " 'minimum ? i know that this ratio is quite volatile',\n",
       " '? i know that this ratio is quite volatile ,',\n",
       " 'i know that this ratio is quite volatile , but',\n",
       " 'know that this ratio is quite volatile , but is',\n",
       " 'that this ratio is quite volatile , but is there',\n",
       " 'this ratio is quite volatile , but is there an',\n",
       " 'ratio is quite volatile , but is there an opportunity',\n",
       " 'is quite volatile , but is there an opportunity to',\n",
       " 'quite volatile , but is there an opportunity to maybe',\n",
       " 'volatile , but is there an opportunity to maybe add',\n",
       " ', but is there an opportunity to maybe add duration',\n",
       " 'but is there an opportunity to maybe add duration to',\n",
       " 'is there an opportunity to maybe add duration to defend',\n",
       " 'there an opportunity to maybe add duration to defend the',\n",
       " 'an opportunity to maybe add duration to defend the nim',\n",
       " 'opportunity to maybe add duration to defend the nim or',\n",
       " 'to maybe add duration to defend the nim or are',\n",
       " 'maybe add duration to defend the nim or are you',\n",
       " 'add duration to defend the nim or are you happy',\n",
       " 'duration to defend the nim or are you happy to',\n",
       " 'to defend the nim or are you happy to just',\n",
       " 'defend the nim or are you happy to just stay',\n",
       " 'the nim or are you happy to just stay sure',\n",
       " 'nim or are you happy to just stay sure and',\n",
       " 'or are you happy to just stay sure and keep',\n",
       " 'are you happy to just stay sure and keep this',\n",
       " 'you happy to just stay sure and keep this excess',\n",
       " 'happy to just stay sure and keep this excess ?',\n",
       " 'bill , let me follow up , bill demchak ,',\n",
       " ', let me follow up , bill demchak , let',\n",
       " 'let me follow up , bill demchak , let me',\n",
       " 'me follow up , bill demchak , let me follow',\n",
       " 'follow up , bill demchak , let me follow up',\n",
       " 'up , bill demchak , let me follow up to',\n",
       " ', bill demchak , let me follow up to that',\n",
       " 'bill demchak , let me follow up to that answer',\n",
       " 'demchak , let me follow up to that answer you',\n",
       " ', let me follow up to that answer you just',\n",
       " 'let me follow up to that answer you just gave',\n",
       " 'me follow up to that answer you just gave .',\n",
       " 'follow up to that answer you just gave . the',\n",
       " 'up to that answer you just gave . the growth',\n",
       " 'to that answer you just gave . the growth in',\n",
       " 'that answer you just gave . the growth in securities',\n",
       " 'answer you just gave . the growth in securities ,',\n",
       " 'you just gave . the growth in securities , the',\n",
       " 'just gave . the growth in securities , the longer',\n",
       " 'gave . the growth in securities , the longer duration',\n",
       " '. the growth in securities , the longer duration securities',\n",
       " 'the growth in securities , the longer duration securities ,',\n",
       " 'growth in securities , the longer duration securities , was',\n",
       " 'in securities , the longer duration securities , was that',\n",
       " 'securities , the longer duration securities , was that in',\n",
       " ', the longer duration securities , was that in the',\n",
       " 'the longer duration securities , was that in the last',\n",
       " 'longer duration securities , was that in the last couple',\n",
       " 'duration securities , was that in the last couple of',\n",
       " 'securities , was that in the last couple of quarters',\n",
       " ', was that in the last couple of quarters you',\n",
       " 'was that in the last couple of quarters you did',\n",
       " 'that in the last couple of quarters you did more',\n",
       " 'in the last couple of quarters you did more synthetics',\n",
       " 'the last couple of quarters you did more synthetics ,',\n",
       " 'last couple of quarters you did more synthetics , you',\n",
       " 'couple of quarters you did more synthetics , you used',\n",
       " 'of quarters you did more synthetics , you used rather',\n",
       " 'quarters you did more synthetics , you used rather than',\n",
       " 'you did more synthetics , you used rather than cash',\n",
       " 'did more synthetics , you used rather than cash .',\n",
       " 'more synthetics , you used rather than cash . was',\n",
       " 'synthetics , you used rather than cash . was this',\n",
       " ', you used rather than cash . was this just',\n",
       " 'you used rather than cash . was this just a',\n",
       " 'used rather than cash . was this just a shift',\n",
       " 'rather than cash . was this just a shift to',\n",
       " 'than cash . was this just a shift to cash',\n",
       " 'cash . was this just a shift to cash and',\n",
       " '. was this just a shift to cash and rather',\n",
       " 'was this just a shift to cash and rather than',\n",
       " 'this just a shift to cash and rather than using',\n",
       " 'just a shift to cash and rather than using the',\n",
       " 'a shift to cash and rather than using the synthetics',\n",
       " 'shift to cash and rather than using the synthetics or',\n",
       " 'to cash and rather than using the synthetics or did',\n",
       " 'cash and rather than using the synthetics or did you',\n",
       " 'and rather than using the synthetics or did you actually',\n",
       " 'rather than using the synthetics or did you actually do',\n",
       " 'than using the synthetics or did you actually do more',\n",
       " 'using the synthetics or did you actually do more ?',\n",
       " 'last quick thing on just also on low rates ,',\n",
       " 'quick thing on just also on low rates , is',\n",
       " 'thing on just also on low rates , is there',\n",
       " 'on just also on low rates , is there any',\n",
       " 'just also on low rates , is there any more',\n",
       " 'also on low rates , is there any more room',\n",
       " 'on low rates , is there any more room on',\n",
       " 'low rates , is there any more room on interest',\n",
       " 'rates , is there any more room on interest expense',\n",
       " ', is there any more room on interest expense ,',\n",
       " 'is there any more room on interest expense , anything',\n",
       " 'there any more room on interest expense , anything you',\n",
       " 'any more room on interest expense , anything you could',\n",
       " 'more room on interest expense , anything you could do',\n",
       " 'room on interest expense , anything you could do whether',\n",
       " \"on interest expense , anything you could do whether it's\",\n",
       " \"interest expense , anything you could do whether it's deposits\",\n",
       " \"expense , anything you could do whether it's deposits or\",\n",
       " \", anything you could do whether it's deposits or wholesale\",\n",
       " \"anything you could do whether it's deposits or wholesale funding\",\n",
       " \"you could do whether it's deposits or wholesale funding or\",\n",
       " \"could do whether it's deposits or wholesale funding or are\",\n",
       " \"do whether it's deposits or wholesale funding or are you\",\n",
       " \"whether it's deposits or wholesale funding or are you really\",\n",
       " \"it's deposits or wholesale funding or are you really tapped\",\n",
       " 'deposits or wholesale funding or are you really tapped out',\n",
       " 'or wholesale funding or are you really tapped out on',\n",
       " 'wholesale funding or are you really tapped out on lowering',\n",
       " 'funding or are you really tapped out on lowering your',\n",
       " 'or are you really tapped out on lowering your interest',\n",
       " 'are you really tapped out on lowering your interest expense',\n",
       " 'you really tapped out on lowering your interest expense potentially',\n",
       " 'really tapped out on lowering your interest expense potentially ?',\n",
       " 'well , maybe if i could sneak one more rate',\n",
       " ', maybe if i could sneak one more rate question',\n",
       " 'maybe if i could sneak one more rate question in',\n",
       " 'if i could sneak one more rate question in here',\n",
       " 'i could sneak one more rate question in here .',\n",
       " 'could sneak one more rate question in here . you',\n",
       " 'sneak one more rate question in here . you had',\n",
       " 'one more rate question in here . you had made',\n",
       " 'more rate question in here . you had made the',\n",
       " 'rate question in here . you had made the comment',\n",
       " 'question in here . you had made the comment in',\n",
       " 'in here . you had made the comment in your',\n",
       " 'here . you had made the comment in your prepped',\n",
       " '. you had made the comment in your prepped remarks',\n",
       " 'you had made the comment in your prepped remarks about',\n",
       " 'had made the comment in your prepped remarks about the',\n",
       " 'made the comment in your prepped remarks about the lower',\n",
       " 'the comment in your prepped remarks about the lower for',\n",
       " 'comment in your prepped remarks about the lower for longer',\n",
       " 'in your prepped remarks about the lower for longer risk',\n",
       " 'your prepped remarks about the lower for longer risk having',\n",
       " 'prepped remarks about the lower for longer risk having ramifications',\n",
       " 'remarks about the lower for longer risk having ramifications not',\n",
       " 'about the lower for longer risk having ramifications not just',\n",
       " 'the lower for longer risk having ramifications not just for',\n",
       " 'lower for longer risk having ramifications not just for this',\n",
       " 'for longer risk having ramifications not just for this year',\n",
       " 'longer risk having ramifications not just for this year but',\n",
       " 'risk having ramifications not just for this year but for',\n",
       " 'having ramifications not just for this year but for out',\n",
       " 'ramifications not just for this year but for out years',\n",
       " 'not just for this year but for out years as',\n",
       " 'just for this year but for out years as well',\n",
       " 'for this year but for out years as well .',\n",
       " 'this year but for out years as well . is',\n",
       " 'year but for out years as well . is your',\n",
       " 'but for out years as well . is your concern',\n",
       " 'for out years as well . is your concern more',\n",
       " 'out years as well . is your concern more not',\n",
       " 'years as well . is your concern more not just',\n",
       " 'as well . is your concern more not just about',\n",
       " 'well . is your concern more not just about say',\n",
       " '. is your concern more not just about say a',\n",
       " 'is your concern more not just about say a three-',\n",
       " 'your concern more not just about say a three- or',\n",
       " 'concern more not just about say a three- or six-month',\n",
       " 'more not just about say a three- or six-month delay',\n",
       " 'not just about say a three- or six-month delay and',\n",
       " 'just about say a three- or six-month delay and when',\n",
       " 'about say a three- or six-month delay and when the',\n",
       " 'say a three- or six-month delay and when the fed',\n",
       " 'a three- or six-month delay and when the fed moves',\n",
       " 'three- or six-month delay and when the fed moves but',\n",
       " 'or six-month delay and when the fed moves but about',\n",
       " 'six-month delay and when the fed moves but about how',\n",
       " 'delay and when the fed moves but about how much',\n",
       " 'and when the fed moves but about how much they',\n",
       " 'when the fed moves but about how much they eventually',\n",
       " 'the fed moves but about how much they eventually move',\n",
       " 'fed moves but about how much they eventually move ?',\n",
       " 'moves but about how much they eventually move ? is',\n",
       " 'but about how much they eventually move ? is that',\n",
       " 'about how much they eventually move ? is that the',\n",
       " 'how much they eventually move ? is that the change',\n",
       " 'much they eventually move ? is that the change ,',\n",
       " 'they eventually move ? is that the change , which',\n",
       " 'eventually move ? is that the change , which is',\n",
       " 'move ? is that the change , which is the',\n",
       " '? is that the change , which is the bigger',\n",
       " 'is that the change , which is the bigger ,',\n",
       " 'that the change , which is the bigger , the',\n",
       " 'the change , which is the bigger , the when',\n",
       " 'change , which is the bigger , the when or',\n",
       " ', which is the bigger , the when or how',\n",
       " 'which is the bigger , the when or how much',\n",
       " 'is the bigger , the when or how much in',\n",
       " 'the bigger , the when or how much in your',\n",
       " 'bigger , the when or how much in your mind',\n",
       " ', the when or how much in your mind ?',\n",
       " 'bill , i had a follow-up question on the comments',\n",
       " ', i had a follow-up question on the comments you',\n",
       " 'i had a follow-up question on the comments you just',\n",
       " 'had a follow-up question on the comments you just made',\n",
       " 'a follow-up question on the comments you just made about',\n",
       " 'follow-up question on the comments you just made about your',\n",
       " 'question on the comments you just made about your efforts',\n",
       " 'on the comments you just made about your efforts to',\n",
       " 'the comments you just made about your efforts to be',\n",
       " 'comments you just made about your efforts to be well-positioned',\n",
       " 'you just made about your efforts to be well-positioned post-qe',\n",
       " 'just made about your efforts to be well-positioned post-qe .',\n",
       " 'made about your efforts to be well-positioned post-qe . with',\n",
       " 'about your efforts to be well-positioned post-qe . with qe',\n",
       " 'your efforts to be well-positioned post-qe . with qe now',\n",
       " 'efforts to be well-positioned post-qe . with qe now over',\n",
       " \"to be well-positioned post-qe . with qe now over we've\",\n",
       " \"be well-positioned post-qe . with qe now over we've started\",\n",
       " \"well-positioned post-qe . with qe now over we've started to\",\n",
       " \"post-qe . with qe now over we've started to see\",\n",
       " \". with qe now over we've started to see loan\",\n",
       " \"with qe now over we've started to see loan growth\",\n",
       " \"qe now over we've started to see loan growth outpace\",\n",
       " \"now over we've started to see loan growth outpace deposit\",\n",
       " \"over we've started to see loan growth outpace deposit growth\",\n",
       " \"we've started to see loan growth outpace deposit growth across\",\n",
       " 'started to see loan growth outpace deposit growth across the',\n",
       " 'to see loan growth outpace deposit growth across the banking',\n",
       " 'see loan growth outpace deposit growth across the banking system',\n",
       " 'loan growth outpace deposit growth across the banking system as',\n",
       " 'growth outpace deposit growth across the banking system as a',\n",
       " 'outpace deposit growth across the banking system as a whole',\n",
       " 'deposit growth across the banking system as a whole but',\n",
       " \"growth across the banking system as a whole but it's\",\n",
       " \"across the banking system as a whole but it's interesting\",\n",
       " \"the banking system as a whole but it's interesting that\",\n",
       " \"banking system as a whole but it's interesting that that\",\n",
       " \"system as a whole but it's interesting that that doesn't\",\n",
       " \"as a whole but it's interesting that that doesn't appear\",\n",
       " \"a whole but it's interesting that that doesn't appear to\",\n",
       " \"whole but it's interesting that that doesn't appear to be\",\n",
       " \"but it's interesting that that doesn't appear to be the\",\n",
       " \"it's interesting that that doesn't appear to be the case\",\n",
       " \"interesting that that doesn't appear to be the case for\",\n",
       " \"that that doesn't appear to be the case for the\",\n",
       " \"that doesn't appear to be the case for the big\",\n",
       " \"doesn't appear to be the case for the big banks\",\n",
       " 'appear to be the case for the big banks where',\n",
       " 'to be the case for the big banks where deposit',\n",
       " 'be the case for the big banks where deposit growth',\n",
       " 'the case for the big banks where deposit growth is',\n",
       " 'case for the big banks where deposit growth is still',\n",
       " 'for the big banks where deposit growth is still outpacing',\n",
       " 'the big banks where deposit growth is still outpacing loan',\n",
       " 'big banks where deposit growth is still outpacing loan growth',\n",
       " 'banks where deposit growth is still outpacing loan growth according',\n",
       " 'where deposit growth is still outpacing loan growth according to',\n",
       " 'deposit growth is still outpacing loan growth according to the',\n",
       " 'growth is still outpacing loan growth according to the h8',\n",
       " 'is still outpacing loan growth according to the h8 data',\n",
       " 'still outpacing loan growth according to the h8 data .',\n",
       " 'outpacing loan growth according to the h8 data . and',\n",
       " 'loan growth according to the h8 data . and i',\n",
       " 'growth according to the h8 data . and i guess',\n",
       " 'according to the h8 data . and i guess as',\n",
       " 'to the h8 data . and i guess as far',\n",
       " 'the h8 data . and i guess as far as',\n",
       " 'h8 data . and i guess as far as pnc',\n",
       " 'data . and i guess as far as pnc goes',\n",
       " '. and i guess as far as pnc goes specifically',\n",
       " 'and i guess as far as pnc goes specifically over',\n",
       " 'i guess as far as pnc goes specifically over the',\n",
       " 'guess as far as pnc goes specifically over the last',\n",
       " 'as far as pnc goes specifically over the last couple',\n",
       " 'far as pnc goes specifically over the last couple of',\n",
       " 'as pnc goes specifically over the last couple of years',\n",
       " \"pnc goes specifically over the last couple of years we've\",\n",
       " \"goes specifically over the last couple of years we've seen\",\n",
       " \"specifically over the last couple of years we've seen some\",\n",
       " \"over the last couple of years we've seen some quarters\",\n",
       " \"the last couple of years we've seen some quarters where\",\n",
       " \"last couple of years we've seen some quarters where your\",\n",
       " \"couple of years we've seen some quarters where your loan\",\n",
       " \"of years we've seen some quarters where your loan growth\",\n",
       " \"years we've seen some quarters where your loan growth has\",\n",
       " \"we've seen some quarters where your loan growth has outpaced\",\n",
       " 'seen some quarters where your loan growth has outpaced your',\n",
       " 'some quarters where your loan growth has outpaced your deposit',\n",
       " 'quarters where your loan growth has outpaced your deposit growth',\n",
       " 'where your loan growth has outpaced your deposit growth and',\n",
       " 'your loan growth has outpaced your deposit growth and others',\n",
       " 'loan growth has outpaced your deposit growth and others where',\n",
       " \"growth has outpaced your deposit growth and others where it's\",\n",
       " \"has outpaced your deposit growth and others where it's been\",\n",
       " \"outpaced your deposit growth and others where it's been the\",\n",
       " \"your deposit growth and others where it's been the reverse\",\n",
       " \"deposit growth and others where it's been the reverse .\",\n",
       " \"growth and others where it's been the reverse . can\",\n",
       " \"and others where it's been the reverse . can you\",\n",
       " \"others where it's been the reverse . can you give\",\n",
       " \"where it's been the reverse . can you give us\",\n",
       " \"it's been the reverse . can you give us a\",\n",
       " 'been the reverse . can you give us a little',\n",
       " 'the reverse . can you give us a little bit',\n",
       " 'reverse . can you give us a little bit of',\n",
       " '. can you give us a little bit of just',\n",
       " 'can you give us a little bit of just speak',\n",
       " 'you give us a little bit of just speak to',\n",
       " \"give us a little bit of just speak to what's\",\n",
       " \"us a little bit of just speak to what's been\",\n",
       " \"a little bit of just speak to what's been driving\",\n",
       " \"little bit of just speak to what's been driving these\",\n",
       " \"bit of just speak to what's been driving these trends\",\n",
       " \"of just speak to what's been driving these trends and\",\n",
       " \"just speak to what's been driving these trends and i\",\n",
       " \"speak to what's been driving these trends and i guess\",\n",
       " \"to what's been driving these trends and i guess what\",\n",
       " \"what's been driving these trends and i guess what you\",\n",
       " 'been driving these trends and i guess what you guys',\n",
       " 'driving these trends and i guess what you guys expect',\n",
       " 'these trends and i guess what you guys expect going',\n",
       " 'trends and i guess what you guys expect going forward',\n",
       " 'and i guess what you guys expect going forward ?',\n",
       " 'bill just on your points about on credit expansion ,',\n",
       " 'just on your points about on credit expansion , this',\n",
       " 'on your points about on credit expansion , this quarter',\n",
       " 'your points about on credit expansion , this quarter you',\n",
       " 'points about on credit expansion , this quarter you had',\n",
       " 'about on credit expansion , this quarter you had point',\n",
       " 'on credit expansion , this quarter you had point balances',\n",
       " 'credit expansion , this quarter you had point balances below',\n",
       " 'expansion , this quarter you had point balances below the',\n",
       " ', this quarter you had point balances below the averages',\n",
       " 'this quarter you had point balances below the averages and',\n",
       " 'quarter you had point balances below the averages and some',\n",
       " 'you had point balances below the averages and some of',\n",
       " \"had point balances below the averages and some of it's\",\n",
       " \"point balances below the averages and some of it's the\",\n",
       " \"balances below the averages and some of it's the non-strategic\",\n",
       " \"below the averages and some of it's the non-strategic runoff\",\n",
       " ...]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentence2index_dataset(data):\n",
    "    X=[]\n",
    "    y=[]\n",
    "    for sentence in data:\n",
    "        list_ = []\n",
    "        for token in tokenize(sentence):\n",
    "            if token in token2id.keys():\n",
    "                list_.append(token2id[token])\n",
    "        X.append(list_[:(SEQUENCE_LENGTH-1)])\n",
    "        y.append(list_[-1])        \n",
    "    y = array(y)\n",
    "    X = array(X)\n",
    "    y = to_categorical(y, num_classes=len(token2id))\n",
    "        \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_input, train_label = sentence2index_dataset(train_d)\n",
    "val_input, val_label =  sentence2index_dataset(val_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71295 71295 5838\n",
      "21263 21263 5838\n"
     ]
    }
   ],
   "source": [
    "print(len(train_input), len(train_label), len(train_label[0]))\n",
    "print(len(val_input), len(val_label), len(val_label[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1535,   74,    3,   14,   25,    5,  184,  504,   17])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_input[50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# len(build_sequence(20, tokenize(raw[0][0].lower()))[0].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def data_loader(data_input):\n",
    "#     tokenizer.fit_on_texts(all_tokens) #fit on texts\n",
    "#     sequences = tokenizer.texts_to_sequences(data_input)\n",
    "#     vocab_size = len(token2id)\n",
    "#     X = []\n",
    "#     y = []\n",
    "#     for i in range(len(sequences)):\n",
    "#         X.append((sequences[i][:(SEQUENCE_LENGTH-1)]))\n",
    "#         y.append((sequences[i][-1]))\n",
    "\n",
    "#     y = array(y)\n",
    "#     X = array(X)\n",
    "#     y = to_categorical(y, num_classes=vocab_size)\n",
    "#     return X, y, vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_file(lines, filename):\n",
    "    data = '\\n'.join(lines)\n",
    "    file = open(filename, 'w')\n",
    "    file.write(data)\n",
    "    file.close()\n",
    "\n",
    "out_filename = 'Balance sheet_new.txt'\n",
    "save_file(all_d, out_filename)\n",
    "# load doc into memory\n",
    "def load_file(filename):\n",
    "    file = open(filename, 'r')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text\n",
    " \n",
    "in_filename = 'Balance sheet_new.txt'\n",
    "doc = load_file(in_filename)\n",
    "lines = doc.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from random import randint\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    " \n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    file = open(filename, 'r')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text\n",
    " \n",
    "# generate a sequence from a language model\n",
    "def generate_seq(model, seq_length, seed_text, n_words):\n",
    "    result = list()\n",
    "    text = seed_text\n",
    "    for _ in range(n_words):\n",
    "    # encode the text as integer\n",
    "        encod = []\n",
    "        for token in tokenize(text):\n",
    "            if token in token2id.keys():\n",
    "                encod.append(token2id[token])\n",
    "#         encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        # truncate sequences to a fixed length\n",
    "        encod = pad_sequences([encod], maxlen=seq_length, truncating='pre')\n",
    "        # predict probabilities for each word\n",
    "        y = model.predict_classes(encod, verbose=0)\n",
    "        # map predicted word index to word\n",
    "        output = ''\n",
    "        for word, index in token2id.items():\n",
    "            if index == y:\n",
    "                output = word\n",
    "                break\n",
    "    # append to input\n",
    "        in_text += ' ' + output\n",
    "        result.append(output)\n",
    "        if output =='?':\n",
    "            break\n",
    "    return ' '.join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "71295/71295 [==============================] - 98s 1ms/step - loss: 6.2196 - acc: 0.0486\n",
      "\n",
      "Epoch 00001: loss improved from inf to 6.21960, saving model to LSTM_new1.hdf5\n",
      "Epoch 2/100\n",
      "71295/71295 [==============================] - 90s 1ms/step - loss: 5.6898 - acc: 0.0820\n",
      "\n",
      "Epoch 00002: loss improved from 6.21960 to 5.68980, saving model to LSTM_new1.hdf5\n",
      "Epoch 3/100\n",
      "71295/71295 [==============================] - 90s 1ms/step - loss: 5.2643 - acc: 0.1245\n",
      "\n",
      "Epoch 00003: loss improved from 5.68980 to 5.26427, saving model to LSTM_new1.hdf5\n",
      "Epoch 4/100\n",
      "71295/71295 [==============================] - 91s 1ms/step - loss: 4.9663 - acc: 0.1493\n",
      "\n",
      "Epoch 00004: loss improved from 5.26427 to 4.96631, saving model to LSTM_new1.hdf5\n",
      "Epoch 5/100\n",
      "71295/71295 [==============================] - 91s 1ms/step - loss: 4.7261 - acc: 0.1731\n",
      "\n",
      "Epoch 00005: loss improved from 4.96631 to 4.72608, saving model to LSTM_new1.hdf5\n",
      "Epoch 6/100\n",
      "71295/71295 [==============================] - 92s 1ms/step - loss: 4.5350 - acc: 0.1910\n",
      "\n",
      "Epoch 00006: loss improved from 4.72608 to 4.53498, saving model to LSTM_new1.hdf5\n",
      "Epoch 7/100\n",
      "71295/71295 [==============================] - 92s 1ms/step - loss: 4.3750 - acc: 0.2024\n",
      "\n",
      "Epoch 00007: loss improved from 4.53498 to 4.37499, saving model to LSTM_new1.hdf5\n",
      "Epoch 8/100\n",
      "71295/71295 [==============================] - 91s 1ms/step - loss: 4.2331 - acc: 0.2148\n",
      "\n",
      "Epoch 00008: loss improved from 4.37499 to 4.23310, saving model to LSTM_new1.hdf5\n",
      "Epoch 9/100\n",
      "71295/71295 [==============================] - 92s 1ms/step - loss: 4.0967 - acc: 0.2237\n",
      "\n",
      "Epoch 00009: loss improved from 4.23310 to 4.09666, saving model to LSTM_new1.hdf5\n",
      "Epoch 10/100\n",
      "71295/71295 [==============================] - 91s 1ms/step - loss: 3.9679 - acc: 0.2329\n",
      "\n",
      "Epoch 00010: loss improved from 4.09666 to 3.96786, saving model to LSTM_new1.hdf5\n",
      "Epoch 11/100\n",
      "71295/71295 [==============================] - 92s 1ms/step - loss: 3.8450 - acc: 0.2416\n",
      "\n",
      "Epoch 00011: loss improved from 3.96786 to 3.84502, saving model to LSTM_new1.hdf5\n",
      "Epoch 12/100\n",
      "71295/71295 [==============================] - 91s 1ms/step - loss: 3.7220 - acc: 0.2526\n",
      "\n",
      "Epoch 00012: loss improved from 3.84502 to 3.72203, saving model to LSTM_new1.hdf5\n",
      "Epoch 13/100\n",
      "71295/71295 [==============================] - 91s 1ms/step - loss: 3.5983 - acc: 0.2634\n",
      "\n",
      "Epoch 00013: loss improved from 3.72203 to 3.59830, saving model to LSTM_new1.hdf5\n",
      "Epoch 14/100\n",
      "71295/71295 [==============================] - 91s 1ms/step - loss: 3.4736 - acc: 0.2752\n",
      "\n",
      "Epoch 00014: loss improved from 3.59830 to 3.47359, saving model to LSTM_new1.hdf5\n",
      "Epoch 15/100\n",
      "71295/71295 [==============================] - 91s 1ms/step - loss: 3.3479 - acc: 0.2877\n",
      "\n",
      "Epoch 00015: loss improved from 3.47359 to 3.34794, saving model to LSTM_new1.hdf5\n",
      "Epoch 16/100\n",
      "71295/71295 [==============================] - 91s 1ms/step - loss: 3.2182 - acc: 0.3026\n",
      "\n",
      "Epoch 00016: loss improved from 3.34794 to 3.21816, saving model to LSTM_new1.hdf5\n",
      "Epoch 17/100\n",
      "71295/71295 [==============================] - 93s 1ms/step - loss: 3.0877 - acc: 0.3192\n",
      "\n",
      "Epoch 00017: loss improved from 3.21816 to 3.08769, saving model to LSTM_new1.hdf5\n",
      "Epoch 18/100\n",
      "71295/71295 [==============================] - 93s 1ms/step - loss: 2.9551 - acc: 0.3386\n",
      "\n",
      "Epoch 00018: loss improved from 3.08769 to 2.95511, saving model to LSTM_new1.hdf5\n",
      "Epoch 19/100\n",
      "71295/71295 [==============================] - 92s 1ms/step - loss: 2.8250 - acc: 0.3583\n",
      "\n",
      "Epoch 00019: loss improved from 2.95511 to 2.82499, saving model to LSTM_new1.hdf5\n",
      "Epoch 20/100\n",
      "71295/71295 [==============================] - 87s 1ms/step - loss: 2.6934 - acc: 0.3782\n",
      "\n",
      "Epoch 00020: loss improved from 2.82499 to 2.69342, saving model to LSTM_new1.hdf5\n",
      "Epoch 21/100\n",
      "71295/71295 [==============================] - 89s 1ms/step - loss: 2.5613 - acc: 0.4024\n",
      "\n",
      "Epoch 00021: loss improved from 2.69342 to 2.56129, saving model to LSTM_new1.hdf5\n",
      "Epoch 22/100\n",
      "71295/71295 [==============================] - 88s 1ms/step - loss: 2.4293 - acc: 0.4255\n",
      "\n",
      "Epoch 00022: loss improved from 2.56129 to 2.42927, saving model to LSTM_new1.hdf5\n",
      "Epoch 23/100\n",
      "71295/71295 [==============================] - 89s 1ms/step - loss: 2.3029 - acc: 0.4476\n",
      "\n",
      "Epoch 00023: loss improved from 2.42927 to 2.30293, saving model to LSTM_new1.hdf5\n",
      "Epoch 24/100\n",
      "71295/71295 [==============================] - 7544s 106ms/step - loss: 2.1735 - acc: 0.4737\n",
      "\n",
      "Epoch 00024: loss improved from 2.30293 to 2.17349, saving model to LSTM_new1.hdf5\n",
      "Epoch 25/100\n",
      "71295/71295 [==============================] - 89s 1ms/step - loss: 2.0472 - acc: 0.4979\n",
      "\n",
      "Epoch 00025: loss improved from 2.17349 to 2.04723, saving model to LSTM_new1.hdf5\n",
      "Epoch 26/100\n",
      "71295/71295 [==============================] - 89s 1ms/step - loss: 1.9276 - acc: 0.5233\n",
      "\n",
      "Epoch 00026: loss improved from 2.04723 to 1.92760, saving model to LSTM_new1.hdf5\n",
      "Epoch 27/100\n",
      "71295/71295 [==============================] - 89s 1ms/step - loss: 1.8026 - acc: 0.5495\n",
      "\n",
      "Epoch 00027: loss improved from 1.92760 to 1.80257, saving model to LSTM_new1.hdf5\n",
      "Epoch 28/100\n",
      "71295/71295 [==============================] - 90s 1ms/step - loss: 1.6821 - acc: 0.5763\n",
      "\n",
      "Epoch 00028: loss improved from 1.80257 to 1.68212, saving model to LSTM_new1.hdf5\n",
      "Epoch 29/100\n",
      "71295/71295 [==============================] - 90s 1ms/step - loss: 1.5680 - acc: 0.6032\n",
      "\n",
      "Epoch 00029: loss improved from 1.68212 to 1.56799, saving model to LSTM_new1.hdf5\n",
      "Epoch 30/100\n",
      "71295/71295 [==============================] - 91s 1ms/step - loss: 1.4573 - acc: 0.6320\n",
      "\n",
      "Epoch 00030: loss improved from 1.56799 to 1.45734, saving model to LSTM_new1.hdf5\n",
      "Epoch 31/100\n",
      "71295/71295 [==============================] - 91s 1ms/step - loss: 1.3499 - acc: 0.6562\n",
      "\n",
      "Epoch 00031: loss improved from 1.45734 to 1.34991, saving model to LSTM_new1.hdf5\n",
      "Epoch 32/100\n",
      "71295/71295 [==============================] - 92s 1ms/step - loss: 1.2395 - acc: 0.6833\n",
      "\n",
      "Epoch 00032: loss improved from 1.34991 to 1.23953, saving model to LSTM_new1.hdf5\n",
      "Epoch 33/100\n",
      "71295/71295 [==============================] - 93s 1ms/step - loss: 1.1429 - acc: 0.7091\n",
      "\n",
      "Epoch 00033: loss improved from 1.23953 to 1.14294, saving model to LSTM_new1.hdf5\n",
      "Epoch 34/100\n",
      "71295/71295 [==============================] - 94s 1ms/step - loss: 1.0533 - acc: 0.7309\n",
      "\n",
      "Epoch 00034: loss improved from 1.14294 to 1.05331, saving model to LSTM_new1.hdf5\n",
      "Epoch 35/100\n",
      "71295/71295 [==============================] - 94s 1ms/step - loss: 0.9608 - acc: 0.7548\n",
      "\n",
      "Epoch 00035: loss improved from 1.05331 to 0.96082, saving model to LSTM_new1.hdf5\n",
      "Epoch 36/100\n",
      "71295/71295 [==============================] - 95s 1ms/step - loss: 0.8763 - acc: 0.7768\n",
      "\n",
      "Epoch 00036: loss improved from 0.96082 to 0.87634, saving model to LSTM_new1.hdf5\n",
      "Epoch 37/100\n",
      "71295/71295 [==============================] - 95s 1ms/step - loss: 0.7921 - acc: 0.8002\n",
      "\n",
      "Epoch 00037: loss improved from 0.87634 to 0.79205, saving model to LSTM_new1.hdf5\n",
      "Epoch 38/100\n",
      "71295/71295 [==============================] - 95s 1ms/step - loss: 0.7196 - acc: 0.8191\n",
      "\n",
      "Epoch 00038: loss improved from 0.79205 to 0.71959, saving model to LSTM_new1.hdf5\n",
      "Epoch 39/100\n",
      "71295/71295 [==============================] - 96s 1ms/step - loss: 0.6495 - acc: 0.8375\n",
      "\n",
      "Epoch 00039: loss improved from 0.71959 to 0.64947, saving model to LSTM_new1.hdf5\n",
      "Epoch 40/100\n",
      "71295/71295 [==============================] - 96s 1ms/step - loss: 0.5906 - acc: 0.8527\n",
      "\n",
      "Epoch 00040: loss improved from 0.64947 to 0.59057, saving model to LSTM_new1.hdf5\n",
      "Epoch 41/100\n",
      "71295/71295 [==============================] - 97s 1ms/step - loss: 0.5364 - acc: 0.8672\n",
      "\n",
      "Epoch 00041: loss improved from 0.59057 to 0.53641, saving model to LSTM_new1.hdf5\n",
      "Epoch 42/100\n",
      "71295/71295 [==============================] - 98s 1ms/step - loss: 0.4778 - acc: 0.8833\n",
      "\n",
      "Epoch 00042: loss improved from 0.53641 to 0.47782, saving model to LSTM_new1.hdf5\n",
      "Epoch 43/100\n",
      "71295/71295 [==============================] - 99s 1ms/step - loss: 0.4358 - acc: 0.8945\n",
      "\n",
      "Epoch 00043: loss improved from 0.47782 to 0.43581, saving model to LSTM_new1.hdf5\n",
      "Epoch 44/100\n",
      "71295/71295 [==============================] - 100s 1ms/step - loss: 0.3891 - acc: 0.9069\n",
      "\n",
      "Epoch 00044: loss improved from 0.43581 to 0.38913, saving model to LSTM_new1.hdf5\n",
      "Epoch 45/100\n",
      "71295/71295 [==============================] - 100s 1ms/step - loss: 0.3604 - acc: 0.9133\n",
      "\n",
      "Epoch 00045: loss improved from 0.38913 to 0.36036, saving model to LSTM_new1.hdf5\n",
      "Epoch 46/100\n",
      "71295/71295 [==============================] - 101s 1ms/step - loss: 0.3229 - acc: 0.9239\n",
      "\n",
      "Epoch 00046: loss improved from 0.36036 to 0.32286, saving model to LSTM_new1.hdf5\n",
      "Epoch 47/100\n",
      "71295/71295 [==============================] - 102s 1ms/step - loss: 0.2885 - acc: 0.9346\n",
      "\n",
      "Epoch 00047: loss improved from 0.32286 to 0.28851, saving model to LSTM_new1.hdf5\n",
      "Epoch 48/100\n",
      "71295/71295 [==============================] - 102s 1ms/step - loss: 0.2747 - acc: 0.9384\n",
      "\n",
      "Epoch 00048: loss improved from 0.28851 to 0.27465, saving model to LSTM_new1.hdf5\n",
      "Epoch 49/100\n",
      "71295/71295 [==============================] - 103s 1ms/step - loss: 0.2525 - acc: 0.9423\n",
      "\n",
      "Epoch 00049: loss improved from 0.27465 to 0.25250, saving model to LSTM_new1.hdf5\n",
      "Epoch 50/100\n",
      "71295/71295 [==============================] - 103s 1ms/step - loss: 0.2308 - acc: 0.9471\n",
      "\n",
      "Epoch 00050: loss improved from 0.25250 to 0.23084, saving model to LSTM_new1.hdf5\n",
      "Epoch 51/100\n",
      "22400/71295 [========>.....................] - ETA: 1:13 - loss: 0.1927 - acc: 0.9580"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-6f411c3fbb33>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m# fit the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m# model.fit(train_input, train_label, validation_data =(val_input, val_label), batch_size= 200, epochs=100, callbacks=callbacks_list)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;31m# categorical_crossentropy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlps/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/envs/nlps/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlps/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlps/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlps/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1397\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1398\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1399\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1400\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab, 200, input_length= SEQUENCE_LENGTH-1))\n",
    "model.add(LSTM(200, return_sequences=True))\n",
    "model.add(LSTM(200))\n",
    "model.add(Dense(200, activation='relu'))\n",
    "model.add(Dense(vocab, activation='softmax'))\n",
    "\n",
    "#import the checkpoint to save current model\n",
    "filepath=\"LSTM_new1.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "# earlystopper = EarlyStopping(monitor='val_loss', patience=3, verbose=1)\n",
    "# callbacks_list = [checkpoint, earlystopper]\n",
    "callbacks_list = [checkpoint]\n",
    "# compile model\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit the model\n",
    "# model.fit(train_input, train_label, validation_data =(val_input, val_label), batch_size= 200, epochs=100, callbacks=callbacks_list)\n",
    "model.fit(train_input, train_label, batch_size= 200, epochs=100, callbacks=callbacks_list)\n",
    "\n",
    "# save the model to filey\n",
    "model.save('model_new1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save the model to filey\n",
    "model.save('model_new1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# filepath=\"LSTM_new1.hdf5\"\n",
    "# checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "# earlystopper = EarlyStopping(monitor='val_loss', patience=1, verbose=1)\n",
    "# callbacks_list = [checkpoint, earlystopper]\n",
    "\n",
    "# model.load_weights(filepath)\n",
    "# model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# model.fit(train_input, train_label, validation_data =(val_input, val_label), batch_size= 200, epochs=100, callbacks=callbacks_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plk.dump(tokenizer, open('tokenizer_new.pkl', 'wb'))\n",
    "model.save('model_new1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "might be a one year additional grant period if needed but just remind us of how much is private that\n",
      "\n",
      "you re putting on for the fed down how are you thinking about the industry are you seeing any signs of the industry will impact as you think that s a little bit more than construction and just isolating out that the optimal mix growth according to the balance sheet\n"
     ]
    }
   ],
   "source": [
    "#test 1\n",
    "# load cleaned text sequences\n",
    "in_filename = 'Balance sheet.txt'\n",
    "doc = load_doc(in_filename)\n",
    "lines = doc.split('\\n')\n",
    "seq_length = len(lines[0].split()) - 1\n",
    " \n",
    "# load the model\n",
    "model = load_model('model_new1.h5')\n",
    " \n",
    "# load the tokenizer\n",
    "tokenizer = plk.load(open('tokenizer_new.pkl', 'rb'))\n",
    " \n",
    "# select a seed text\n",
    "seed_text = lines[randint(0,len(lines))]\n",
    "print(seed_text + '\\n')\n",
    " \n",
    "# generate new text\n",
    "generated = generate_seq(model, tokenizer, seq_length, seed_text, 50)\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what percentage was attributed to lower purchase accounting accretion ?\n",
      "\n",
      "auto , it sounds like is for the current proposals . when you look at it outstanding and net interest income is growing and itâ??s looking for a long rate environment , still improved mortgage growth having been about low growth outside the years or how are you thinking that\n"
     ]
    }
   ],
   "source": [
    "model = load_model('model_new1.h5')\n",
    "seq_length = 9\n",
    "# select a seed text\n",
    "seed_text = lines[randint(0,len(lines))]\n",
    "print(seed_text + '\\n')\n",
    " \n",
    "# generate new text\n",
    "generated = generate_seq(model, seq_length, seed_text, 50)\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 9, 200)            1167600   \n",
      "_________________________________________________________________\n",
      "lstm_9 (LSTM)                (None, 9, 200)            320800    \n",
      "_________________________________________________________________\n",
      "lstm_10 (LSTM)               (None, 200)               320800    \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 200)               40200     \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 5838)              1173438   \n",
      "=================================================================\n",
      "Total params: 3,022,838\n",
      "Trainable params: 3,022,838\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reduce your asset yields overall , donâ??t the trade-off on rate sensitivity from here ? i mean 22% points their loans have growing nicely . it’s junk type . or has that impacted your deposit penetration that you have in terms of c&i , we get a subordinated debt securities\n"
     ]
    }
   ],
   "source": [
    "seed_text = 'can you'\n",
    "# generate new text\n",
    "generated = generate_seq(model, seq_length, seed_text, 50)\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "that will accelerate , if you say the balances are market loans and maybe great risk pressure but we’ve got somewhat between 25bps for growth , while spreads was up this quarter ? what would the fed changes that you had to resort to that ? and how we are growing most of the wholesale funding rules that's by the loan-to-value ? and then maybe a question on that is if we get additional rate hikes ? or are there still a december sign return to your customers , that securities loaned or liz and some believe that all could\n"
     ]
    }
   ],
   "source": [
    "# seed_text = lines[randint(0,len(lines))]\n",
    "# print(seed_text + '\\n')\n",
    "seed_text = 'what do you think' \n",
    "# generate new text\n",
    "generated = generate_seq(model, seq_length, seed_text, 100)\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "that will accelerate , if you say the balances are market loans and maybe great risk pressure but we’ve got somewhat between 25bps for growth , while spreads was up this quarter ?\n"
     ]
    }
   ],
   "source": [
    "# seed_text = lines[randint(0,len(lines))]\n",
    "# print(seed_text + '\\n')\n",
    "seed_text = 'what do you think' \n",
    "# generate new text\n",
    "generated = generate_seq(model, seq_length, seed_text, 100)\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and the net interest margin dollars in the back half of the year ?\n"
     ]
    }
   ],
   "source": [
    "# print(seed_text + '\\n')\n",
    "seed_text = 'balance sheet' \n",
    "# generate new text\n",
    "generated = generate_seq(model, seq_length, seed_text, 100)\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print(seed_text + '\\n')\n",
    "seed_text = 'balance sheet' \n",
    "# generate new text\n",
    "generated = generate_seq(model, seq_length, seed_text, 100)\n",
    "print(generated)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
