{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import random\n",
    "random.seed(1006)\n",
    "\n",
    "from itertools import cycle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import interp\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge, BayesianRidge, Lasso\n",
    "from sklearn.metrics import make_scorer, mean_squared_error\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('dark_background')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set for your computer\n",
    "data_directory = '/'.join(os.getcwd().split(\"/\")[:-1]) + '/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = [173,  74,  20, 101,  83,   1,  38,  39,  72,  50,  21, 164,  57,\n",
    "       169,   8,  63, 102,  34,  80, 192, 139,  88, 112, 116,  61,  46,\n",
    "        51, 165, 135,  89, 108,   7,  25,  15, 125,  93, 130,  71]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in data\n",
    "orig_data = pd.read_csv(data_directory+'qaData.csv', parse_dates=['Date'])\n",
    "orig_data['EarningTag2'] = orig_data['EarningTag2'].str.strip()\n",
    "\n",
    "#Add Year and Month, Quarter from Data\n",
    "orig_data['Year'] = orig_data['Date'].dt.year\n",
    "orig_data['Month'] = orig_data['Date'].dt.month\n",
    "orig_data['Quarter'] = orig_data['Month'].apply(lambda x: 1 if x < 4 else 2 if x < 7 else 3 if x < 9 else 4)\n",
    "orig_data['Company'] = orig_data['Company'].str.title().str.replace(\" \", \"\")\n",
    "orig_data['EventType'] = orig_data['EventType'].str.title().str.replace(\" \", \"\")\n",
    "orig_data['Participants'] = orig_data['Participants'].str.title().str.replace(\" \", \"\")\n",
    "orig_data['AnalystName'] = orig_data['AnalystName'].str.title().str.replace(\" \", \"\")\n",
    "orig_data['AnalystCompany'] = orig_data['AnalystCompany'].str.title().str.replace(\" \", \"\")\n",
    "orig_data['EarningTag2'] = orig_data['EarningTag2'].str.title().str.replace(\" \", \"\")\n",
    "\n",
    "#Pivot tag\n",
    "pivot_data = (pd.pivot_table(orig_data, index=['Company', 'Participants', 'Month', 'Year', 'Quarter', 'EventType'], columns='EarningTag2', aggfunc='size', fill_value=0)).reset_index()\n",
    "\n",
    "#Melt data\n",
    "pivot_melt_data = pd.melt(pivot_data, id_vars=['Company', 'Participants', 'Month', 'Year', 'Quarter', 'EventType'], var_name='Tag', value_name='NumQ')\n",
    "#One-hot encode\n",
    "pivot_melt_data = pd.concat([pivot_melt_data, \n",
    "                             pd.get_dummies(pivot_melt_data['Company'], prefix='C', prefix_sep=\"_\"),\n",
    "                             pd.get_dummies(pivot_melt_data['Participants'], prefix='P', prefix_sep=\"_\"),\n",
    "                             pd.get_dummies(pivot_melt_data['EventType'], prefix='ET', prefix_sep=\"_\"),\n",
    "                             pd.get_dummies(pivot_melt_data['Tag'], prefix='T', prefix_sep=\"_\")], axis=1)\n",
    "pivot_melt_data = pivot_melt_data.reset_index(drop=True)\n",
    "\n",
    "#Analysts Present Data\n",
    "event_analyst_data = orig_data[['Company', 'Participants', 'AnalystName', 'AnalystCompany', 'Month', 'Year', 'Quarter', 'EventType']].drop_duplicates().reset_index(drop=True)\n",
    "event_analyst_data = pd.concat([event_analyst_data, \n",
    "                                pd.get_dummies(event_analyst_data['AnalystName'], prefix='AP', prefix_sep=\"_\"),\n",
    "                                pd.get_dummies(event_analyst_data['AnalystCompany'], prefix='ACP', prefix_sep=\"_\")], axis=1).drop(['AnalystName', 'AnalystCompany'], axis=1)\n",
    "event_analyst_data = event_analyst_data.groupby(['Company', 'Participants', 'Year', 'Month', 'Quarter', 'EventType']).sum().reset_index()\n",
    "\n",
    "all_features_data = pd.merge(pivot_melt_data, event_analyst_data, on=['Company', 'Participants', 'Month', 'Year', 'Quarter', 'EventType'])\n",
    "\n",
    "#Index Data\n",
    "groups = []\n",
    "for i, (name, group) in enumerate(all_features_data.groupby(['Company', 'Participants', 'Month', 'Year', 'Quarter', 'EventType'])):\n",
    "    g2 = group.copy()\n",
    "    g2['EventNumber'] = i\n",
    "    groups.append(g2)\n",
    "    \n",
    "indexed_data = pd.concat(groups)\n",
    "\n",
    "#Merge\n",
    "indexed_data = indexed_data.drop(['Company', 'Participants', 'Tag', 'EventType'], axis=1)\n",
    "indexed_data = indexed_data.reset_index(drop=True)\n",
    "\n",
    "train, test = indexed_data.loc[~indexed_data['EventNumber'].isin(test_set)].copy().reset_index(drop=True), \\\n",
    "                indexed_data.loc[indexed_data['EventNumber'].isin(test_set)].copy().reset_index(drop=True)\n",
    "\n",
    "X_train, y_train = train.drop(['NumQ','EventNumber'], axis=1), train['NumQ'].values\n",
    "X_test, y_test = test.drop(['NumQ', 'EventNumber'], axis=1), test['NumQ'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = np.zeros(50)\n",
    "scores_ridge = np.zeros(50)\n",
    "scores_lasso = np.zeros(50)\n",
    "scores_br = np.zeros(50)\n",
    "scores_gbc = np.zeros(50)\n",
    "scores_rf = np.zeros(50)\n",
    "\n",
    "estimator = LinearRegression().fit(X_train, y_train)\n",
    "scores[0] = mean_squared_error(y_test, estimator.predict(X_test).round())\n",
    "    \n",
    "estimator_ridge = Ridge().fit(X_train, y_train)\n",
    "scores_ridge[0] = mean_squared_error(y_test, estimator_ridge.predict(X_test).round())\n",
    "    \n",
    "estimator_lasso = Lasso().fit(X_train, y_train)\n",
    "scores_lasso[0] = mean_squared_error(y_test, estimator_lasso.predict(X_test).round())    \n",
    "    \n",
    "estimator_br = BayesianRidge().fit(X_train, y_train)\n",
    "scores_br[0] = mean_squared_error(y_test, estimator_br.predict(X_test).round())\n",
    "    \n",
    "estimator_gbc = GradientBoostingRegressor(warm_start=True).fit(X_train, y_train)\n",
    "scores_gbc[0] = mean_squared_error(y_test, estimator_gbc.predict(X_test).round())\n",
    "    \n",
    "estimator_rf = RandomForestRegressor(warm_start=True).fit(X_train, y_train)\n",
    "scores_rf[0] = mean_squared_error(y_test, estimator_rf.predict(X_test).round())\n",
    "\n",
    "for comp in range(1, 50):\n",
    "    model = NMF(n_components=comp)\n",
    "    X_train_W = model.fit_transform(X_train)\n",
    "    X_test_W = model.transform(X_test)\n",
    "    \n",
    "    estimator = LinearRegression().fit(X_train_W, y_train)\n",
    "    scores[comp] = mean_squared_error(y_test, estimator.predict(X_test_W).round())\n",
    "    \n",
    "    estimator_ridge = Ridge().fit(X_train_W, y_train)\n",
    "    scores_ridge[comp] = mean_squared_error(y_test, estimator_ridge.predict(X_test_W).round())\n",
    "    \n",
    "    estimator_lasso = Lasso().fit(X_train_W, y_train)\n",
    "    scores_lasso[comp] = mean_squared_error(y_test, estimator_lasso.predict(X_test_W).round())\n",
    "    \n",
    "    estimator_br = BayesianRidge().fit(X_train_W, y_train)\n",
    "    scores_br[comp] = mean_squared_error(y_test, estimator_br.predict(X_test_W).round())\n",
    "    \n",
    "    estimator_gbc = GradientBoostingRegressor(warm_start=True).fit(X_train_W, y_train)\n",
    "    scores_gbc[comp] = mean_squared_error(y_test, estimator_gbc.predict(X_test_W).round())\n",
    "    \n",
    "    estimator_rf = RandomForestRegressor(warm_start=True).fit(X_train_W, y_train)\n",
    "    scores_rf[comp] = mean_squared_error(y_test, estimator_rf.predict(X_test_W).round())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lm 5.298872180451128\n",
      "ridge 5.355263157894737\n",
      "lasso 6.795112781954887\n",
      "br 5.2593984962406015\n",
      "gbc 3.992481203007519\n",
      "rf 3.922932330827068\n"
     ]
    }
   ],
   "source": [
    "print('lm', scores.min(), scores.argmin())\n",
    "print('ridge', scores_ridge.min(), scores_ridge.argmin())\n",
    "print('lasso', scores_lasso.min(), scores_lasso.argmin())\n",
    "print('br', scores_br.min(), scores_br.argmin())\n",
    "print('gbc', scores_gbc.min(), scores_gbc.argmin())\n",
    "print('rf', scores_rf.min(), scores_rf.argmin())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NMF(n_components=44).fit(X_train)\n",
    "X_train_W = model.transform(X_train)\n",
    "\n",
    "param_grid = {'loss':['ls', 'huber', 'lad', 'quantile'],\n",
    "              'learning_rate':10.0**np.arange(-3,0,1),\n",
    "              'min_samples_split':np.arange(2,10,2),\n",
    "              'min_samples_leaf':np.arange(1,15,2),\n",
    "              'max_depth': np.arange(1,10,1)}\n",
    "\n",
    "grid = GridSearchCV(GradientBoostingRegressor(warm_start=True), cv=5, param_grid=param_grid, return_train_score=False, scoring='neg_mean_squared_error')\n",
    "grid.fit(X_train_W, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.8496240601503757"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_W = model.transform(X_test)\n",
    "\n",
    "estimator = GradientBoostingRegressor(learning_rate=0.1, loss='ls', max_depth=4, min_samples_leaf=5, min_samples_split=2, warm_start=True).fit(X_train_W, y_train)\n",
    "mean_squared_error(y_test, estimator.predict(X_test_W).round())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pickle as pkl\n",
    "#with open('models/bestTagNumberModel.p', 'wb') as f:\n",
    "#    pkl.dump(estimator, f, protocol=pkl.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "pass_data = pd.concat([indexed_data, pd.Series(estimator.predict(model.transform(indexed_data.drop(['NumQ', 'EventNumber'], axis=1))).round())], axis=1)[['EventNumber',0]+pd.get_dummies(pivot_melt_data['Tag'], prefix='T', prefix_sep=\"_\").columns.tolist()]\n",
    "pass_data.columns = ['EventNumber', 'PredQ'] + pd.get_dummies(pivot_melt_data['Tag'], prefix='T', prefix_sep=\"_\").columns.tolist()\n",
    "pass_data.to_csv('data/tagCntModel.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
