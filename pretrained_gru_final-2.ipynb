{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4510,
     "status": "ok",
     "timestamp": 1544411552530,
     "user": {
      "displayName": "Yiyan Chen",
      "photoUrl": "",
      "userId": "08805131014141803120"
     },
     "user_tz": 300
    },
    "id": "TsMRimlvfcr9",
    "outputId": "763c8c74-dd80-46a4-84b7-be2d7e3faafd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy import array\n",
    "import pickle as plk\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Bidirectional\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM, GRU\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.utils import np_utils\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Embedding\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import os\n",
    "from sacrebleu import sentence_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "DA-rbv0hf9Se"
   },
   "outputs": [],
   "source": [
    "# path = os.getcwd()+ '/drive/My Drive/capstone'\n",
    "path = os.getcwd()\n",
    "df = pd.read_excel(path+'/Q&A_Database_new.xlsx','QA', skiprows=3)\n",
    "df2 = pd.read_csv(path+ '/combined_csv_forecast.csv', encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "WO6pAiZ9f_le"
   },
   "outputs": [],
   "source": [
    "df['Breakout'] = [x.strip() for x in df['Breakout'].values]\n",
    "l = df['Breakout'].unique() # 14 unique categories\n",
    "dic = {} #create dictionary for questions\n",
    "for category in l:\n",
    "    list_1 = list(df.loc[df['Breakout'] == category]['Question'])\n",
    "    dic[category] = list_1\n",
    "l2 = df2['sub_lvl'].unique()\n",
    "dic_trans = {}\n",
    "for x in l2:\n",
    "    sub = x.split('-')[1]\n",
    "    if sub != 'CIB' and sub!= 'CB' and sub!='CCB' and sub!='AWM':\n",
    "        dic_trans[x] = sub[0]+sub.lower()[1:]\n",
    "    else:\n",
    "        dic_trans[x] = sub\n",
    "#create dictionary for questions\n",
    "for category in l2:\n",
    "    cat = dic_trans[category]\n",
    "    list_ = list(df2.loc[df2['sub_lvl']  == category]['Question'])\n",
    "    dic[cat]+=list_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "eyLwmah7isBn"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import re\n",
    "import string\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "\n",
    "# tokenizer = spacy.load('en_core_web_sm')\n",
    "punctuations = string.punctuation\n",
    "\n",
    "#tokenize sentence by sentence\n",
    "def question_split(input_):\n",
    "    list_ = []\n",
    "    for q in input_:\n",
    "        q = q.split('\\n')\n",
    "        if len(q) ==1 and len(tokenize(q[0]))>= 10:\n",
    "            list_.append(q)\n",
    "        else:\n",
    "            for i in range(len(q)):\n",
    "                if len(tokenize(q[i])) >= 10:\n",
    "                    list_.append(q[i])\n",
    "    return list_\n",
    "\n",
    "def tokenize(sent):\n",
    "#   sent = re.sub('[^A-Za-z&]', ' ', sent) # replace non-letter with space\n",
    "#   sent = re.sub(r'\\b[a-zA-Z]\\b', '', sent) #remove single letter \n",
    "    sent = re.sub('y ou', 'you', sent)\n",
    "    sent = re.sub('y es', 'yes', sent)\n",
    "    sent = re.sub('v o', 'vo', sent)\n",
    "    sent = re.sub(\"don't\", 'dont', sent)\n",
    "    sent = re.sub('[^A-Za-z&]', ' ', sent)\n",
    "    sent = re.sub( r'([a-zA-Z])([,.!?])', r'\\1 \\2', sent)\n",
    "#     tokens = tokenizer(sent)\n",
    "    return sent.split()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "ZNX43ejVizuo"
   },
   "outputs": [],
   "source": [
    "#load training and validation dataset\n",
    "CATEGORY = 'Balance sheet'\n",
    "dic_len = len(dic[CATEGORY])\n",
    "all_data = dic[CATEGORY]\n",
    "split_val = int(dic_len*0.8)\n",
    "train_data =dic[CATEGORY][:split_val]\n",
    "val_data = dic[CATEGORY][split_val: ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "lf8_Pxoxi2cy"
   },
   "outputs": [],
   "source": [
    "# organize into sequences of tokens\n",
    "def build_sequence(length, input_):\n",
    "    input_ = ['<s>'] + input_ + ['</s>']\n",
    "    if len(input_) < length:\n",
    "        input_ = input_ + ['<pad>'] * (length - len(input_))\n",
    "    sequences = list()\n",
    "    for l in range(length, len(input_)+1):\n",
    "        seq = input_[l-length:l]\n",
    "        line = ' '.join(seq)\n",
    "        sequences.append(line)\n",
    "#     print('Total Sequences: {}' .format(len(sequences)))\n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "az7pud5VjC9a"
   },
   "outputs": [],
   "source": [
    "def data_generator(dic, length):\n",
    "    out_list = []\n",
    "    raw = question_split(dic)\n",
    "    for i in range(len(raw)):\n",
    "        sent = tokenize(raw[i][0].lower())\n",
    "        if len(sent) >= 10:\n",
    "            q_sequence = build_sequence(length, sent)\n",
    "            out_list += q_sequence\n",
    "    return out_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "B6-SxutKpGDn"
   },
   "outputs": [],
   "source": [
    "all_data_words = []\n",
    "for i in range(len(all_data)):\n",
    "    sent = tokenize(all_data[i].lower())\n",
    "    all_data_words += sent\n",
    "all_data_words = list(set(all_data_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "waWg5fDtjEae"
   },
   "outputs": [],
   "source": [
    "# add words not in the embeddings\n",
    "words_to_load = 50000\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "SOS_IDX = 2\n",
    "EOS_IDX = 3\n",
    "import numpy as np\n",
    "# reserve the 1st 2nd token for padding and <UNK> respectively\n",
    "wiki_path ='/Users/cyian/Desktop/NYU/FALL2018/DS-GA1011_NLP/project/'\n",
    "with open(wiki_path+'/wiki-news-300d-1M.vec') as f:\n",
    "    loaded_embeddings_ft_en = np.zeros((words_to_load+4, 300))\n",
    "    words_ft_en = {}\n",
    "    idx2words_ft_en = {}\n",
    "    ordered_words_ft_en = []\n",
    "    ordered_words_ft_en.extend(['<pad>', '<unk>', '<s>', '</s>'])\n",
    "    loaded_embeddings_ft_en[0,:] = np.zeros(300)\n",
    "    loaded_embeddings_ft_en[1,:] = np.random.normal(size = 300)\n",
    "    loaded_embeddings_ft_en[2,:] = np.random.normal(size = 300)\n",
    "    loaded_embeddings_ft_en[3,:] = np.random.normal(size = 300)\n",
    "    for i, line in enumerate(f):\n",
    "        if i >= words_to_load: \n",
    "            break\n",
    "        s = line.split()\n",
    "        loaded_embeddings_ft_en[i+4, :] = np.asarray(s[1:])\n",
    "        words_ft_en[s[0]] = i+4\n",
    "        idx2words_ft_en[i+4] = s[0]\n",
    "        ordered_words_ft_en.append(s[0])\n",
    "    length = len(np.setdiff1d(all_data_words, ordered_words_ft_en))\n",
    "    tmp_embeddings = np.zeros((length, 300))\n",
    "    for idx, word in enumerate(np.setdiff1d(all_data_words, ordered_words_ft_en)):\n",
    "        words_ft_en[word] = idx+words_to_load+4\n",
    "        idx2words_ft_en[idx+words_to_load+4] = word\n",
    "        tmp_embeddings[idx, :] = np.random.normal(size = 300)\n",
    "    loaded_embeddings_ft_en = np.concatenate((loaded_embeddings_ft_en, tmp_embeddings), axis = 0)\n",
    "    words_ft_en['<pad>'] = PAD_IDX\n",
    "    words_ft_en['<unk>'] = UNK_IDX\n",
    "    words_ft_en['<s>'] = SOS_IDX\n",
    "    words_ft_en['</s>'] = EOS_IDX\n",
    "    idx2words_ft_en[PAD_IDX] = '<pad>'\n",
    "    idx2words_ft_en[UNK_IDX] = '<unk>'\n",
    "    idx2words_ft_en[SOS_IDX] = '<s>'\n",
    "    idx2words_ft_en[EOS_IDX] = '</s>'\n",
    "    ordered_words_ft_en = list(words_ft_en.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "zckTfH8kkuCl"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "EMBEDDING_DIM = 300\n",
    "MAX_SEQUENCE_LENGTH = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "BH1d63qvk4Zd"
   },
   "outputs": [],
   "source": [
    "all_d = data_generator(all_data,MAX_SEQUENCE_LENGTH)\n",
    "train_d = data_generator(train_data,MAX_SEQUENCE_LENGTH)\n",
    "val_d = data_generator(val_data,MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "qiKjFifmlEbv"
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import RMSprop\n",
    "def data_loader(data_input):\n",
    "    X = []\n",
    "    y = []\n",
    "    y_ind = []\n",
    "    for i in range(len(data_input)):\n",
    "        X.append([words_ft_en[x] if x in ordered_words_ft_en else UNK_IDX for x in data_input[i].split()[:-1]])\n",
    "        sub = data_input[i].split()[-1]\n",
    "        if sub in ordered_words_ft_en:\n",
    "            y.append(loaded_embeddings_ft_en[words_ft_en[sub]])\n",
    "            y_ind.append(words_ft_en[sub])\n",
    "        else:\n",
    "            y.append(loaded_embeddings_ft_en[UNK_IDX])\n",
    "            y_ind.append(UNK_IDX)\n",
    "    return np.array(X), np.array(y), np.array(y_ind)\n",
    "# train_input2, train_label2, train_label_idx2 =  data_loader(train_d)\n",
    "# val_input2, val_label2, val_label_idx2 =  data_loader(val_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "-H5SVX22tSfw"
   },
   "outputs": [],
   "source": [
    "# plk.dump(train_input2, open(path + '/train_input2.p', 'wb'))\n",
    "# plk.dump(train_label2, open(path + '/train_label2.p', 'wb'))\n",
    "# plk.dump(train_label_idx2, open(path + '/train_label_idx2.p', 'wb'))\n",
    "# plk.dump(val_input2, open(path+ '/val_input2.p', 'wb'))\n",
    "# plk.dump(val_label2, open(path +'/val_label2.p', 'wb'))\n",
    "# plk.dump(val_label_idx2, open(path +'/val_label_idx2.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "_d4Yq-xdtlPf"
   },
   "outputs": [],
   "source": [
    "def save_file(lines, filename):\n",
    "    data = '\\n'.join(lines)\n",
    "    file = open(filename, 'w')\n",
    "    file.write(data)\n",
    "    file.close()\n",
    "\n",
    "out_filename = 'Balance sheet.txt'\n",
    "save_file(all_data, out_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "L_BqVmGWt0RQ"
   },
   "outputs": [],
   "source": [
    "# load doc into memory\n",
    "def load_file(filename):\n",
    "    file = open(filename, 'r')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "55cJfxEUvIaV"
   },
   "outputs": [],
   "source": [
    "train_input = plk.load(open(path + '/train_input.p', 'rb'))\n",
    "train_label = plk.load(open(path + '/train_label.p', 'rb'))\n",
    "train_label_idx = plk.load(open(path + '/train_label_idx.p', 'rb'))\n",
    "val_input = plk.load(open(path + '/val_input.p', 'rb'))\n",
    "val_label = plk.load(open(path + '/val_label.p', 'rb'))\n",
    "val_label_idx = plk.load(open(path + '/val_label_idx.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "colab_type": "code",
    "id": "goK1JYz0k7tl",
    "outputId": "21f3f004-ff11-4426-861b-b7fccbe8a7cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 29, 300)           15193200  \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 29, 600)           1081800   \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 600)               1621800   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               60100     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 50644)             5115044   \n",
      "=================================================================\n",
      "Total params: 23,071,944\n",
      "Trainable params: 23,071,944\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 43109 samples, validate on 13179 samples\n",
      "Epoch 1/10\n",
      "13184/43109 [========>.....................] - ETA: 7:06 - loss: 7.3160 - acc: 0.0446"
     ]
    }
   ],
   "source": [
    "# sequences = tokenizer.texts_to_sequences(lines)\n",
    "# vocab_size = len(tokenizer.word_index) + 1\n",
    "# MAX_NUM_WORDS = words_to_load\n",
    "# tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "# tokenizer.fit_on_texts(lines)\n",
    "# sequences = tokenizer.texts_to_sequences(lines)\n",
    "# word_index = tokenizer.word_index\n",
    "# data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "# X, y = data[:,:-1], data[:,-1]\n",
    "vocab_size = loaded_embeddings_ft_en.shape[0]\n",
    "# y = to_categorical(y, num_classes=vocab_size)\n",
    "# seq_length = X.shape[1]\n",
    "train_label_cate = to_categorical(train_label_idx, vocab_size)\n",
    "val_label_cate = to_categorical(val_label_idx,  vocab_size)\n",
    "# define model\n",
    "model = Sequential()\n",
    "embedding_layer = Embedding(vocab_size,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[loaded_embeddings_ft_en],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH-1,\n",
    "                            trainable=True)\n",
    "model.add(embedding_layer)\n",
    "model.add(Bidirectional(GRU(300, return_sequences=True)))\n",
    "model.add(Bidirectional(GRU(300)))\n",
    "model.add(Dense(100, activation='tanh'))\n",
    "model.add(Dense(train_label_cate.shape[1], activation='softmax'))\n",
    "\n",
    "#import the checkpoint to save current model\n",
    "filepath=path+\"/GRU_2.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "earlystopper = EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n",
    "callbacks_list = [checkpoint, earlystopper]\n",
    "# compile model\n",
    "\n",
    "# rms = RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "# fit the model\n",
    "model.fit(train_input, train_label_cate, validation_data =(val_input, val_label_cate), batch_size= 128, epochs=10, callbacks=callbacks_list)\n",
    "\n",
    " \n",
    "# save the model to file\n",
    "model.save(path+'/model_all_2.h5')\n",
    "# save the tokenizer\n",
    "# plk.dump(tokenizer, open('tokenizer.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "B5VUFsVIvHEE"
   },
   "outputs": [],
   "source": [
    "from random import randint\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    " \n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    file = open(filename, 'r')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text\n",
    " \n",
    "# generate a sequence from a language model\n",
    "def generate_seq(model, seed_text_idx, n_words):\n",
    "    bleu_score = []\n",
    "    result = list()\n",
    "    target_text = lines[seed_text_idx]\n",
    "    seed_text = ' '.join(target_text.split()[:5])\n",
    "    target_text_test = ' '.join(target_text.split()[5:])\n",
    "    in_text = seed_text\n",
    "    \n",
    "    for _ in range(n_words):\n",
    "    # encode the text as integer\n",
    "#         encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        encoded = [words_ft_en[x] if x in ordered_words_ft_en else UNK_IDX for x in in_text.split()]\n",
    "        # truncate sequences to a fixed length\n",
    "        encoded = pad_sequences([encoded], maxlen=MAX_SEQUENCE_LENGTH-1, truncating='pre')\n",
    "        print(encoded)\n",
    "        # predict probabilities for each word\n",
    "        yhat = model.predict_classes(encoded, verbose=0)\n",
    "        # map \n",
    "#         predicted word index to word\n",
    "        out_word = idx2words_ft_en[yhat[0]]\n",
    "        if yhat[0] == EOS_IDX:\n",
    "            break\n",
    "    # append to input\n",
    "        in_text += ' ' + out_word\n",
    "        result.append(out_word)\n",
    "    seq = ' '.join(result)\n",
    "    ret_seq = seed_text + ' '+seq\n",
    "    bleu_score = sentence_bleu(seq, target_text_test)\n",
    "    return ret_seq, target_text, bleu_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1788
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 25322,
     "status": "ok",
     "timestamp": 1544394547931,
     "user": {
      "displayName": "Yiyan Chen",
      "photoUrl": "",
      "userId": "08805131014141803120"
     },
     "user_tz": 300
    },
    "id": "_p46F4FLsR3L",
    "outputId": "42600c43-c05f-4075-ae1b-2d21141888a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   2  33  86  41 463]]\n",
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   2  33  86  41 463  30]]\n",
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   2  33  86  41 463  30  30]]\n",
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   2  33  86  41 463  30  30  30]]\n",
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   2  33  86  41 463  30  30  30  30]]\n",
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   2  33  86  41 463  30  30  30  30  30]]\n",
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    2  33  86  41 463  30  30  30  30  30  30]]\n",
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   2\n",
      "   33  86  41 463  30  30  30  30  30  30  30]]\n",
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   2  33\n",
      "   86  41 463  30  30  30  30  30  30  30  30]]\n",
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   2  33  86\n",
      "   41 463  30  30  30  30  30  30  30  30  30]]\n",
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   2  33  86  41\n",
      "  463  30  30  30  30  30  30  30  30  30  30]]\n",
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   2  33  86  41 463\n",
      "   30  30  30  30  30  30  30  30  30  30  30]]\n",
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   2  33  86  41 463  30\n",
      "   30  30  30  30  30  30  30  30  30  30  30]]\n",
      "[[  0   0   0   0   0   0   0   0   0   0   0   2  33  86  41 463  30  30\n",
      "   30  30  30  30  30  30  30  30  30  30  30]]\n",
      "[[  0   0   0   0   0   0   0   0   0   0   2  33  86  41 463  30  30  30\n",
      "   30  30  30  30  30  30  30  30  30  30  30]]\n",
      "[[  0   0   0   0   0   0   0   0   0   2  33  86  41 463  30  30  30  30\n",
      "   30  30  30  30  30  30  30  30  30  30  30]]\n",
      "[[  0   0   0   0   0   0   0   0   2  33  86  41 463  30  30  30  30  30\n",
      "   30  30  30  30  30  30  30  30  30  30  30]]\n",
      "[[  0   0   0   0   0   0   0   2  33  86  41 463  30  30  30  30  30  30\n",
      "   30  30  30  30  30  30  30  30  30  30  30]]\n",
      "[[  0   0   0   0   0   0   2  33  86  41 463  30  30  30  30  30  30  30\n",
      "   30  30  30  30  30  30  30  30  30  30  30]]\n",
      "[[  0   0   0   0   0   2  33  86  41 463  30  30  30  30  30  30  30  30\n",
      "   30  30  30  30  30  30  30  30  30  30  30]]\n",
      "[[  0   0   0   0   2  33  86  41 463  30  30  30  30  30  30  30  30  30\n",
      "   30  30  30  30  30  30  30  30  30  30  30]]\n",
      "[[  0   0   0   2  33  86  41 463  30  30  30  30  30  30  30  30  30  30\n",
      "   30  30  30  30  30  30  30  30  30  30  30]]\n",
      "[[  0   0   2  33  86  41 463  30  30  30  30  30  30  30  30  30  30  30\n",
      "   30  30  30  30  30  30  30  30  30  30  30]]\n",
      "[[  0   2  33  86  41 463  30  30  30  30  30  30  30  30  30  30  30  30\n",
      "   30  30  30  30  30  30  30  30  30  30  30]]\n",
      "[[  2  33  86  41 463  30  30  30  30  30  30  30  30  30  30  30  30  30\n",
      "   30  30  30  30  30  30  30  30  30  30  30]]\n",
      "[[ 33  86  41 463  30  30  30  30  30  30  30  30  30  30  30  30  30  30\n",
      "   30  30  30  30  30  30  30  30  30  30 249]]\n",
      "[[ 86  41 463  30  30  30  30  30  30  30  30  30  30  30  30  30  30  30\n",
      "   30  30  30  30  30  30  30  30  30 249  30]]\n",
      "[[ 41 463  30  30  30  30  30  30  30  30  30  30  30  30  30  30  30  30\n",
      "   30  30  30  30  30  30  30  30 249  30  30]]\n",
      "[[463  30  30  30  30  30  30  30  30  30  30  30  30  30  30  30  30  30\n",
      "   30  30  30  30  30  30  30 249  30  30  30]]\n",
      "[[ 30  30  30  30  30  30  30  30  30  30  30  30  30  30  30  30  30  30\n",
      "   30  30  30  30  30  30 249  30  30  30  30]]\n",
      "[[ 30  30  30  30  30  30  30  30  30  30  30  30  30  30  30  30  30  30\n",
      "   30  30  30  30  30 249  30  30  30  30  30]]\n",
      "[[ 30  30  30  30  30  30  30  30  30  30  30  30  30  30  30  30  30  30\n",
      "   30  30  30  30 249  30  30  30  30  30  30]]\n",
      "[[ 30  30  30  30  30  30  30  30  30  30  30  30  30  30  30  30  30  30\n",
      "   30  30  30 249  30  30  30  30  30  30  30]]\n",
      "[[ 30  30  30  30  30  30  30  30  30  30  30  30  30  30  30  30  30  30\n",
      "   30  30 249  30  30  30  30  30  30  30  30]]\n",
      "[[ 30  30  30  30  30  30  30  30  30  30  30  30  30  30  30  30  30  30\n",
      "   30 249  30  30  30  30  30  30  30  30  30]]\n",
      "[[ 30  30  30  30  30  30  30  30  30  30  30  30  30  30  30  30  30  30\n",
      "  249  30  30  30  30  30  30  30  30  30  30]]\n",
      "[[ 30  30  30  30  30  30  30  30  30  30  30  30  30  30  30  30  30 249\n",
      "   30  30  30  30  30  30  30  30  30  30  30]]\n",
      "[[ 30  30  30  30  30  30  30  30  30  30  30  30  30  30  30  30 249  30\n",
      "   30  30  30  30  30  30  30  30  30  30  30]]\n",
      "[[ 30  30  30  30  30  30  30  30  30  30  30  30  30  30  30 249  30  30\n",
      "   30  30  30  30  30  30  30  30  30  30  30]]\n",
      "[[ 30  30  30  30  30  30  30  30  30  30  30  30  30  30 249  30  30  30\n",
      "   30  30  30  30  30  30  30  30  30  30  30]]\n",
      "[[ 30  30  30  30  30  30  30  30  30  30  30  30  30 249  30  30  30  30\n",
      "   30  30  30  30  30  30  30  30  30  30  30]]\n",
      "[[ 30  30  30  30  30  30  30  30  30  30  30  30 249  30  30  30  30  30\n",
      "   30  30  30  30  30  30  30  30  30  30  30]]\n",
      "[[ 30  30  30  30  30  30  30  30  30  30  30 249  30  30  30  30  30  30\n",
      "   30  30  30  30  30  30  30  30  30  30  30]]\n",
      "[[ 30  30  30  30  30  30  30  30  30  30 249  30  30  30  30  30  30  30\n",
      "   30  30  30  30  30  30  30  30  30  30  30]]\n",
      "[[ 30  30  30  30  30  30  30  30  30 249  30  30  30  30  30  30  30  30\n",
      "   30  30  30  30  30  30  30  30  30  30  30]]\n",
      "[[ 30  30  30  30  30  30  30  30 249  30  30  30  30  30  30  30  30  30\n",
      "   30  30  30  30  30  30  30  30  30  30  30]]\n",
      "[[ 30  30  30  30  30  30  30 249  30  30  30  30  30  30  30  30  30  30\n",
      "   30  30  30  30  30  30  30  30  30  30  30]]\n",
      "[[ 30  30  30  30  30  30 249  30  30  30  30  30  30  30  30  30  30  30\n",
      "   30  30  30  30  30  30  30  30  30  30  30]]\n",
      "[[ 30  30  30  30  30 249  30  30  30  30  30  30  30  30  30  30  30  30\n",
      "   30  30  30  30  30  30  30  30  30  30  30]]\n",
      "[[ 30  30  30  30 249  30  30  30  30  30  30  30  30  30  30  30  30  30\n",
      "   30  30  30  30  30  30  30  30  30  30  30]]\n",
      "<s> I would have thought it would have been a better hedge outcome for you if you assured that.   </s>\n",
      "<s> I would have thought by by by by by by by by by by by by by by by by by by by by by by by by research by by by by by by by by by by by by by by by by by by by by by by by by by\n",
      "BLEU score is:  0.02062403823169552\n"
     ]
    }
   ],
   "source": [
    "#test 1\n",
    "# load cleaned text sequences\n",
    "in_filename = 'Balance sheet.txt'\n",
    "doc = load_file(in_filename)\n",
    "lines = ['<s> '+x+ ' </s>' for x in doc.split('\\n') if x != '']\n",
    "# seq_length = len(lines[0].split()) - 1\n",
    " \n",
    "# load the model\n",
    "model = load_model(path+'/model_all.h5')\n",
    " \n",
    "# # load the tokenizer\n",
    "# tokenizer = plk.load(open('tokenizer.pkl', 'rb'))\n",
    " \n",
    "# select a seed text\n",
    "seed_text_idx = randint(0,len(lines))\n",
    " \n",
    "# generate new text\n",
    "generated, target, bleu = generate_seq(model, seed_text_idx, 50)\n",
    "print(target)\n",
    "print(generated)\n",
    "print('BLEU score is: ', bleu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 275,
     "status": "ok",
     "timestamp": 1544394049474,
     "user": {
      "displayName": "Yiyan Chen",
      "photoUrl": "",
      "userId": "08805131014141803120"
     },
     "user_tz": 300
    },
    "id": "A_JhLOOJxTZ4",
    "outputId": "f01abe0e-2509-4b09-e2b5-b5b59a3d37d9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43109, 300)"
      ]
     },
     "execution_count": 51,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "5TPvIGOwSlnA"
   },
   "outputs": [],
   "source": [
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, GRU\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "word_dim = 50\n",
    "num_tokens = 15000\n",
    "\n",
    "# Define the layers\n",
    "word_vec_input = Input(shape=(word_dim,))\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "decoder_embed = Embedding(input_dim=num_tokens, output_dim=word_dim, mask_zero=True)\n",
    "decoder_gru_1 = GRU(word_dim, return_sequences=True, return_state=False)\n",
    "decoder_gru_2 = GRU(word_dim, return_sequences=True, return_state=True)\n",
    "decoder_dense = Dense(num_tokens, activation='softmax')\n",
    "\n",
    "# Connect the layers\n",
    "embedded = decoder_embed(decoder_inputs)\n",
    "gru_1_output = decoder_gru_1(embedded, initial_state=word_vec_input)\n",
    "gru_2_output, state_h = decoder_gru_2(gru_1_output)\n",
    "decoder_outputs = decoder_dense(gru_2_output)\n",
    "\n",
    "# Define the model that will be used for training\n",
    "training_model = Model([word_vec_input, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# Also create a model for inference (this returns the GRU state)\n",
    "decoder_model = Model([word_vec_input, decoder_inputs], [decoder_outputs, state_h])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "k-MdE2TrCvdt"
   },
   "outputs": [],
   "source": [
    "# create a weight matrix for words in training docs\n",
    "embedding_matrix = np.zeros((vocab_size, 300))\n",
    "for word, i in t.word_index.items():\n",
    "\tembedding_vector = embeddings_index.get(word)\n",
    "\tif embedding_vector is not None:\n",
    "\t\tembedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 305,
     "status": "ok",
     "timestamp": 1544393977800,
     "user": {
      "displayName": "Yiyan Chen",
      "photoUrl": "",
      "userId": "08805131014141803120"
     },
     "user_tz": 300
    },
    "id": "q_pEEfaRC9pA",
    "outputId": "f521cb5d-0b4f-497b-afe1-a3070a392fdd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 46,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 240,
     "status": "ok",
     "timestamp": 1544394001670,
     "user": {
      "displayName": "Yiyan Chen",
      "photoUrl": "",
      "userId": "08805131014141803120"
     },
     "user_tz": 300
    },
    "id": "unUWD6tjDZrK",
    "outputId": "9fc6d2af-cade-4bae-9f41-59edbd7c6100"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  31, 1010, 1487, ...,  229,   85,    3])"
      ]
     },
     "execution_count": 48,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "Yjbo5uvkDiKq"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "pretrained_gru_final.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
